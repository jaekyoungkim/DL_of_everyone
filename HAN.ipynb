{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8Csv8a+7qQc3HplruxIQu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaekyoungkim/DL_of_everyone/blob/main/HAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5cUxpwNzdm5"
      },
      "outputs": [],
      "source": [
        "# https://blog.naver.com/hist0134/221386940063\n",
        "# http://hist0134.blog.me/221179965199\n",
        "# https://github.com/hist0613/keras-implementations/blob/main/IMDB-HieAtt.ipynb\n",
        "# https://github.com/kwonsuhan/Exchange-Rate-Caster/blob/a4dff9961e44572b7a950c347944f3386f9cbe1e/code/han/HAN_model.py\n",
        "# [Keras] Hierarchical Attention Networks for Document Classification 구현"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMDB-HieAtt.ipynb\n",
        "# We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. \n",
        "# "
      ],
      "metadata": {
        "id": "G27Jxk2L02cN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nltk.download('punkt')\n",
        "import nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8ZPKbBo53aY",
        "outputId": "859346e6-c1b0-4820-c5d2-7da5bf2126dd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `'punkt''\n",
            "/bin/bash: -c: line 0: `nltk.download('punkt')'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "#4/1AX4XfWgKzT5FnEsEqbHv_MJeAYGD6bx-GaC_4wW5f-wWQPUEX9zI2zEL4ug\n",
        "# crm = pd.read_csv('gdrive/MyDrive/DW_DATA/CRMDATA_product.csv', encoding = 'c==p949')  # cp949\n",
        "# CRM품목정리 파일 활용\n",
        "# utf-8  or cp 949 로 코딩"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYSo3Sc3tkmX",
        "outputId": "f016b400-6abc-47f9-cfcf-6c2b88ecadc6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SENTENCES = 10\n",
        "MAX_SENTENCE_LENGTH = 25"
      ],
      "metadata": {
        "id": "kXxG-aZQ6Sbt"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# refer: http://ai.stanford.edu/~amaas/data/sentiment/\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "data_dir = \"gdrive/MyDrive/dataset/IMDB/aclImdb\"  # 데이터 받아서 해당 폴더 생성한거에 집어넣음\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "print(train_dir)\n",
        "print(test_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPgMXfp1trmI",
        "outputId": "b4d22dd4-56e4-4020-c88a-b9532020b7ee"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gdrive/MyDrive/dataset/IMDMB/aclImdb/train\n",
            "gdrive/MyDrive/dataset/IMDMB/aclImdb/test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_dir = os.path.join(base_dir, 'train')\n",
        "split_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ftjDAdsw1CLM",
        "outputId": "090fe270-22b4-4e4c-881f-51d959904f02"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gdrive/MyDrive/dataset/IMDMB/aclImdb/train'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(split='train'):\n",
        "    x_data = []\n",
        "    y_data = []\n",
        "    \n",
        "    base_dir = 'gdrive/MyDrive/dataset/IMDB/aclImdb'\n",
        "    split_dir = os.path.join(base_dir, split)\n",
        "    for sentiment, y in [('neg', 0), ('pos', 1)]:\n",
        "        data_dir = os.path.join(split_dir, sentiment)\n",
        "        for file_name in os.listdir(data_dir):\n",
        "            file_path = os.path.join(data_dir, file_name)\n",
        "            with open(file_path, 'r', encoding='utf-8') as fp:\n",
        "                review = fp.read()\n",
        "            x_data.append(review)\n",
        "            y_data.append(y)\n",
        "            \n",
        "    return x_data, y_data"
      ],
      "metadata": {
        "id": "0qXBLIJl8hy3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_data, train_y_data = load_dataset(split='train')\n",
        "test_x_data, test_y_data = load_dataset(split='test')\n",
        "\n",
        "print(\"len(train_x_data): {}\".format(len(train_x_data)))\n",
        "print(\"len(test_x_data): {}\".format(len(test_x_data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxlNkpgdz5Ef",
        "outputId": "184f67b7-a4a7-4aaa-a4e7-ddbbc21f7e6e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(train_x_data): 25000\n",
            "len(test_x_data): 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_data[5]"
      ],
      "metadata": {
        "id": "2-ZQVa4l6SfF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "220d9b29-0ede-43a3-f7ca-56708415cd51"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Are you kidding me?! A show highlighting someone who opens cans and envelopes for a meal? How talented do you have to be to do this? She MAY be able to cook but it is NOT portrayed in this half-hour stomach churning painful production. I know she has a Martha-Stewart-esquire empire. So does Warren Buffett but I don\\'t see him with fake knockers opening cans of cream corn and Alpo.<br /><br />She has a nephew named...Brycer. Brycer? Stop talking about anyone a name that stupid.<br /><br />More time is spent on \"table-scapes\" than actual cooking. Who has that kind of time?! Silicon should be on your spatula, not on my TV. This show should be on Cartoon Network, NOT Food Network.'"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y_data[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71uPbeN96ZcY",
        "outputId": "07fec319-1581-42e8-a47b-d97c0fe6ec0e"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer  # 토큰화하는 패키지\n",
        "from keras.preprocessing.sequence import pad_sequences # padding 을 위한 패키지\n",
        "#from keras.utils import to_categorical # 범주형 변수로 만들어줌 / 실행안되는 이유확인\n",
        "from tensorflow.keras.utils import to_categorical # \n",
        "from nltk.tokenize import sent_tokenize  # 문장을 token화 시킴\n"
      ],
      "metadata": {
        "id": "EF3wKKPp6Zfd"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=sent_tokenize(train_x_data[1])"
      ],
      "metadata": {
        "id": "rpqPvtQmIMov"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a  #문장단위로 끊어줌, 끊는 기준이 있을것임"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgsLVMnLINYu",
        "outputId": "f6cc9d4a-0fa4-4b9d-9e10-459f9acf6773"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['If i could have rated this movie by 0 i would have !',\n",
              " 'I see some ppl at IMDb says that this is the funniest movie of the year , etc etc excuse me ?',\n",
              " 'are you ppl snorting LSD or ........?',\n",
              " 'There is absolutely NOTHING funny about this movie N O T H I N G !',\n",
              " 'I actually want my 27 minutes back of my life that i spent watching this piece of crap.',\n",
              " '<br /><br />I read someone sitting on an airplane watching this movie stopped watching after 30 minutes , i totally understand that , i actually would have watched snakes on a plane for 2 times over instead of watching this movie once !',\n",
              " '<br /><br />DO NOT watch this movie , do something else useful with your life do the dishes , walk the dog , hell... anything is better than spending time in front of the TV watching hot rod.']"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = tokenizer.texts_to_sequences(a)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiIZ8soDIRuq",
        "outputId": "3ff5d07b-ac0b-45c4-c4d2-6dde18a2933a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[43, 10, 98, 25, 1196, 11, 17, 31, 2196, 10, 58, 25], [10, 63, 47, 18881, 30, 869, 546, 12, 11, 6, 1, 1536, 17, 4, 1, 301, 506, 506, 1385, 68], [23, 22, 18881, 17516, 11799, 38], [46, 6, 419, 161, 152, 42, 11, 17, 3184, 1461, 789, 2150, 10, 3184, 1299], [10, 160, 178, 56, 7720, 228, 141, 4, 56, 114, 12, 10, 995, 147, 11, 412, 4, 590], [7, 7, 10, 339, 296, 1241, 20, 32, 3922, 147, 11, 17, 2339, 147, 100, 1086, 228, 10, 456, 386, 12, 10, 160, 58, 25, 287, 3202, 20, 3, 1545, 15, 230, 209, 121, 298, 4, 147, 11, 17, 281], [7, 7, 77, 21, 103, 11, 17, 77, 137, 329, 4463, 16, 125, 114, 77, 1, 13497, 1192, 1, 826, 603, 231, 6, 126, 71, 3281, 55, 8, 982, 4, 1, 240, 147, 879, 5479]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = pad_sequences(b, maxlen=25) # 길이를 25로 맞추기\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXuT3uerLQ_e",
        "outputId": "52256f03-8d1a-4680-946a-1926c3c38590"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0    43    10    98    25  1196    11    17    31  2196    10    58\n",
            "     25]\n",
            " [    0     0     0     0     0    10    63    47 18881    30   869   546\n",
            "     12    11     6     1  1536    17     4     1   301   506   506  1385\n",
            "     68]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0    23    22 18881 17516 11799\n",
            "     38]\n",
            " [    0     0     0     0     0     0     0     0     0     0    46     6\n",
            "    419   161   152    42    11    17  3184  1461   789  2150    10  3184\n",
            "   1299]\n",
            " [    0     0     0     0     0     0     0    10   160   178    56  7720\n",
            "    228   141     4    56   114    12    10   995   147    11   412     4\n",
            "    590]\n",
            " [ 1086   228    10   456   386    12    10   160    58    25   287  3202\n",
            "     20     3  1545    15   230   209   121   298     4   147    11    17\n",
            "    281]\n",
            " [ 4463    16   125   114    77     1 13497  1192     1   826   603   231\n",
            "      6   126    71  3281    55     8   982     4     1   240   147   879\n",
            "   5479]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_x_data)\n",
        "tokenizer.fit_on_texts(test_x_data)\n",
        "\n",
        "max_nb_words = len(tokenizer.word_index) + 1  # 토큰나이즈했을때에 나타나는 단어개수만큼\n"
      ],
      "metadata": {
        "id": "44Qxt8TtGPRv"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg8Vim0w-iQ9",
        "outputId": "e71b62e8-1737-4fb0-d6ee-80108d286b8c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras_preprocessing.text.Tokenizer at 0x7f8a1730bb50>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_nb_words # 124253"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yx26Kll-lJw",
        "outputId": "22e65806-7951-439d-d530-7f833aa62d40"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124253"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SENTENCE_LENGTH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FexyIekKHLrc",
        "outputId": "a78a194e-2a37-4901-80c7-84d6825957dc"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SENTENCES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCopOJnuHMiT",
        "outputId": "a53eaf61-a74b-43c3-949e-0adb6351c18a"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpdtuVdR-yFa",
        "outputId": "fdf7248b-0629-4ece-b5dd-d6272cd80177"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()\n",
        "# 여기서 punkt를 설치할 수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLHYU4z_FkQw",
        "outputId": "7f9d63ac-b7cf-483a-a58a-62b55300b439"
      },
      "execution_count": 66,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt\n",
            "    Downloading package punkt to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt.zip.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def doc2hierarchical(text,\n",
        "                     max_sentences=MAX_SENTENCES,\n",
        "                     max_sentence_length=MAX_SENTENCE_LENGTH):\n",
        "    sentences = sent_tokenize(text)  \n",
        "    tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "    tokenized_sentences = pad_sequences(tokenized_sentences, maxlen=max_sentence_length)\n",
        "\n",
        "    pad_size = max_sentences - tokenized_sentences.shape[0]\n",
        "\n",
        "    if pad_size <= 0:  # tokenized_sentences.shape[0] < max_sentences\n",
        "        tokenized_sentences = tokenized_sentences[:max_sentences]\n",
        "    else:\n",
        "        tokenized_sentences = np.pad(\n",
        "            tokenized_sentences, ((0, pad_size), (0, 0)),\n",
        "            mode='constant', constant_values=0\n",
        "        )\n",
        "    \n",
        "    return tokenized_sentences\n",
        "            \n",
        "def build_dataset(x_data, y_data, \n",
        "                  max_sentences=MAX_SENTENCES, \n",
        "                  max_sentence_length=MAX_SENTENCE_LENGTH,\n",
        "                  tokenizer=tokenizer):\n",
        "    \n",
        "    nb_instances = len(x_data)\n",
        "    X_data = np.zeros((nb_instances, max_sentences, max_sentence_length), dtype='int32')\n",
        "    for i, review in enumerate(x_data):\n",
        "        tokenized_sentences = doc2hierarchical(review)\n",
        "            \n",
        "        X_data[i] = tokenized_sentences[None, ...]\n",
        "        \n",
        "    nb_classes = len(set(y_data))  # 0,1 2개 카테고리 > 2값이 나옴\n",
        "    Y_data = to_categorical(y_data, nb_classes)  #  1/0 binary를 2개의 컬럼으로 나타냄\n",
        "    \n",
        "    return X_data, Y_data\n",
        "\n",
        "train_X_data, train_Y_data = build_dataset(train_x_data, train_y_data)\n",
        "test_X_data, test_Y_data = build_dataset(test_x_data, test_y_data)\n",
        "\n",
        "print(\"train_X_data.shape: {}\".format(train_X_data.shape))\n",
        "print(\"test_X_data.shape: {}\".format(test_X_data.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCN3o4KM5HrD",
        "outputId": "f026461c-9598-4de8-f249-8a87d81efd37"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X_data.shape: (25000, 10, 25)\n",
            "test_X_data.shape: (25000, 10, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_X_data, val_X_data, train_Y_data, val_Y_data = train_test_split(train_X_data, train_Y_data, \n",
        "                                                                      test_size=0.1, \n",
        "                                                                      random_state=42)\n",
        "print(\"train_X_data.shape: {}\".format(train_X_data.shape))\n",
        "print(\"train_Y_data.shape: {}\".format(train_Y_data.shape))\n",
        "print(\"val_X_data.shape: {}\".format(val_X_data.shape))\n",
        "print(\"val_Y_data.shape: {}\".format(val_Y_data.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNcKOXyA6KDb",
        "outputId": "8bed0440-373d-4268-c0d7-1dc0ccb72f72"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X_data.shape: (22500, 10, 25)\n",
            "train_Y_data.shape: (22500, 2)\n",
            "val_X_data.shape: (2500, 10, 25)\n",
            "val_Y_data.shape: (2500, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X_data[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S16lt544GtY3",
        "outputId": "038f226a-0ca4-419a-f64e-e547b6e63232"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    0,     0,     0,    56,   325,     2,    10,    37,     5,\n",
              "          817,    62,   364,   182,   859,   861,    97,     2,   103,\n",
              "           93,    16,   260,   350,    15,     3,   423],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           73,   210,    11,    27,    20, 83582,  2295,     2,   877,\n",
              "            5,   726,     9,     5,   260,  6526,  1011],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,   146,    50,    10,   131,    11,    17,     6,   372,\n",
              "           10,   383,     9,     8,     3,    49,    95],\n",
              "       [    0,     0,   267,    42,     9,     1,   113,   355,   158,\n",
              "           64,  1367,     6,    39,    34, 12595,     2,   359,   332,\n",
              "           18,  1606,    48,   162,     9,    34,    49],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,    10,   101,     8,    27,   129,     1,   150,   588,\n",
              "           37,    33,    70,   160,  1334,     8,   274],\n",
              "       [    0,    10,    62,   433,    12,  2590,    90,    11,    19,\n",
              "          292,   600,    50,    33,    90,     9,    84,    43,    33,\n",
              "           70,    91,    12,    58,    39,    26,   625],\n",
              "       [    0,     0,     0,     0,     0,    43,    22,    37,     5,\n",
              "          103,    62,   364,   182,    97,    39,     5,    94,   245,\n",
              "            4,    93,    91,    10,   377,    11,    27],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_Y_data[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUE6CQP9G12x",
        "outputId": "bd5137c6-e574-415b-eb69-32d142d3a6bf"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# train , test 데이터 분리하기\n",
        "train_X_data, val_X_data, train_Y_data, val_Y_data = train_test_split(train_X_data, train_Y_data, \n",
        "                                                                      test_size=0.1, \n",
        "                                                                      random_state=42)\n",
        "\n",
        "print(\"train_X_data.shape: {}\".format(train_X_data.shape))\n",
        "print(\"train_Y_data.shape: {}\".format(train_Y_data.shape))\n",
        "print(\"val_X_data.shape: {}\".format(val_X_data.shape))\n",
        "print(\"val_Y_data.shape: {}\".format(val_Y_data.shape))"
      ],
      "metadata": {
        "id": "k8mLU7sU6Shr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0411abb3-a8e4-4032-8f8e-31960ff66446"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X_data.shape: (18225, 10, 25)\n",
            "train_Y_data.shape: (18225, 2)\n",
            "val_X_data.shape: (2025, 10, 25)\n",
            "val_Y_data.shape: (2025, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# google이 이미 word2vec의 결과로 각 word에 대한 vector를 만들어서 배포\n",
        "# 하지만 용량이 커서 작은 용량에서는 어렵죠. 그래서 작은 용량의 word2vec vector로 존재\n",
        "# vector를 특정 분야에 맞게 특화시키킬 원할 경우,\n",
        "# Word2vec.intersect_word2vec_format(googleNews_filepath, binary=True, lockf=1.0)을 통해 쉽게 초기값을 설정\n",
        "# word2vec-GoogleNews-vectors에서 이미 학습된 word2vector를 다운가능\n",
        "# 다만, 학습된 모델을 가져오는 것이 아니라, “학습된 vector”만을 수치로 가져오는 것이죠.\n",
        "\n",
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')\n",
        "vec_king = wv['king']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9rmGu6bQ3fR",
        "outputId": "ae02ee4c-56b7-4a03-bd45-97d4151b1310"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv['boy'].shape # 300차원 벡터로 표현됨  GoogleNews-vectors-negative300"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xcuC-1HTuid",
        "outputId": "782b309a-fe90-41e8-aa9a-f468948033dd"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dir = 'gdrive/MyDrive/dataset/word2vec' # 파일 넣은 위치\n",
        "# word2vec을 가져옴\n",
        "def load_word2vec(tokenizer=tokenizer):\n",
        "    from gensim.models import KeyedVectors\n",
        "    embedding_path = os.path.join(embedding_dir, 'GoogleNews-vectors-negative300.bin')\n",
        "    # https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz 해당링크에서 파일 다운로드 1.5GB정도됨\n",
        "    embeddings_index = KeyedVectors.load_word2vec_format(embedding_path, binary=True)\n",
        "    \n",
        "    return embeddings_index\n",
        "    "
      ],
      "metadata": {
        "id": "Uz4dJkL6_VR4"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embedding(embedding_type='word2vec',\n",
        "                   tokenizer=tokenizer,\n",
        "                   embedding_dim=300):\n",
        "    \n",
        "    if embedding_type == 'word2vec':\n",
        "        embeddings_index = load_word2vec()  # 위에서 언급한 함수가 사용됨 load_word2vec\n",
        "        \n",
        "    embedding_matrix = np.random.normal(0, 1, (max_nb_words, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        try:\n",
        "            embedding_vector = embeddings_index[word]\n",
        "        except KeyError:\n",
        "            embedding_vector = None\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "    return embedding_matrix\n"
      ],
      "metadata": {
        "id": "bHkCVzI4TumR"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = load_embedding('word2vec')\n",
        "\n",
        "print(\"embedding_matrix.shape: {}\".format(embedding_matrix.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55Hd3H1WZynu",
        "outputId": "4f6581ff-344d-413c-bdd8-744e4d2e9ef4"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding_matrix.shape: (124253, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix # 총 124253 개의 단어들을 각각 300차원으로 만들어서 표현함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFKP4bYbacNr",
        "outputId": "aee7a6cb-0490-4793-8f5a-aa0c341f091e"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.36300594,  0.80858456,  1.75805115, ..., -0.58748376,\n",
              "         0.3996815 ,  0.41353852],\n",
              "       [ 0.08007812,  0.10498047,  0.04980469, ...,  0.00366211,\n",
              "         0.04760742, -0.06884766],\n",
              "       [-0.26642567, -2.07034651,  0.23595789, ..., -0.71999431,\n",
              "        -1.74246022, -0.27273225],\n",
              "       ...,\n",
              "       [-0.33974752, -0.01249639, -0.92992407, ...,  0.11315422,\n",
              "         0.25316221,  0.32326909],\n",
              "       [-0.09130859, -0.28320312,  0.07128906, ..., -0.33789062,\n",
              "        -0.36132812,  0.29101562],\n",
              "       [-0.18066406,  0.10351562, -0.0324707 , ...,  0.19238281,\n",
              "        -0.14550781,  0.0480957 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mVSeeWRIe4W",
        "outputId": "af0fa7ad-7605-4177-da8a-648818874ee4"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124253"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation function 3가지\n",
        "def recall(y_target, y_pred):   # 2개 요소 실제값, 예측값 필요\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
        "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
        "\n",
        "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
        "    \n",
        "    # TP = y_target_yn * y_pred_yn\n",
        "    # FN = y_target_yn - (y_target_yn * y_pred_yn) \n",
        "    \n",
        "    # return a single tensor value\n",
        "    return recall\n",
        "\n",
        "\n",
        "def precision(y_target, y_pred):# 2개 요소 실제값, 예측값 필요\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
        "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
        "\n",
        "    # Precision = (True Positive) / (True Positive + False Positive)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
        "\n",
        "    # FP = y_pred_yn - (y_target_yn * y_pred_yn) \n",
        "    \n",
        "    # return a single tensor value\n",
        "    return precision\n",
        "\n",
        "\n",
        "def f1score(y_target, y_pred): # 2개 요소 실제값, 예측값 필요\n",
        "    _recall = recall(y_target, y_pred)\n",
        "    _precision = precision(y_target, y_pred)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
        "    \n",
        "    # return a single tensor value\n",
        "    return _f1score"
      ],
      "metadata": {
        "id": "CkUoJBvCNHBq"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "#from keras.engine.topology import Layer 수정해야함\n",
        "#from tensorflow.python.keras.layers import Layer, InputSpec\n",
        "\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "\n",
        "\n",
        "from keras.layers import Input, Embedding, Dense\n",
        "from keras.layers import Lambda, Permute, RepeatVector, Multiply\n",
        "from keras.layers import Bidirectional, TimeDistributed\n",
        "from keras.layers import CuDNNGRU, GRU\n",
        "from keras.layers import BatchNormalization, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "LoiA1TDobNdd"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(Layer):  # Layer를 가져옴\n",
        "    def __init__(self, attention_dim, **kwargs): #  **kwargs : list가져올때 씀\n",
        "        self.attention_dim = attention_dim\n",
        "        super(AttentionLayer, self).__init__(**kwargs)  # attentionlayer의 속성을 가져옴\n",
        "    \n",
        "    def build(self, input_shape):  \n",
        "        self.W = self.add_weight(name='Attention_Weight', # attention 값 이값을 확인해야하는것 같음\n",
        "                                 shape=(input_shape[-1], self.attention_dim),\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)  # 학습가능\n",
        "        self.b = self.add_weight(name='Attention_Bias',\n",
        "                                 shape=(self.attention_dim, ), # attention값과 쌍을 이룸?\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "        self.u = self.add_weight(name='Attention_Context_Vector',  # u: context vector\n",
        "                                 shape=(self.attention_dim, 1),  # attention값과 쌍을 이룸?\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "        \n",
        "    def call(self, x):\n",
        "        # refer to the original paper\n",
        "        # link: https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
        "        u_it = K.tanh(K.dot(x, self.W) + self.b) # hiddenlayer와의 계산\n",
        "        a_it = K.dot(u_it, self.u)  # a가 attention을 의미함 word attention이라고 볼 수 있음\n",
        "        a_it = K.squeeze(a_it, -1) # 일자형태로 만들기\n",
        "        a_it = K.softmax(a_it)  # softmax함수 적용 합이1이 되게함\n",
        "        \n",
        "        return a_it\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1])\n",
        "    \n"
      ],
      "metadata": {
        "id": "0bpE5B-9_VVK"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def WeightedSum(attentions, representations): # \n",
        "    # from Shape(batch_size, len_units) to Shape(batch_size, rnn_dim * 2, len_units)\n",
        "    repeated_attentions = RepeatVector(K.int_shape(representations)[-1])(attentions)\n",
        "    # from Shape(batch_size, rnn_dim * 2, len_units) to Shape(batch_size, len_units, lstm_dim * 2)\n",
        "    repeated_attentions = Permute([2, 1])(repeated_attentions)\n",
        "\n",
        "    # compute representation as the weighted sum of representations\n",
        "    aggregated_representation = Multiply()([representations, repeated_attentions])\n",
        "    aggregated_representation = Lambda(lambda x: K.sum(x, axis=1))(aggregated_representation)\n",
        "\n",
        "    return aggregated_representation # 합쳐진 representation\n",
        "    \n",
        "    \n",
        "def HieAtt(embedding_matrix,  # word2vec한 값\n",
        "           max_sentences, \n",
        "           max_sentence_length,\n",
        "           nb_classes,  # 클래스의 개수 여기서는 2개 \n",
        "           embedding_dim=300,  # 임베딩 차원 여기서는 300차원으로 변환시킴\n",
        "           attention_dim=100,\n",
        "           rnn_dim=150,\n",
        "           include_dense_batch_normalization=False,\n",
        "           include_dense_dropout=True,\n",
        "           nb_dense=1,\n",
        "           dense_dim=300,\n",
        "           dense_dropout=0.2,\n",
        "           optimizer = tf.keras.optimizers.Adam(lr=0.001)):\n",
        "    # Use tf.keras.optimizers.Adam(learning_rate) instead of keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "    # embedding_matrix = (max_nb_words + 1, embedding_dim)\n",
        "    max_nb_words = embedding_matrix.shape[0] - 1  # 124253-1\n",
        "    embedding_layer = Embedding(max_nb_words + 1, \n",
        "                                embedding_dim,   \n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=max_sentence_length,     # input\n",
        "                                trainable=False)   # embedding한걸 다시 학습할 필요없음\n",
        "\n",
        "    # first, build a sentence encoder\n",
        "    sentence_input = Input(shape=(max_sentence_length, ), dtype='int32')\n",
        "    embedded_sentence = embedding_layer(sentence_input)   # sentence에 대한 임베딩 / sentence단위의 임베딩으로 한단계 위\n",
        "    embedded_sentence = Dropout(dense_dropout)(embedded_sentence)\n",
        "    contextualized_sentence = Bidirectional(GRU(rnn_dim, return_sequences=True))(embedded_sentence) # CuDNNGRU\n",
        "    # embedded sentence 를 bidirectional featrue2개가 생성되고 두개가 합쳐짐\n",
        "    \n",
        "    # word attention computation\n",
        "    word_attention = AttentionLayer(attention_dim)(contextualized_sentence)  # 위에서 설정한 attentionlayer함수사용\n",
        "    sentence_representation = WeightedSum(word_attention, contextualized_sentence)  # 위에서 설정한 weightedsum 함수사용\n",
        "    \n",
        "    sentence_encoder = Model(inputs=[sentence_input],   # from keras.models import Model,\n",
        "                             outputs=[sentence_representation])  # sentence를 벡터로 표현하는 과정\n",
        "\n",
        "    # then, build a document encoder (최종 아웃풋임)\n",
        "    document_input = Input(shape=(max_sentences, max_sentence_length), dtype='int32')\n",
        "    embedded_document = TimeDistributed(sentence_encoder)(document_input)\n",
        "    contextualized_document = Bidirectional(GRU(rnn_dim, return_sequences=True))(embedded_document)  # CuDNNGRU\n",
        "    # sentence를 이용해서 doc에 대한 representation v계산\n",
        "    \n",
        "    # sentence attention computation\n",
        "    sentence_attention = AttentionLayer(attention_dim)(contextualized_document)  \n",
        "    # 해당 코드에서 가장 중요한 부분 : sentence_attention  각 sentence별 attention값이 계산되는것을 확인\n",
        "    document_representation = WeightedSum(sentence_attention, contextualized_document)\n",
        "    # doc에 대해 하나로 표현가능한 vector값이 나오게됨\n",
        "\n",
        "    # 마지막 fc layer를 통해서 분류문제를 해결하고자 함\n",
        "    # finally, add fc layers for classification\n",
        "    fc_layers = Sequential()\n",
        "    for _ in range(nb_dense): # 클래스 수만큼 fc layer를 수행함\n",
        "        if include_dense_batch_normalization == True:  #  batch norm 을 한경우\n",
        "            fc_layers.add(BatchNormalization())\n",
        "        fc_layers.add(Dense(dense_dim, activation='relu'))\n",
        "        if include_dense_dropout == True:   # drop_out 을 한경우\n",
        "            fc_layers.add(Dropout(dense_dropout))\n",
        "    fc_layers.add(Dense(nb_classes, activation='softmax'))  # soft max \n",
        "    \n",
        "    pred_sentiment = fc_layers(document_representation) # 최종 결과값이라 볼 수 있음\n",
        "\n",
        "    model = Model(inputs=[document_input],\n",
        "                  outputs=[pred_sentiment])\n",
        "    \n",
        "    ############### build attention extractor ###############\n",
        "    word_attention_extractor = Model(inputs=[sentence_input],\n",
        "                                     outputs=[word_attention])\n",
        "    word_attentions = TimeDistributed(word_attention_extractor)(document_input)\n",
        "    attention_extractor = Model(inputs=[document_input],\n",
        "                                     outputs=[word_attentions, sentence_attention])\n",
        "    # 모델에 대한 마지막 정의\n",
        "    model.compile(loss=['categorical_crossentropy'],\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    return model, attention_extractor, sentence_encoder, sentence_attention\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ta1-PGckjQVt",
        "outputId": "d26b5a79-9050-4521-9a22-5865520aec96"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"HieAtt\"\n",
        "model_path = '/models/checkpoints/{}.h5'.format(model_name)\n",
        "checkpointer = ModelCheckpoint(filepath=model_path,\n",
        "                               monitor='val_acc',\n",
        "                               verbose=True,\n",
        "                               save_best_only=True,\n",
        "                               mode='max')\n",
        "\n",
        "model, attention_extractor, sent_enc, sent_att = HieAtt(embedding_matrix=embedding_matrix,\n",
        "                                    max_sentences=MAX_SENTENCES,\n",
        "                                    max_sentence_length=MAX_SENTENCE_LENGTH,\n",
        "                                    nb_classes=2,\n",
        "                                    embedding_dim=300,\n",
        "                                    attention_dim=100,\n",
        "                                    rnn_dim=150,\n",
        "                                    include_dense_batch_normalization=False,\n",
        "                                    include_dense_dropout=True,\n",
        "                                    nb_dense=1,\n",
        "                                    dense_dim=300,\n",
        "                                    dense_dropout=0.2,\n",
        "                                    optimizer = tf.keras.optimizers.Adam(lr=0.001))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owsfhXOzeYmP",
        "outputId": "64d75c92-3b34-4cf2-f9f3-de207b889a10"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McMr1wV-HnSy",
        "outputId": "3b9e33a5-96b4-432f-a254-254de2b24c06"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.functional.Functional at 0x7f89d0c1ac90>"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()  # OUTPUT SHAPE를 확인 가능"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VJni7kJQOPH",
        "outputId": "7c8a5116-d7fc-4876-fa7b-5bd5f77c0c82"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_49\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_26 (InputLayer)          [(None, 10, 25)]     0           []                               \n",
            "                                                                                                  \n",
            " time_distributed_24 (TimeDistr  (None, 10, 300)     37712900    ['input_26[0][0]']               \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " bidirectional_25 (Bidirectiona  (None, 10, 300)     406800      ['time_distributed_24[0][0]']    \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " attention_layer_25 (AttentionL  (None, 10)          30200       ['bidirectional_25[0][0]']       \n",
            " ayer)                                                                                            \n",
            "                                                                                                  \n",
            " repeat_vector_25 (RepeatVector  (None, 300, 10)     0           ['attention_layer_25[0][0]']     \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " permute_25 (Permute)           (None, 10, 300)      0           ['repeat_vector_25[0][0]']       \n",
            "                                                                                                  \n",
            " multiply_25 (Multiply)         (None, 10, 300)      0           ['bidirectional_25[0][0]',       \n",
            "                                                                  'permute_25[0][0]']             \n",
            "                                                                                                  \n",
            " lambda_25 (Lambda)             (None, 300)          0           ['multiply_25[0][0]']            \n",
            "                                                                                                  \n",
            " sequential_12 (Sequential)     (None, 2)            90902       ['lambda_25[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 38,240,802\n",
            "Trainable params: 964,902\n",
            "Non-trainable params: 37,275,900\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_extractor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYqw1Dt9IJfg",
        "outputId": "a0a45700-fb60-4482-84d0-07fd9ff9c816"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.functional.Functional at 0x7f89d60154d0>"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=[train_X_data];type(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LeJjEp9KLKK",
        "outputId": "37a86b3b-a4da-41fb-cd45-ef6089f24d95"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y=[train_Y_data]; type(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAWsT5QMKL0m",
        "outputId": "70a8ce4a-fa3c-41b5-fc21-b8d6fcef3901"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_data=(val_X_data, val_Y_data) ;type(validation_data) # TUPLE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xKcx-nhKVUB",
        "outputId": "26f70894-fc6c-4e62-b89a-3040b70193ea"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tuple"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_data=[val_X_data, val_Y_data] ;type(validation_data) # LIST "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGIJ36t_SWTe",
        "outputId": "1651bed9-f87e-4eaf-d25b-953606bd0503"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제 데이터에 적용하는 과정\n",
        "# Hyperparameter\n",
        "max_sentences = 35\n",
        "max_sentence_length = 10  # maxlen이랑 같음\n",
        "# 사전에서 단어 수(embedding layer에서 사용)\n",
        "max_nb_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-EpZMJYGqtc",
        "outputId": "339ccc88-840f-4e87-ddd4-5d2132cb94c9"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500997"
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x=[train_X_data],  # x : list\n",
        "                    y=[train_Y_data],  # y: list\n",
        "                    batch_size=128,\n",
        "                    epochs=30,\n",
        "                    verbose=True,\n",
        "                    validation_data=(val_X_data, val_Y_data)  # tuple , 'NoneType' object is not callable\n",
        "                    )\n",
        "                    #,callbacks=[early_stop])\n",
        "\n",
        "\n",
        "# error\n",
        "# def _fixup_shape(images, labels):\n",
        "#    images.set_shape([None, 15, 256, 256, 3])\n",
        "#    labels.set_shape([None, 12])\n",
        "#    return images, labels\n",
        "# as_list() is not defined on an unknown tensorshape\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "jND3x0w8P64X",
        "outputId": "1b09c089-da51-447a-bb48-cf74b809404c"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-219-f26895638a81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_Y_data\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# tuple , 'NoneType' object is not callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                     )\n\u001b[1;32m      8\u001b[0m                     \u001b[0;31m#,callbacks=[early_stop])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "85M851dxHCWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(model_path)\n",
        "score = model.evaluate(test_X_data, test_Y_data, verbose=0, batch_size=128)\n",
        "print(\"Test Accuracy of {}: {}\".format(model_name, score[1]))"
      ],
      "metadata": {
        "id": "eimpQWvt_VYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B4GLYO136SkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.clf()\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q83837Y4_dhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "GTsqYBoB_dkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sn\n",
        "\n",
        "word_rev_index = {}\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    word_rev_index[i] = word\n",
        "\n",
        "def sentiment_analysis(review):        \n",
        "    tokenized_sentences = doc2hierarchical(review)\n",
        "    \n",
        "    # word attention만 가져오기\n",
        "    pred_attention = attention_extractor.predict(np.asarray([tokenized_sentences]))[0][0]\n",
        "    for sent_idx, sentence in enumerate(tokenized_sentences):\n",
        "        if sentence[-1] == 0:\n",
        "            continue\n",
        "            \n",
        "        for word_idx in range(MAX_SENTENCE_LENGTH):\n",
        "            if sentence[word_idx] != 0:\n",
        "                words = [word_rev_index[word_id] for word_id in sentence[word_idx:]]\n",
        "                pred_att = pred_attention[sent_idx][-len(words):]\n",
        "                pred_att = np.expand_dims(pred_att, axis=0)\n",
        "                break\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(len(words), 1))\n",
        "        plt.rc('xtick', labelsize=16)\n",
        "        midpoint = (max(pred_att[:, 0]) - min(pred_att[:, 0])) / 2\n",
        "        heatmap = sn.heatmap(pred_att, xticklabels=words, yticklabels=False, square=True, linewidths=0.1, cmap='coolwarm', center=midpoint, vmin=0, vmax=1)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.show()\n",
        "        \n",
        "# sentiment_analysis(\"Delicious healthy food. The steak is amazing. Fish and pork are awesome too. Service is above and beyond. Not a bad thing to say about this place. Worth every penny!\")\n",
        "sentiment_analysis(\"Absolute perfection end game !! Good acting performance to all the characters. Great cgi's. Truly epic & perfect ending to a long journey of marvel movie. Go "
      ],
      "metadata": {
        "id": "376HU-uc_dnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XO6gj4Ot_dq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kj8023Ao6Smy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document classification에 사용할 데이터셋은 Yelp Reviews 입니다. sentiment classification을 하는 데이터셋인데, 각 줄마다 하나의 json 데이터가 있는 구조\n",
        "x_train = []\n",
        "y_train = []\n",
        "with open(data_dir, 'r', encoding=\"utf-8\") as fp:\n",
        "    for line in fp:\n",
        "        # ['user_id', 'stars', 'text', 'funny', 'useful', 'review_id', 'business_id', 'date', 'cool']\n",
        "        line = json.loads(line)\n",
        "        x_train.append(line['text'])\n",
        "        y_train.append(int(line['stars']) - 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "qrGxYb5T02fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FAZXJPLR02h8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}