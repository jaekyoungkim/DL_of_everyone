{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "모두의 딥러닝.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPbhqmkY/CCMD200EXqJrP5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaekyoungkim/DL_of_everyone/blob/main/%EB%AA%A8%EB%91%90%EC%9D%98_%EB%94%A5%EB%9F%AC%EB%8B%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBdwtxPEEpvj"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1장 나의 첫 딥러닝 "
      ],
      "metadata": {
        "id": "ivKNNAuhoTN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 폐암 수술 환자의 생존율 예측"
      ],
      "metadata": {
        "id": "b5f2BAjloWBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "zHNH06Ko6ptO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(3)\n",
        "tf.random.set_seed(3)"
      ],
      "metadata": {
        "id": "Mp-f_kY26yMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_set =np.loadtxt(\"data/ThoraricSurgery.csv\", delimiter =',')"
      ],
      "metadata": {
        "id": "MDhWVVvJ63x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RZyJMQIoWPU",
        "outputId": "65de5536-5d39-4bc1-8cc1-a06576a81cda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[293.  ,   1.  ,   3.8 , ...,   0.  ,  62.  ,   0.  ],\n",
              "       [  1.  ,   2.  ,   2.88, ...,   0.  ,  60.  ,   0.  ],\n",
              "       [  8.  ,   2.  ,   3.19, ...,   0.  ,  66.  ,   1.  ],\n",
              "       ...,\n",
              "       [406.  ,   6.  ,   5.36, ...,   0.  ,  62.  ,   0.  ],\n",
              "       [ 25.  ,   8.  ,   4.32, ...,   0.  ,  58.  ,   1.  ],\n",
              "       [447.  ,   8.  ,   5.2 , ...,   0.  ,  49.  ,   0.  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=Data_set[:,0:17]\n",
        "Y=Data_set[:,17]"
      ],
      "metadata": {
        "id": "tTND4jFXoWR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix8tR6CT-Hbz",
        "outputId": "ef9b0c90-b924-42cd-eed3-0a25cf4bebdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
              "       0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model =Sequential()\n",
        "model.add(Dense(30, input_dim=17, activation = 'relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "c7ZamlPUoWUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer ='adam', metrics=['accuracy'])\n",
        "model.fit(X,Y, epochs=100, batch_size =10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxmPNhoN9K_p",
        "outputId": "20eded91-6898-4242-ca96-d5646b0324ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 6.3049 - accuracy: 0.4596\n",
            "Epoch 2/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.6444 - accuracy: 0.8426\n",
            "Epoch 3/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.5007 - accuracy: 0.8447\n",
            "Epoch 4/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4919 - accuracy: 0.8489\n",
            "Epoch 5/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4682 - accuracy: 0.8511\n",
            "Epoch 6/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4578 - accuracy: 0.8511\n",
            "Epoch 7/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4582 - accuracy: 0.8489\n",
            "Epoch 8/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4567 - accuracy: 0.8511\n",
            "Epoch 9/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4489 - accuracy: 0.8489\n",
            "Epoch 10/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4525 - accuracy: 0.8489\n",
            "Epoch 11/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4642 - accuracy: 0.8511\n",
            "Epoch 12/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4625 - accuracy: 0.8532\n",
            "Epoch 13/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4648 - accuracy: 0.8511\n",
            "Epoch 14/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4547 - accuracy: 0.8426\n",
            "Epoch 15/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4922 - accuracy: 0.8383\n",
            "Epoch 16/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4669 - accuracy: 0.8489\n",
            "Epoch 17/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4708 - accuracy: 0.8511\n",
            "Epoch 18/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4382 - accuracy: 0.8489\n",
            "Epoch 19/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4434 - accuracy: 0.8511\n",
            "Epoch 20/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4500 - accuracy: 0.8468\n",
            "Epoch 21/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4435 - accuracy: 0.8511\n",
            "Epoch 22/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4361 - accuracy: 0.8511\n",
            "Epoch 23/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4287 - accuracy: 0.8489\n",
            "Epoch 24/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4403 - accuracy: 0.8489\n",
            "Epoch 25/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4334 - accuracy: 0.8511\n",
            "Epoch 26/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4429 - accuracy: 0.8489\n",
            "Epoch 27/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4435 - accuracy: 0.8511\n",
            "Epoch 28/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4312 - accuracy: 0.8511\n",
            "Epoch 29/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4261 - accuracy: 0.8511\n",
            "Epoch 30/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4213 - accuracy: 0.8511\n",
            "Epoch 31/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4461 - accuracy: 0.8511\n",
            "Epoch 32/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4260 - accuracy: 0.8511\n",
            "Epoch 33/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4366 - accuracy: 0.8511\n",
            "Epoch 34/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4166 - accuracy: 0.8511\n",
            "Epoch 35/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4309 - accuracy: 0.8511\n",
            "Epoch 36/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4230 - accuracy: 0.8511\n",
            "Epoch 37/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.8511\n",
            "Epoch 38/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4251 - accuracy: 0.8511\n",
            "Epoch 39/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4674 - accuracy: 0.8468\n",
            "Epoch 40/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4133 - accuracy: 0.8511\n",
            "Epoch 41/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.8511\n",
            "Epoch 42/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4455 - accuracy: 0.8511\n",
            "Epoch 43/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4157 - accuracy: 0.8511\n",
            "Epoch 44/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4106 - accuracy: 0.8489\n",
            "Epoch 45/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4131 - accuracy: 0.8511\n",
            "Epoch 46/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4129 - accuracy: 0.8511\n",
            "Epoch 47/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4206 - accuracy: 0.8426\n",
            "Epoch 48/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4108 - accuracy: 0.8511\n",
            "Epoch 49/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4131 - accuracy: 0.8511\n",
            "Epoch 50/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4177 - accuracy: 0.8447\n",
            "Epoch 51/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4228 - accuracy: 0.8532\n",
            "Epoch 52/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4057 - accuracy: 0.8532\n",
            "Epoch 53/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4180 - accuracy: 0.8489\n",
            "Epoch 54/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4161 - accuracy: 0.8489\n",
            "Epoch 55/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4162 - accuracy: 0.8511\n",
            "Epoch 56/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4009 - accuracy: 0.8511\n",
            "Epoch 57/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4141 - accuracy: 0.8532\n",
            "Epoch 58/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4079 - accuracy: 0.8532\n",
            "Epoch 59/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4050 - accuracy: 0.8532\n",
            "Epoch 60/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4038 - accuracy: 0.8489\n",
            "Epoch 61/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4145 - accuracy: 0.8511\n",
            "Epoch 62/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4301 - accuracy: 0.8468\n",
            "Epoch 63/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4124 - accuracy: 0.8532\n",
            "Epoch 64/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4339 - accuracy: 0.8511\n",
            "Epoch 65/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.8489\n",
            "Epoch 66/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4387 - accuracy: 0.8468\n",
            "Epoch 67/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4221 - accuracy: 0.8489\n",
            "Epoch 68/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4114 - accuracy: 0.8511\n",
            "Epoch 69/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4100 - accuracy: 0.8511\n",
            "Epoch 70/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4068 - accuracy: 0.8553\n",
            "Epoch 71/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4074 - accuracy: 0.8489\n",
            "Epoch 72/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4248 - accuracy: 0.8340\n",
            "Epoch 73/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4056 - accuracy: 0.8511\n",
            "Epoch 74/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8532\n",
            "Epoch 75/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.3966 - accuracy: 0.8553\n",
            "Epoch 76/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4144 - accuracy: 0.8574\n",
            "Epoch 77/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4245 - accuracy: 0.8468\n",
            "Epoch 78/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4123 - accuracy: 0.8511\n",
            "Epoch 79/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4258 - accuracy: 0.8426\n",
            "Epoch 80/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4121 - accuracy: 0.8532\n",
            "Epoch 81/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4131 - accuracy: 0.8511\n",
            "Epoch 82/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4231 - accuracy: 0.8553\n",
            "Epoch 83/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4010 - accuracy: 0.8532\n",
            "Epoch 84/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4393 - accuracy: 0.8426\n",
            "Epoch 85/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4124 - accuracy: 0.8532\n",
            "Epoch 86/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.3978 - accuracy: 0.8489\n",
            "Epoch 87/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4026 - accuracy: 0.8511\n",
            "Epoch 88/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4073 - accuracy: 0.8532\n",
            "Epoch 89/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.3956 - accuracy: 0.8511\n",
            "Epoch 90/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4011 - accuracy: 0.8532\n",
            "Epoch 91/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4232 - accuracy: 0.8532\n",
            "Epoch 92/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4306 - accuracy: 0.8489\n",
            "Epoch 93/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4011 - accuracy: 0.8511\n",
            "Epoch 94/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.3958 - accuracy: 0.8532\n",
            "Epoch 95/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.8511\n",
            "Epoch 96/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4268 - accuracy: 0.8468\n",
            "Epoch 97/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4207 - accuracy: 0.8511\n",
            "Epoch 98/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.3931 - accuracy: 0.8596\n",
            "Epoch 99/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4099 - accuracy: 0.8553\n",
            "Epoch 100/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.3913 - accuracy: 0.8511\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f49763f2650>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "K770c9tD-Ss0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2장 딥러닝을 위한 기초수학 "
      ],
      "metadata": {
        "id": "jo421bxUoWsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gS7T6zO2Fmqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3장 딥러닝의 동작원리 "
      ],
      "metadata": {
        "id": "UHr1NxmGIsfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x= [2,4,6,8]\n",
        "y= [81,93,91,97]\n",
        "\n",
        "mx =np.mean(x)\n",
        "my =np.mean(y)\n"
      ],
      "metadata": {
        "id": "m7szAUZs-Svi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "divisor = sum([(i-mx)**2 for i in x])"
      ],
      "metadata": {
        "id": "E08yH4NrJq6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "divisor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4D8zmEO-Jww9",
        "outputId": "16c8dd27-24ac-4b11-b612-a29a2f56fdbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20.0"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def top(x,mx,y,my):\n",
        "  d=0\n",
        "  for i in range(len(x)):\n",
        "    d +=(x[i]-mx) * (y[i]-my)\n",
        "  return d\n"
      ],
      "metadata": {
        "id": "zowfL9C2JyBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dividend = top(x,mx,y,my)"
      ],
      "metadata": {
        "id": "obWCzcMkKEzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = dividend / divisor"
      ],
      "metadata": {
        "id": "C4AJs3ZEKHYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twjFy8PQKJeP",
        "outputId": "5c521179-cf03-4da3-f352-a774992ff753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = my - (mx * a)"
      ],
      "metadata": {
        "id": "AUq61M1GKJoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aPgmU-wKMUl",
        "outputId": "c07df2f7-db51-4804-de62-5737fb8d1f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79.0"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.3x +79 = y"
      ],
      "metadata": {
        "id": "pHLxHENmKMdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 평균제곱오차 코딩으로 확인"
      ],
      "metadata": {
        "id": "wkZrxwWQKOpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_a_b= [ 3,76]"
      ],
      "metadata": {
        "id": "DeDW0U9lNagt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [[2,81], [4,93],[6,91],[8,97]]"
      ],
      "metadata": {
        "id": "cEt3q7vxNdfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = [i[0] for i in data]\n",
        "y = [i[1] for i in data]"
      ],
      "metadata": {
        "id": "iBzRP29mNjmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L2j3J1-Nrqg",
        "outputId": "764ca2e2-2be6-40bc-8dab-b23a2090e069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4, 6, 8]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSRHl6EpNr9L",
        "outputId": "a3df7e6f-899d-46bb-b70d-820fbabb9100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[81, 93, 91, 97]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(x):\n",
        "  return fake_a_b[0] * x + fake_a_b[1]"
      ],
      "metadata": {
        "id": "MJY2o8CnNsoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(y, y_hat):\n",
        "  return ((y-y_hat)**2).mean()"
      ],
      "metadata": {
        "id": "FeFlISh-N2xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_val(y, predict_result):\n",
        "  return mse(np.array(y), np.array(predict_result))"
      ],
      "metadata": {
        "id": "CYnnj2x0N67x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_result =[]"
      ],
      "metadata": {
        "id": "RNsbhy5tOCrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(x)):\n",
        "  predict_result.append(predict(x[i]))\n",
        "  print(\"공부시간=%.f, 실제점수=%.f 예측점수=%.f\"%(x[i],y[i],predict(x[i])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfB-JboyOVX-",
        "outputId": "0e63fb90-07ce-47be-99e0-117e24514675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "공부시간=2, 실제점수=81 예측점수=82\n",
            "공부시간=4, 실제점수=93 예측점수=88\n",
            "공부시간=6, 실제점수=91 예측점수=94\n",
            "공부시간=8, 실제점수=97 예측점수=100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"mse 최종값:\",str(mse_val(predict_result, y)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj-vz7U6Qgap",
        "outputId": "19926c1f-a908-4ce2-aaf8-b6c56287bebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mse 최종값: 11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 오차 수정하기(경사하강법)"
      ],
      "metadata": {
        "id": "oPzPtLMuQ6a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = np.array(x)\n",
        "x_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7dpBqffRVWX",
        "outputId": "1b55ed8e-e066-4464-948c-4f95d5b9f8fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 4, 6, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_data = np.array(y)\n",
        "y_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYO9meNxbvK5",
        "outputId": "f4fba8a3-b96b-4a54-dade-3a9dceae512b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([81, 93, 91, 97])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(x,y)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "OPlPS-opbycm",
        "outputId": "7c95b76d-9c5f-4e1c-fcc7-3a2cb6d9aa43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a= 0 \n",
        "b= 0\n",
        "# 초기값"
      ],
      "metadata": {
        "id": "1apm2rA5RVZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr=0.03 # 학습률"
      ],
      "metadata": {
        "id": "9NHygPs6b7MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=2001"
      ],
      "metadata": {
        "id": "TlKglGi5b872"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(epochs):\n",
        "  y_pred = a* x_data + b\n",
        "  error = y_data - y_pred\n",
        "  a_diff = -(2/len(x_data)) *  sum(x_data * error)  # 오차함수를 a 로 미분\n",
        "  b_diff = -(2/len(x_data)) * sum(error)  # 오차함수를  b 로 미분\n",
        "\n",
        "  a= a - lr*a_diff\n",
        "  b= b - lr*b_diff\n",
        "\n",
        "  if i %100 ==0 :\n",
        "    print(\"epoch=%.f, 기울기=%.04f, 절편=%0.04f\"%(i,a,b))\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpoqoo5ub-zm",
        "outputId": "bb9881fa-62c8-4a4c-9018-ca2bf99d0a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0, 기울기=27.8400, 절편=5.4300\n",
            "epoch=100, 기울기=7.0739, 절편=50.5117\n",
            "epoch=200, 기울기=4.0960, 절편=68.2822\n",
            "epoch=300, 기울기=2.9757, 절편=74.9678\n",
            "epoch=400, 기울기=2.5542, 절편=77.4830\n",
            "epoch=500, 기울기=2.3956, 절편=78.4293\n",
            "epoch=600, 기울기=2.3360, 절편=78.7853\n",
            "epoch=700, 기울기=2.3135, 절편=78.9192\n",
            "epoch=800, 기울기=2.3051, 절편=78.9696\n",
            "epoch=900, 기울기=2.3019, 절편=78.9886\n",
            "epoch=1000, 기울기=2.3007, 절편=78.9957\n",
            "epoch=1100, 기울기=2.3003, 절편=78.9984\n",
            "epoch=1200, 기울기=2.3001, 절편=78.9994\n",
            "epoch=1300, 기울기=2.3000, 절편=78.9998\n",
            "epoch=1400, 기울기=2.3000, 절편=78.9999\n",
            "epoch=1500, 기울기=2.3000, 절편=79.0000\n",
            "epoch=1600, 기울기=2.3000, 절편=79.0000\n",
            "epoch=1700, 기울기=2.3000, 절편=79.0000\n",
            "epoch=1800, 기울기=2.3000, 절편=79.0000\n",
            "epoch=1900, 기울기=2.3000, 절편=79.0000\n",
            "epoch=2000, 기울기=2.3000, 절편=79.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = a*x_data+b"
      ],
      "metadata": {
        "id": "vXqChYQacgQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x,y)\n",
        "plt.plot([min(x_data), max(x_data)], [min(y_pred), max(y_pred)])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "hwua_lRHcjS6",
        "outputId": "06a96d9c-1ddb-478b-95a7-7326a58e5650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [[2,0,81],[4,4,93],[6,2,91],[8,3,97]]\n",
        "x1 = [i[0] for i in data]\n",
        "x2 = [i[1] for i in data]\n",
        "y = [i[2] for i in data]\n"
      ],
      "metadata": {
        "id": "NpizT7uOcp5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d"
      ],
      "metadata": {
        "id": "F6xQAfK2c_qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = plt.axes(projection='3d')  #그래프 유형 정하기#\n",
        "ax.set_xlabel('study_hours')\n",
        "ax.set_ylabel('private_class')\n",
        "ax.set_zlabel('score')\n",
        "ax.scatter(x1,x2,y)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "5qJjK0A3dQxR",
        "outputId": "4c2a65bc-dfad-42c1-8a0c-3d6711089aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(x1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neVd8PUEd3c4",
        "outputId": "94c7acb6-eedf-4a81-e4d5-bf09a8735970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1_data = np.array(x1)\n",
        "x2_data = np.array(x2)\n",
        "y_data = np.array(y)\n"
      ],
      "metadata": {
        "id": "JaEIJKG8d5C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a1=0\n",
        "a2=0\n",
        "b=0\n",
        "epochs=2001\n",
        "lr =0.02\n",
        "for i in range(epochs):\n",
        "\n",
        "  y_pred = a1 * x1_data + a2 * x2_data + b\n",
        "  error = y_data - y_pred\n",
        "\n",
        "  a1_diff = -(2/len(x1_data)) * sum(x1_data * error)\n",
        "  a2_diff = -(2/len(x2_data)) * sum(x2_data * error)\n",
        "  b_diff = -(2/len(x1_data)) *  sum(y_data - y_pred)\n",
        "  a1 = a1 -lr * a1_diff\n",
        "  a2 = a2 -lr * a2_diff\n",
        "  b= b- lr * b_diff\n",
        "\n",
        "  if i%100 ==0:\n",
        "    print(\"epoch=%.f, 기울기1=%.04f, 기울기2=%.04f, 절편=%.04f\" % (i,a1,a2,b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb1elpRNc_s9",
        "outputId": "f914a960-d213-4174-ca42-f0306f812024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0, 기울기1=18.5600, 기울기2=8.4500, 절편=3.6200\n",
            "epoch=100, 기울기1=7.2994, 기울기2=4.2867, 절편=38.0427\n",
            "epoch=200, 기울기1=4.5683, 기울기2=3.3451, 절편=56.7901\n",
            "epoch=300, 기울기1=3.1235, 기울기2=2.8463, 절편=66.7100\n",
            "epoch=400, 기울기1=2.3591, 기울기2=2.5823, 절편=71.9589\n",
            "epoch=500, 기울기1=1.9546, 기울기2=2.4427, 절편=74.7362\n",
            "epoch=600, 기울기1=1.7405, 기울기2=2.3688, 절편=76.2058\n",
            "epoch=700, 기울기1=1.6273, 기울기2=2.3297, 절편=76.9833\n",
            "epoch=800, 기울기1=1.5673, 기울기2=2.3090, 절편=77.3948\n",
            "epoch=900, 기울기1=1.5356, 기울기2=2.2980, 절편=77.6125\n",
            "epoch=1000, 기울기1=1.5189, 기울기2=2.2922, 절편=77.7277\n",
            "epoch=1100, 기울기1=1.5100, 기울기2=2.2892, 절편=77.7886\n",
            "epoch=1200, 기울기1=1.5053, 기울기2=2.2875, 절편=77.8209\n",
            "epoch=1300, 기울기1=1.5028, 기울기2=2.2867, 절편=77.8380\n",
            "epoch=1400, 기울기1=1.5015, 기울기2=2.2862, 절편=77.8470\n",
            "epoch=1500, 기울기1=1.5008, 기울기2=2.2860, 절편=77.8518\n",
            "epoch=1600, 기울기1=1.5004, 기울기2=2.2859, 절편=77.8543\n",
            "epoch=1700, 기울기1=1.5002, 기울기2=2.2858, 절편=77.8556\n",
            "epoch=1800, 기울기1=1.5001, 기울기2=2.2858, 절편=77.8563\n",
            "epoch=1900, 기울기1=1.5001, 기울기2=2.2857, 절편=77.8567\n",
            "epoch=2000, 기울기1=1.5000, 기울기2=2.2857, 절편=77.8569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8vUo0-9ERVcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5장 참거짓 판단 : 로지스틱 회"
      ],
      "metadata": {
        "id": "yRrML95tqfnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 시그모이드 함수 y= 1/(1+exp(-(ax+b)))\n",
        "#"
      ],
      "metadata": {
        "id": "7jskbr4nqi7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data= [[2,0],[4,0],[6,0],[8,1],[10,1],[12,1],[14,1]]\n",
        "x_data = [i[0] for i in data]\n",
        "y_data = [i[1] for i in data]"
      ],
      "metadata": {
        "id": "4b47pdcPrjl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgQ17tNzxT3j",
        "outputId": "b081d5b2-8c19-470c-bf93-78c6a23d7f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4, 6, 8, 10, 12, 14]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P7UWUQQxU1Z",
        "outputId": "59b3e0a5-67df-4ff8-8a2b-206c5b48d883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 1, 1, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(x_data, y_data)\n",
        "plt.xlim(0,15)\n",
        "plt.ylim(-.1, 1.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "W5pS9JoFua4e",
        "outputId": "7aa3e1d4-e3e7-44b7-9dee-e923df6c26df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.1, 1.1)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=0\n",
        "b=0\n",
        "lr = 0.05 # 학습률\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "UKXC3G63uhjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.e **(-x))"
      ],
      "metadata": {
        "id": "uiJTW-S_ulQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#경사 하강법 실행\n",
        "for i in range(2001):\n",
        "  for x_data, y_data in data:\n",
        "    a_diff =x_data * (sigmoid(a*x_data+b) - y_data)\n",
        "    b_diff =sigmoid(a* x_data +b) - y_data\n",
        "    a=a-lr*a_diff\n",
        "    b=b-lr*b_diff\n",
        "    if i%1000 == 0:\n",
        "      print(\"epoch=%f, 기울기=%.04f, 절편=%.04f\"% (i,a,b))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGVxTIsOupsH",
        "outputId": "65ff9350-e9cd-47f9-9bb3-a1de0d90cd2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0.000000, 기울기=2.5314, 절편=-17.4485\n",
            "epoch=0.000000, 기울기=2.5313, 절편=-17.4485\n",
            "epoch=0.000000, 기울기=2.5029, 절편=-17.4532\n",
            "epoch=0.000000, 기울기=2.5314, 절편=-17.4497\n",
            "epoch=0.000000, 기울기=2.5316, 절편=-17.4497\n",
            "epoch=0.000000, 기울기=2.5316, 절편=-17.4497\n",
            "epoch=0.000000, 기울기=2.5316, 절편=-17.4497\n",
            "epoch=1000.000000, 기울기=2.6841, 절편=-18.5366\n",
            "epoch=1000.000000, 기울기=2.6840, 절편=-18.5366\n",
            "epoch=1000.000000, 기울기=2.6598, 절편=-18.5407\n",
            "epoch=1000.000000, 기울기=2.6841, 절편=-18.5376\n",
            "epoch=1000.000000, 기울기=2.6842, 절편=-18.5376\n",
            "epoch=1000.000000, 기울기=2.6842, 절편=-18.5376\n",
            "epoch=1000.000000, 기울기=2.6842, 절편=-18.5376\n",
            "epoch=2000.000000, 기울기=2.8159, 절편=-19.4736\n",
            "epoch=2000.000000, 기울기=2.8159, 절편=-19.4736\n",
            "epoch=2000.000000, 기울기=2.7947, 절편=-19.4772\n",
            "epoch=2000.000000, 기울기=2.8159, 절편=-19.4745\n",
            "epoch=2000.000000, 기울기=2.8160, 절편=-19.4745\n",
            "epoch=2000.000000, 기울기=2.8160, 절편=-19.4745\n",
            "epoch=2000.000000, 기울기=2.8160, 절편=-19.4745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQoUdpIBw_oj",
        "outputId": "48e9a7df-b8dd-4676-a769-92759a969e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a #기울기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHIgNZDRxAXx",
        "outputId": "eef5439f-54ec-4489-fec4-ba8f490b5498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.8160309859140518"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b 3 절편"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNtNVDnHxBCU",
        "outputId": "4cef582a-7c28-4f71-fd17-d9d2edb77948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-19.474503873281222"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x_data, y_data)\n",
        "plt.xlim(0,15)\n",
        "plt.ylim(-.1,1.1)\n",
        "x_range = (np.arange(0,15, 0.1))  # 0-15까지 0.1간격으로 만든다\n",
        "plt.plot(np.arange(0,15,0.1), np.array([sigmoid(a*x+b) for x in x_range]))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "iFWiXcjcw3kY",
        "outputId": "faf0d92e-bde6-4899-d60b-9071126041ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.arange(0,15, 0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edizOevFvU0l",
        "outputId": "9b5889e2-37bc-4d2f-ea14-6bf8d4035412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ,\n",
              "        1.1,  1.2,  1.3,  1.4,  1.5,  1.6,  1.7,  1.8,  1.9,  2. ,  2.1,\n",
              "        2.2,  2.3,  2.4,  2.5,  2.6,  2.7,  2.8,  2.9,  3. ,  3.1,  3.2,\n",
              "        3.3,  3.4,  3.5,  3.6,  3.7,  3.8,  3.9,  4. ,  4.1,  4.2,  4.3,\n",
              "        4.4,  4.5,  4.6,  4.7,  4.8,  4.9,  5. ,  5.1,  5.2,  5.3,  5.4,\n",
              "        5.5,  5.6,  5.7,  5.8,  5.9,  6. ,  6.1,  6.2,  6.3,  6.4,  6.5,\n",
              "        6.6,  6.7,  6.8,  6.9,  7. ,  7.1,  7.2,  7.3,  7.4,  7.5,  7.6,\n",
              "        7.7,  7.8,  7.9,  8. ,  8.1,  8.2,  8.3,  8.4,  8.5,  8.6,  8.7,\n",
              "        8.8,  8.9,  9. ,  9.1,  9.2,  9.3,  9.4,  9.5,  9.6,  9.7,  9.8,\n",
              "        9.9, 10. , 10.1, 10.2, 10.3, 10.4, 10.5, 10.6, 10.7, 10.8, 10.9,\n",
              "       11. , 11.1, 11.2, 11.3, 11.4, 11.5, 11.6, 11.7, 11.8, 11.9, 12. ,\n",
              "       12.1, 12.2, 12.3, 12.4, 12.5, 12.6, 12.7, 12.8, 12.9, 13. , 13.1,\n",
              "       13.2, 13.3, 13.4, 13.5, 13.6, 13.7, 13.8, 13.9, 14. , 14.1, 14.2,\n",
              "       14.3, 14.4, 14.5, 14.6, 14.7, 14.8, 14.9])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybgJkLDFwDVg",
        "outputId": "1b44a988-41ee-47d6-f46e-7336c550e3d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNEBLZFdxPNI",
        "outputId": "41bb2a68-4134-4aa2-82e4-c91247ebd2c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wmFf2rzVxPrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7장 다층 퍼셉트론 "
      ],
      "metadata": {
        "id": "F5Z0a4B9xnFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 은닉층을 만들어 공간을 왜곡하면 두영역을 가로지르는 선이 직선으로 바뀌게됨\n",
        "# "
      ],
      "metadata": {
        "id": "wFMw7Z4zyKl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 코딩으로 xor문제 해결하기"
      ],
      "metadata": {
        "id": "zTHHrlPGyJmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#가중치와 바이어스\n",
        "import numpy as np\n",
        "w11= np.array([-2,-2])\n",
        "w12 = np.array([2,2])\n",
        "w2 = np.array([1,1])\n",
        "b1=3\n",
        "b2=-1\n",
        "b3=-1"
      ],
      "metadata": {
        "id": "LOygHD-T9T8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = 0\n",
        "x2 = 0"
      ],
      "metadata": {
        "id": "xM8CecvqxPow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MLP(3,5,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrpPHl7wxmLc",
        "outputId": "d6ac1edb-6c3a-4117-d72f-f4cd281177f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(1,2) * w12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcq_A_PTxyRS",
        "outputId": "a9e255b3-b5f3-46e1-fa0d-25545406c993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum((1,2) * w12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk-og9kox4dj",
        "outputId": "e2fdf95e-21f6-4d66-cdf4-90abdb5f8198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MLP(np.array([x1,x2]) , w11. b1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "3RNgATjQxKf5",
        "outputId": "28fa0dfb-5528-4839-bb40-e43a87bd0172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-1d3731c26ce0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mw11\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'x2'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def MLP(x,w,b):\n",
        "  y=np.sum(w * x)+b\n",
        "  if y<=0:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "  "
      ],
      "metadata": {
        "id": "QfwufLYr9gQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nand 게이트\n",
        "def NAND(x1,x2):\n",
        "  return MLP(np.array([x1,x2]) , w11, b1)\n",
        "\n",
        "# or 게이트\n",
        "def OR(x1,x2):\n",
        "  return MLP(np.array([x1,x2]),w12,b2)\n",
        "\n",
        "# and 게이트\n",
        "def AND(x1,x2):\n",
        "  return MLP(np.array([x1,x2]),w2,b3)\n",
        "\n",
        "# xor 게이트\n",
        "def XOR(x1,x2):\n",
        "  return AND(NAND(x1,x2), OR(x1,x2))"
      ],
      "metadata": {
        "id": "zYp2rKHz9nDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x1,x2값을 번갈아 대입하며 최종값 출력\n",
        "if __name__ == '__main__':\n",
        "  for x in [(0,0),(1,0),(0,1),(1,1)]:\n",
        "    y = XOR(x[0],x[1])\n",
        "    print(\"입력값:\" + str(x) + \"출력값:\" + str(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUrFxMksF-cP",
        "outputId": "66a4eea6-35fc-47df-bf30-af452655c0b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력값:(0, 0)출력값:0\n",
            "입력값:(1, 0)출력값:1\n",
            "입력값:(0, 1)출력값:1\n",
            "입력값:(1, 1)출력값:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QHYO6hYdGeGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8장 오차 역전파 "
      ],
      "metadata": {
        "id": "YWXbilL-_kYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 오차 역전파는 경사하강법의 확장개념임\n",
        "# 환경변수지정 > 신경망 실행 > 결과를 실제값과 비교 > 출력층 가중치 수정 > 은닉층 가중치 수정 > 반복"
      ],
      "metadata": {
        "id": "fIS-kA_7xpUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1HDGqL6jIFeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9장 신경망에서 딥러닝으로 "
      ],
      "metadata": {
        "id": "obCGkzPzIH1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient vanishing : 기울기의 값이 점점 작아져 맨 처음 층까지 전달되지 않는 기울기 소실 문제 발생\n",
        "# 이는 활성화 함수로 사용된 시그모이드 함수의 특성 떄문임\n",
        "# 이를 위해 시그모이드가 아닌 다른 함수들을 대체하기 시작 ex. tanh, relu softplus( log(1+exp(x)) )"
      ],
      "metadata": {
        "id": "b5Ws6qKSIJ3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 속도와 정확도 문제를 해결하는 고급 경사 하강법\n",
        "# 1) sgd ( 확률적 경사 하강법) : 전체 데이터를 전부 사용하는 것이 아니라 랜덤하게 추출한 일부 데이터를 사용. 중간 결과의 진폭이 크고 불안정해 보일수 있음\n",
        "# 그러나 속도가 확연히 빠르면서도 최적해에 근사한 값을 찾아낸다는 장점\n",
        "# keras.optimizer.SGD(lr=0.1)"
      ],
      "metadata": {
        "id": "8NZ3-iFMIxCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) momentum : 관성 탄력  경사하강법에 탄력을 더해주는것 \n",
        "# keras.optimizers.SGD(lr=0.1, momentum =0.9) # 모멘텀 계수 추가"
      ],
      "metadata": {
        "id": "lucb68Z0MbNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3)  nag(네스테로프 모멘텀) : 모멘텀이 이동시킬 방향으로 미리 이동해서 그레디언트를 계산, 불피요한 이동을 줄이는 효과\n",
        "# keras.optimizers.SGD(lr=0.1, momentum =0.9, nesterov=True)"
      ],
      "metadata": {
        "id": "BZC2-uJVMwTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) adagrad: 변수의 업데이트가 잦으면 학습률을 적게하여 이동 보폭을 조절하는 방법, 보폭크기 개선\n",
        "# keras.optimizers.Adagrad(lr=0.1, epsilon =1e-6)"
      ],
      "metadata": {
        "id": "9m12JSZNM6ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) RMSProp : adagrad의 보폭 민감도를 보완\n",
        "# keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon = 1e-08, decay=0.0)"
      ],
      "metadata": {
        "id": "UryIKpvWNGjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Adam : rmsprop + momentum 정확도와 보폭크기 개선\n",
        "# keras.optimizers.Adam(lr=0.01)  # 현재가장 많이 사용"
      ],
      "metadata": {
        "id": "GUL3ixClNQrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZBJT8sX5Ndql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10장 모델 설계하기 "
      ],
      "metadata": {
        "id": "QBt3crD8NeBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import  Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "np.random.seed(3)\n",
        "tf.random.set_seed(3) "
      ],
      "metadata": {
        "id": "S4cA_1dUNg7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_set =np.loadtxt(\"data/ThoraricSurgery.csv\", delimiter =',')"
      ],
      "metadata": {
        "id": "zOHvmNF7OF4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=Data_set[:,0:17]\n",
        "Y=Data_set[:,17]"
      ],
      "metadata": {
        "id": "UzdqmFGYNyhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KKqP9mOPm3Z",
        "outputId": "22a3e39c-497d-4278-e2df-3f4bd18cac34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(470, 17)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M0oDhxfQHlS",
        "outputId": "7ee3b2a0-deca-4c04-b9fb-d4fd02e3b76e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[293.  ,   1.  ,   3.8 , ...,   1.  ,   0.  ,  62.  ],\n",
              "       [  1.  ,   2.  ,   2.88, ...,   1.  ,   0.  ,  60.  ],\n",
              "       [  8.  ,   2.  ,   3.19, ...,   1.  ,   0.  ,  66.  ],\n",
              "       ...,\n",
              "       [406.  ,   6.  ,   5.36, ...,   0.  ,   0.  ,  62.  ],\n",
              "       [ 25.  ,   8.  ,   4.32, ...,   0.  ,   0.  ,  58.  ],\n",
              "       [447.  ,   8.  ,   5.2 , ...,   0.  ,   0.  ,  49.  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xav6tRKBPoMs",
        "outputId": "01ed7e34-cd1d-44b6-bbe5-31021ddd1df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(470,)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LGjp-IOQInF",
        "outputId": "71af12d4-ae49-4f9d-c9d6-9d4cb55fb7b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
              "       0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model =Sequential()  # model 시작을 알림\n",
        "# 두개의 층을 가진 모델 model.add 두줄\n",
        "model.add(Dense(30, input_dim=17, activation ='relu')) # 은닉층\n",
        "model.add(Dense(1, activation='sigmoid')) # 출력층\n",
        "# compile  :  모델이 효과적으로 구현될수 있게 여러가지 환경을 설정해주면서 컴파일 하는 부분\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X,Y, epochs=100, batch_size=10) # EPOCH 100 : 각 샘플이 처음부터 끝까지 100번 재사용될때까지 실행을 반복\n",
        "# BATCH_SIZE 10 : 샘플을 한번에 몇개씩 처리할지를 정하는 부분 470개의 샘플을 10개씩 끊어서 집어넣으라는 뜻\n",
        "# 배치사이즈가 너무 크면 학습속도 느려지고, 너무 작으면 각 실행값의 편차가 생겨 전체 결과값이 불안정해질수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "equ9ChtvONas",
        "outputId": "2299300e-45e1-4c6f-d0a0-70d6aedcde1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.8468\n",
            "Epoch 2/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1487 - accuracy: 0.8511\n",
            "Epoch 3/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1490 - accuracy: 0.8489\n",
            "Epoch 4/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.8511\n",
            "Epoch 5/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1489 - accuracy: 0.8511\n",
            "Epoch 6/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1489 - accuracy: 0.8511\n",
            "Epoch 7/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1488 - accuracy: 0.8511\n",
            "Epoch 8/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1485 - accuracy: 0.8511\n",
            "Epoch 9/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1469 - accuracy: 0.8511\n",
            "Epoch 10/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1462 - accuracy: 0.8532\n",
            "Epoch 11/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1452 - accuracy: 0.8532\n",
            "Epoch 12/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1450 - accuracy: 0.8489\n",
            "Epoch 13/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1460 - accuracy: 0.8532\n",
            "Epoch 14/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1464 - accuracy: 0.8489\n",
            "Epoch 15/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1470 - accuracy: 0.8489\n",
            "Epoch 16/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1486 - accuracy: 0.8511\n",
            "Epoch 17/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.8511\n",
            "Epoch 18/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1463 - accuracy: 0.8511\n",
            "Epoch 19/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.8489\n",
            "Epoch 20/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1459 - accuracy: 0.8532\n",
            "Epoch 21/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1437 - accuracy: 0.8511\n",
            "Epoch 22/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.8511\n",
            "Epoch 23/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.8511\n",
            "Epoch 24/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1455 - accuracy: 0.8489\n",
            "Epoch 25/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.8489\n",
            "Epoch 26/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.8553\n",
            "Epoch 27/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1414 - accuracy: 0.8553\n",
            "Epoch 28/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1449 - accuracy: 0.8532\n",
            "Epoch 29/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1398 - accuracy: 0.8511\n",
            "Epoch 30/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1457 - accuracy: 0.8511\n",
            "Epoch 31/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.8532\n",
            "Epoch 32/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1439 - accuracy: 0.8511\n",
            "Epoch 33/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1407 - accuracy: 0.8489\n",
            "Epoch 34/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1425 - accuracy: 0.8532\n",
            "Epoch 35/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1421 - accuracy: 0.8511\n",
            "Epoch 36/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1469 - accuracy: 0.8511\n",
            "Epoch 37/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1400 - accuracy: 0.8574\n",
            "Epoch 38/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1430 - accuracy: 0.8511\n",
            "Epoch 39/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 0.8553\n",
            "Epoch 40/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1344 - accuracy: 0.8532\n",
            "Epoch 41/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1450 - accuracy: 0.8468\n",
            "Epoch 42/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.8532\n",
            "Epoch 43/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1408 - accuracy: 0.8511\n",
            "Epoch 44/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1385 - accuracy: 0.8532\n",
            "Epoch 45/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1358 - accuracy: 0.8596\n",
            "Epoch 46/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1340 - accuracy: 0.8574\n",
            "Epoch 47/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1435 - accuracy: 0.8489\n",
            "Epoch 48/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1367 - accuracy: 0.8532\n",
            "Epoch 49/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1440 - accuracy: 0.8426\n",
            "Epoch 50/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1486 - accuracy: 0.8511\n",
            "Epoch 51/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1485 - accuracy: 0.8511\n",
            "Epoch 52/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1479 - accuracy: 0.8511\n",
            "Epoch 53/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1427 - accuracy: 0.8532\n",
            "Epoch 54/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1397 - accuracy: 0.8553\n",
            "Epoch 55/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1386 - accuracy: 0.8574\n",
            "Epoch 56/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1359 - accuracy: 0.8532\n",
            "Epoch 57/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1368 - accuracy: 0.8489\n",
            "Epoch 58/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.8404\n",
            "Epoch 59/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.8511\n",
            "Epoch 60/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1336 - accuracy: 0.8532\n",
            "Epoch 61/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1376 - accuracy: 0.8511\n",
            "Epoch 62/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1329 - accuracy: 0.8617\n",
            "Epoch 63/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1409 - accuracy: 0.8511\n",
            "Epoch 64/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.8532\n",
            "Epoch 65/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1397 - accuracy: 0.8532\n",
            "Epoch 66/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1350 - accuracy: 0.8596\n",
            "Epoch 67/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.8532\n",
            "Epoch 68/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1465 - accuracy: 0.8511\n",
            "Epoch 69/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1405 - accuracy: 0.8574\n",
            "Epoch 70/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1390 - accuracy: 0.8532\n",
            "Epoch 71/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1369 - accuracy: 0.8553\n",
            "Epoch 72/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.8574\n",
            "Epoch 73/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1348 - accuracy: 0.8532\n",
            "Epoch 74/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1297 - accuracy: 0.8596\n",
            "Epoch 75/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1446 - accuracy: 0.8468\n",
            "Epoch 76/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1326 - accuracy: 0.8617\n",
            "Epoch 77/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.8553\n",
            "Epoch 78/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.8489\n",
            "Epoch 79/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.8532\n",
            "Epoch 80/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.8596\n",
            "Epoch 81/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1371 - accuracy: 0.8511\n",
            "Epoch 82/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1356 - accuracy: 0.8468\n",
            "Epoch 83/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1337 - accuracy: 0.8511\n",
            "Epoch 84/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1425 - accuracy: 0.8489\n",
            "Epoch 85/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1348 - accuracy: 0.8596\n",
            "Epoch 86/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1425 - accuracy: 0.8468\n",
            "Epoch 87/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1483 - accuracy: 0.8511\n",
            "Epoch 88/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1475 - accuracy: 0.8511\n",
            "Epoch 89/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.8574\n",
            "Epoch 90/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1328 - accuracy: 0.8553\n",
            "Epoch 91/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1285 - accuracy: 0.8574\n",
            "Epoch 92/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1270 - accuracy: 0.8660\n",
            "Epoch 93/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.8532\n",
            "Epoch 94/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1397 - accuracy: 0.8532\n",
            "Epoch 95/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1356 - accuracy: 0.8617\n",
            "Epoch 96/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.8532\n",
            "Epoch 97/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.1293 - accuracy: 0.8553\n",
            "Epoch 98/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1282 - accuracy: 0.8596\n",
            "Epoch 99/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1379 - accuracy: 0.8468\n",
            "Epoch 100/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.1301 - accuracy: 0.8638\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc2454ef150>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 교차 엔트로피 : 분류문제에서 주로 쓰임 \n",
        "# 참거짓 : binary_crossentropy\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X,Y, epochs=100, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glfBWZYONaxM",
        "outputId": "e907cfa3-c8b0-4b91-da29-c9c74b706dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.5286 - accuracy: 0.8298\n",
            "Epoch 2/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.5413 - accuracy: 0.8511\n",
            "Epoch 3/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4860 - accuracy: 0.8447\n",
            "Epoch 4/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.5802 - accuracy: 0.8383\n",
            "Epoch 5/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.5040 - accuracy: 0.8553\n",
            "Epoch 6/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4771 - accuracy: 0.8553\n",
            "Epoch 7/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4930 - accuracy: 0.8489\n",
            "Epoch 8/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4940 - accuracy: 0.8532\n",
            "Epoch 9/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4486 - accuracy: 0.8532\n",
            "Epoch 10/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4834 - accuracy: 0.8553\n",
            "Epoch 11/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4661 - accuracy: 0.8553\n",
            "Epoch 12/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4966 - accuracy: 0.8574\n",
            "Epoch 13/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4712 - accuracy: 0.8468\n",
            "Epoch 14/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.5058 - accuracy: 0.8426\n",
            "Epoch 15/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4779 - accuracy: 0.8553\n",
            "Epoch 16/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4625 - accuracy: 0.8404\n",
            "Epoch 17/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4583 - accuracy: 0.8532\n",
            "Epoch 18/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4765 - accuracy: 0.8596\n",
            "Epoch 19/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4532 - accuracy: 0.8404\n",
            "Epoch 20/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4465 - accuracy: 0.8638\n",
            "Epoch 21/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4233 - accuracy: 0.8553\n",
            "Epoch 22/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4394 - accuracy: 0.8468\n",
            "Epoch 23/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4196 - accuracy: 0.8489\n",
            "Epoch 24/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4748 - accuracy: 0.8340\n",
            "Epoch 25/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4281 - accuracy: 0.8596\n",
            "Epoch 26/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4430 - accuracy: 0.8489\n",
            "Epoch 27/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4631 - accuracy: 0.8340\n",
            "Epoch 28/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4206 - accuracy: 0.8532\n",
            "Epoch 29/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4516 - accuracy: 0.8383\n",
            "Epoch 30/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4048 - accuracy: 0.8447\n",
            "Epoch 31/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4440 - accuracy: 0.8447\n",
            "Epoch 32/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4150 - accuracy: 0.8468\n",
            "Epoch 33/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4687 - accuracy: 0.8489\n",
            "Epoch 34/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4016 - accuracy: 0.8553\n",
            "Epoch 35/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4417 - accuracy: 0.8362\n",
            "Epoch 36/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4289 - accuracy: 0.8426\n",
            "Epoch 37/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4688 - accuracy: 0.8553\n",
            "Epoch 38/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4528 - accuracy: 0.8362\n",
            "Epoch 39/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4238 - accuracy: 0.8553\n",
            "Epoch 40/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4095 - accuracy: 0.8617\n",
            "Epoch 41/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4179 - accuracy: 0.8489\n",
            "Epoch 42/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4917 - accuracy: 0.8255\n",
            "Epoch 43/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4207 - accuracy: 0.8489\n",
            "Epoch 44/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4048 - accuracy: 0.8404\n",
            "Epoch 45/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4113 - accuracy: 0.8511\n",
            "Epoch 46/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.3981 - accuracy: 0.8489\n",
            "Epoch 47/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.8277\n",
            "Epoch 48/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4062 - accuracy: 0.8532\n",
            "Epoch 49/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4042 - accuracy: 0.8489\n",
            "Epoch 50/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.3971 - accuracy: 0.8447\n",
            "Epoch 51/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4021 - accuracy: 0.8574\n",
            "Epoch 52/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4009 - accuracy: 0.8468\n",
            "Epoch 53/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4011 - accuracy: 0.8532\n",
            "Epoch 54/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4019 - accuracy: 0.8532\n",
            "Epoch 55/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4178 - accuracy: 0.8404\n",
            "Epoch 56/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4099 - accuracy: 0.8404\n",
            "Epoch 57/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4099 - accuracy: 0.8553\n",
            "Epoch 58/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4282 - accuracy: 0.8489\n",
            "Epoch 59/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.3990 - accuracy: 0.8383\n",
            "Epoch 60/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.3880 - accuracy: 0.8511\n",
            "Epoch 61/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4002 - accuracy: 0.8617\n",
            "Epoch 62/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4003 - accuracy: 0.8511\n",
            "Epoch 63/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4114 - accuracy: 0.8447\n",
            "Epoch 64/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4337 - accuracy: 0.8468\n",
            "Epoch 65/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.3873 - accuracy: 0.8553\n",
            "Epoch 66/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4341 - accuracy: 0.8362\n",
            "Epoch 67/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.3994 - accuracy: 0.8532\n",
            "Epoch 68/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4136 - accuracy: 0.8574\n",
            "Epoch 69/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4349 - accuracy: 0.8426\n",
            "Epoch 70/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4155 - accuracy: 0.8447\n",
            "Epoch 71/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.3811 - accuracy: 0.8511\n",
            "Epoch 72/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4346 - accuracy: 0.8255\n",
            "Epoch 73/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4770 - accuracy: 0.8447\n",
            "Epoch 74/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4009 - accuracy: 0.8468\n",
            "Epoch 75/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.3984 - accuracy: 0.8638\n",
            "Epoch 76/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4267 - accuracy: 0.8511\n",
            "Epoch 77/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.3936 - accuracy: 0.8468\n",
            "Epoch 78/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4241 - accuracy: 0.8574\n",
            "Epoch 79/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4037 - accuracy: 0.8426\n",
            "Epoch 80/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4004 - accuracy: 0.8511\n",
            "Epoch 81/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4619 - accuracy: 0.8383\n",
            "Epoch 82/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4630 - accuracy: 0.8468\n",
            "Epoch 83/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4235 - accuracy: 0.8489\n",
            "Epoch 84/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.3984 - accuracy: 0.8447\n",
            "Epoch 85/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4105 - accuracy: 0.8426\n",
            "Epoch 86/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.3895 - accuracy: 0.8447\n",
            "Epoch 87/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4043 - accuracy: 0.8447\n",
            "Epoch 88/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.3892 - accuracy: 0.8489\n",
            "Epoch 89/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.3904 - accuracy: 0.8468\n",
            "Epoch 90/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4148 - accuracy: 0.8511\n",
            "Epoch 91/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4630 - accuracy: 0.8319\n",
            "Epoch 92/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4025 - accuracy: 0.8574\n",
            "Epoch 93/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4449 - accuracy: 0.8383\n",
            "Epoch 94/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4198 - accuracy: 0.8511\n",
            "Epoch 95/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.3904 - accuracy: 0.8532\n",
            "Epoch 96/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4727 - accuracy: 0.8319\n",
            "Epoch 97/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4389 - accuracy: 0.8319\n",
            "Epoch 98/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4171 - accuracy: 0.8340\n",
            "Epoch 99/100\n",
            "47/47 [==============================] - 0s 1ms/step - loss: 0.4387 - accuracy: 0.8553\n",
            "Epoch 100/100\n",
            "47/47 [==============================] - 0s 2ms/step - loss: 0.4072 - accuracy: 0.8553\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc24cbdabd0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss선택 가능한것들\n",
        "# 평균 제곱계열 : mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, mean_squared_logarithmic_error\n",
        "# 교차 엔트로피 계열 : binary_crossentrpoy, categorical_crossentropy"
      ],
      "metadata": {
        "id": "m_K14RmlNa0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "freM2wDVQCVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11장 데이터 다루기 "
      ],
      "metadata": {
        "id": "UNljUkkdQhaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 피마 인디언 데이터 분석하기\n",
        "# 피마 인디언의 당뇨병 여부를 예측하는 데이터 PIMA-INDIANS_DABETES"
      ],
      "metadata": {
        "id": "N8cxAHk1QjUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플수 768\n",
        "# 컬럼 9"
      ],
      "metadata": {
        "id": "HS377GlFUiDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('data/pima-indians-diabetes.csv', names=['pregnant','plasma','pressure','thickness','insulin','bmi','pedigree','age','class'])"
      ],
      "metadata": {
        "id": "y8AFIbLkUl0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "I7J2uv89MgjU",
        "outputId": "4332c50f-d9d8-409d-afc2-6c1c5dd56c99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6c7fcd66-f90c-4b9b-832d-909762cb50aa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pregnant</th>\n",
              "      <th>plasma</th>\n",
              "      <th>pressure</th>\n",
              "      <th>thickness</th>\n",
              "      <th>insulin</th>\n",
              "      <th>bmi</th>\n",
              "      <th>pedigree</th>\n",
              "      <th>age</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>10</td>\n",
              "      <td>101</td>\n",
              "      <td>76</td>\n",
              "      <td>48</td>\n",
              "      <td>180</td>\n",
              "      <td>32.9</td>\n",
              "      <td>0.171</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>2</td>\n",
              "      <td>122</td>\n",
              "      <td>70</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>36.8</td>\n",
              "      <td>0.340</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>5</td>\n",
              "      <td>121</td>\n",
              "      <td>72</td>\n",
              "      <td>23</td>\n",
              "      <td>112</td>\n",
              "      <td>26.2</td>\n",
              "      <td>0.245</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>1</td>\n",
              "      <td>126</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.1</td>\n",
              "      <td>0.349</td>\n",
              "      <td>47</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>1</td>\n",
              "      <td>93</td>\n",
              "      <td>70</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>30.4</td>\n",
              "      <td>0.315</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>768 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c7fcd66-f90c-4b9b-832d-909762cb50aa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6c7fcd66-f90c-4b9b-832d-909762cb50aa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6c7fcd66-f90c-4b9b-832d-909762cb50aa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     pregnant  plasma  pressure  thickness  insulin   bmi  pedigree  age  class\n",
              "0           6     148        72         35        0  33.6     0.627   50      1\n",
              "1           1      85        66         29        0  26.6     0.351   31      0\n",
              "2           8     183        64          0        0  23.3     0.672   32      1\n",
              "3           1      89        66         23       94  28.1     0.167   21      0\n",
              "4           0     137        40         35      168  43.1     2.288   33      1\n",
              "..        ...     ...       ...        ...      ...   ...       ...  ...    ...\n",
              "763        10     101        76         48      180  32.9     0.171   63      0\n",
              "764         2     122        70         27        0  36.8     0.340   27      0\n",
              "765         5     121        72         23      112  26.2     0.245   30      0\n",
              "766         1     126        60          0        0  30.1     0.349   47      1\n",
              "767         1      93        70         31        0  30.4     0.315   23      0\n",
              "\n",
              "[768 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw-lFim5Iv86",
        "outputId": "53d855c2-c4cf-407e-f584-dea2f2756235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   pregnant  plasma  pressure  thickness  insulin   bmi  pedigree  age  class\n",
            "0         6     148        72         35        0  33.6     0.627   50      1\n",
            "1         1      85        66         29        0  26.6     0.351   31      0\n",
            "2         8     183        64          0        0  23.3     0.672   32      1\n",
            "3         1      89        66         23       94  28.1     0.167   21      0\n",
            "4         0     137        40         35      168  43.1     2.288   33      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnTffSuTIXFu",
        "outputId": "727e04ca-82b0-4587-df4a-b381c81257a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 768 entries, 0 to 767\n",
            "Data columns (total 9 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   pregnant   768 non-null    int64  \n",
            " 1   plasma     768 non-null    int64  \n",
            " 2   pressure   768 non-null    int64  \n",
            " 3   thickness  768 non-null    int64  \n",
            " 4   insulin    768 non-null    int64  \n",
            " 5   bmi        768 non-null    float64\n",
            " 6   pedigree   768 non-null    float64\n",
            " 7   age        768 non-null    int64  \n",
            " 8   class      768 non-null    int64  \n",
            "dtypes: float64(2), int64(7)\n",
            "memory usage: 54.1 KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isE1bRx9U_RW",
        "outputId": "367a7eee-39f9-42d9-b32a-ebdf841f9752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         pregnant      plasma    pressure  ...    pedigree         age       class\n",
            "count  768.000000  768.000000  768.000000  ...  768.000000  768.000000  768.000000\n",
            "mean     3.845052  120.894531   69.105469  ...    0.471876   33.240885    0.348958\n",
            "std      3.369578   31.972618   19.355807  ...    0.331329   11.760232    0.476951\n",
            "min      0.000000    0.000000    0.000000  ...    0.078000   21.000000    0.000000\n",
            "25%      1.000000   99.000000   62.000000  ...    0.243750   24.000000    0.000000\n",
            "50%      3.000000  117.000000   72.000000  ...    0.372500   29.000000    0.000000\n",
            "75%      6.000000  140.250000   80.000000  ...    0.626250   41.000000    1.000000\n",
            "max     17.000000  199.000000  122.000000  ...    2.420000   81.000000    1.000000\n",
            "\n",
            "[8 rows x 9 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['pregnant','class']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAiKw-qnVJGZ",
        "outputId": "1e0ec470-753d-4782-dcaf-1188e8e10648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     pregnant  class\n",
            "0           6      1\n",
            "1           1      0\n",
            "2           8      1\n",
            "3           1      0\n",
            "4           0      1\n",
            "..        ...    ...\n",
            "763        10      0\n",
            "764         2      0\n",
            "765         5      0\n",
            "766         1      1\n",
            "767         1      0\n",
            "\n",
            "[768 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 가공하기\n",
        "print(df[['pregnant','class']].groupby(['pregnant'], as_index=False).mean().sort_values(by='pregnant', ascending=True))\n",
        "# as_index=False : pregnant정보옆에 인덱스를 만들지 여부\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "re0R4N9mHvR_",
        "outputId": "e720998b-efa8-4987-b5b6-c54cb4ca15f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    pregnant     class\n",
            "0          0  0.342342\n",
            "1          1  0.214815\n",
            "2          2  0.184466\n",
            "3          3  0.360000\n",
            "4          4  0.338235\n",
            "5          5  0.368421\n",
            "6          6  0.320000\n",
            "7          7  0.555556\n",
            "8          8  0.578947\n",
            "9          9  0.642857\n",
            "10        10  0.416667\n",
            "11        11  0.636364\n",
            "12        12  0.444444\n",
            "13        13  0.500000\n",
            "14        14  1.000000\n",
            "15        15  1.000000\n",
            "16        17  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# matplotlib를 이용해 그래프 그리기"
      ],
      "metadata": {
        "id": "v_8Y2gddVbVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "U_3YsszlVzD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "EiUIhLdMV3Ya",
        "outputId": "aafcbe67-10d8-4a22-e586-472516859ef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 1440x1440 with 0 Axes>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x1440 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "MaltLLHWWIBK",
        "outputId": "e88502e5-038b-4628-d05f-932e7aaab0bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8b105837-ab7b-4985-ba36-556b3a18f2fc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pregnant</th>\n",
              "      <th>plasma</th>\n",
              "      <th>pressure</th>\n",
              "      <th>thickness</th>\n",
              "      <th>insulin</th>\n",
              "      <th>bmi</th>\n",
              "      <th>pedigree</th>\n",
              "      <th>age</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>pregnant</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.129459</td>\n",
              "      <td>0.141282</td>\n",
              "      <td>-0.081672</td>\n",
              "      <td>-0.073535</td>\n",
              "      <td>0.017683</td>\n",
              "      <td>-0.033523</td>\n",
              "      <td>0.544341</td>\n",
              "      <td>0.221898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>plasma</th>\n",
              "      <td>0.129459</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.152590</td>\n",
              "      <td>0.057328</td>\n",
              "      <td>0.331357</td>\n",
              "      <td>0.221071</td>\n",
              "      <td>0.137337</td>\n",
              "      <td>0.263514</td>\n",
              "      <td>0.466581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pressure</th>\n",
              "      <td>0.141282</td>\n",
              "      <td>0.152590</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.207371</td>\n",
              "      <td>0.088933</td>\n",
              "      <td>0.281805</td>\n",
              "      <td>0.041265</td>\n",
              "      <td>0.239528</td>\n",
              "      <td>0.065068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thickness</th>\n",
              "      <td>-0.081672</td>\n",
              "      <td>0.057328</td>\n",
              "      <td>0.207371</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.436783</td>\n",
              "      <td>0.392573</td>\n",
              "      <td>0.183928</td>\n",
              "      <td>-0.113970</td>\n",
              "      <td>0.074752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>insulin</th>\n",
              "      <td>-0.073535</td>\n",
              "      <td>0.331357</td>\n",
              "      <td>0.088933</td>\n",
              "      <td>0.436783</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.197859</td>\n",
              "      <td>0.185071</td>\n",
              "      <td>-0.042163</td>\n",
              "      <td>0.130548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bmi</th>\n",
              "      <td>0.017683</td>\n",
              "      <td>0.221071</td>\n",
              "      <td>0.281805</td>\n",
              "      <td>0.392573</td>\n",
              "      <td>0.197859</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.140647</td>\n",
              "      <td>0.036242</td>\n",
              "      <td>0.292695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pedigree</th>\n",
              "      <td>-0.033523</td>\n",
              "      <td>0.137337</td>\n",
              "      <td>0.041265</td>\n",
              "      <td>0.183928</td>\n",
              "      <td>0.185071</td>\n",
              "      <td>0.140647</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.033561</td>\n",
              "      <td>0.173844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>0.544341</td>\n",
              "      <td>0.263514</td>\n",
              "      <td>0.239528</td>\n",
              "      <td>-0.113970</td>\n",
              "      <td>-0.042163</td>\n",
              "      <td>0.036242</td>\n",
              "      <td>0.033561</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.238356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <td>0.221898</td>\n",
              "      <td>0.466581</td>\n",
              "      <td>0.065068</td>\n",
              "      <td>0.074752</td>\n",
              "      <td>0.130548</td>\n",
              "      <td>0.292695</td>\n",
              "      <td>0.173844</td>\n",
              "      <td>0.238356</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b105837-ab7b-4985-ba36-556b3a18f2fc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8b105837-ab7b-4985-ba36-556b3a18f2fc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8b105837-ab7b-4985-ba36-556b3a18f2fc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           pregnant    plasma  pressure  ...  pedigree       age     class\n",
              "pregnant   1.000000  0.129459  0.141282  ... -0.033523  0.544341  0.221898\n",
              "plasma     0.129459  1.000000  0.152590  ...  0.137337  0.263514  0.466581\n",
              "pressure   0.141282  0.152590  1.000000  ...  0.041265  0.239528  0.065068\n",
              "thickness -0.081672  0.057328  0.207371  ...  0.183928 -0.113970  0.074752\n",
              "insulin   -0.073535  0.331357  0.088933  ...  0.185071 -0.042163  0.130548\n",
              "bmi        0.017683  0.221071  0.281805  ...  0.140647  0.036242  0.292695\n",
              "pedigree  -0.033523  0.137337  0.041265  ...  1.000000  0.033561  0.173844\n",
              "age        0.544341  0.263514  0.239528  ...  0.033561  1.000000  0.238356\n",
              "class      0.221898  0.466581  0.065068  ...  0.173844  0.238356  1.000000\n",
              "\n",
              "[9 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(df.corr(), linewidths=0.1, vmax=0.5, cmap = plt.cm.gist_heat, annot=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "BjmyRkJ6V6Sm",
        "outputId": "8ddfc25e-4986-4f00-fbf5-90a0bbb6c4d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc2379e1690>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid = sns.FacetGrid(df, col='class')\n",
        "grid.map(plt.hist, 'plasma', bins=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "nDL6yE-CWB-4",
        "outputId": "2c3a232b-02fe-4b9f-e2bc-caeae4f584b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x216 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 모델\n",
        "dataset = np.loadtxt('data/pima-indians-diabetes.csv', delimiter=\",\")\n",
        "np.random.seed(3)\n",
        "tf.random.set_seed(3)\n",
        "x=dataset[:,0:8]\n",
        "y=dataset[:,8]"
      ],
      "metadata": {
        "id": "J4Akxk2CWvDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(12,input_dim=8, activation ='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "oGGhAbv-XEMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "model.fit(x,y, epochs=200, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "247jwqX6XV37",
        "outputId": "228df7f0-bb66-4850-e715-62211a76aa17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "77/77 [==============================] - 1s 1ms/step - loss: 10.5440 - accuracy: 0.6159\n",
            "Epoch 2/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 5.4367 - accuracy: 0.6029\n",
            "Epoch 3/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 2.9292 - accuracy: 0.5208\n",
            "Epoch 4/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 1.5346 - accuracy: 0.5208\n",
            "Epoch 5/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.8896 - accuracy: 0.5000\n",
            "Epoch 6/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.8072 - accuracy: 0.5234\n",
            "Epoch 7/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7655 - accuracy: 0.6549\n",
            "Epoch 8/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.7306 - accuracy: 0.6628\n",
            "Epoch 9/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6943 - accuracy: 0.6706\n",
            "Epoch 10/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6656 - accuracy: 0.6758\n",
            "Epoch 11/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6390 - accuracy: 0.6797\n",
            "Epoch 12/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6198 - accuracy: 0.6823\n",
            "Epoch 13/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6072 - accuracy: 0.6979\n",
            "Epoch 14/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6112 - accuracy: 0.6966\n",
            "Epoch 15/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5993 - accuracy: 0.7005\n",
            "Epoch 16/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5894 - accuracy: 0.6940\n",
            "Epoch 17/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5915 - accuracy: 0.7057\n",
            "Epoch 18/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5854 - accuracy: 0.7122\n",
            "Epoch 19/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5873 - accuracy: 0.6992\n",
            "Epoch 20/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5820 - accuracy: 0.7070\n",
            "Epoch 21/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5838 - accuracy: 0.6810\n",
            "Epoch 22/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5759 - accuracy: 0.7109\n",
            "Epoch 23/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5752 - accuracy: 0.7148\n",
            "Epoch 24/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5767 - accuracy: 0.7070\n",
            "Epoch 25/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5737 - accuracy: 0.7122\n",
            "Epoch 26/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5664 - accuracy: 0.7161\n",
            "Epoch 27/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5801 - accuracy: 0.7057\n",
            "Epoch 28/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5755 - accuracy: 0.7174\n",
            "Epoch 29/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5654 - accuracy: 0.7161\n",
            "Epoch 30/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5635 - accuracy: 0.7188\n",
            "Epoch 31/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5602 - accuracy: 0.7122\n",
            "Epoch 32/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5683 - accuracy: 0.7109\n",
            "Epoch 33/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5661 - accuracy: 0.7018\n",
            "Epoch 34/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5578 - accuracy: 0.7214\n",
            "Epoch 35/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5563 - accuracy: 0.7305\n",
            "Epoch 36/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5657 - accuracy: 0.7122\n",
            "Epoch 37/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5529 - accuracy: 0.7070\n",
            "Epoch 38/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5537 - accuracy: 0.7227\n",
            "Epoch 39/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5552 - accuracy: 0.7174\n",
            "Epoch 40/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5539 - accuracy: 0.7292\n",
            "Epoch 41/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5523 - accuracy: 0.7253\n",
            "Epoch 42/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5533 - accuracy: 0.7227\n",
            "Epoch 43/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5535 - accuracy: 0.7201\n",
            "Epoch 44/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5433 - accuracy: 0.7240\n",
            "Epoch 45/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5648 - accuracy: 0.7122\n",
            "Epoch 46/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5478 - accuracy: 0.7161\n",
            "Epoch 47/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5480 - accuracy: 0.7266\n",
            "Epoch 48/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5413 - accuracy: 0.7318\n",
            "Epoch 49/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5410 - accuracy: 0.7253\n",
            "Epoch 50/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5396 - accuracy: 0.7318\n",
            "Epoch 51/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5349 - accuracy: 0.7279\n",
            "Epoch 52/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5345 - accuracy: 0.7383\n",
            "Epoch 53/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5354 - accuracy: 0.7240\n",
            "Epoch 54/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5387 - accuracy: 0.7188\n",
            "Epoch 55/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7253\n",
            "Epoch 56/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5412 - accuracy: 0.7174\n",
            "Epoch 57/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5511 - accuracy: 0.7240\n",
            "Epoch 58/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5298 - accuracy: 0.7331\n",
            "Epoch 59/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5324 - accuracy: 0.7344\n",
            "Epoch 60/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5280 - accuracy: 0.7292\n",
            "Epoch 61/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5304 - accuracy: 0.7292\n",
            "Epoch 62/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5373 - accuracy: 0.7214\n",
            "Epoch 63/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5283 - accuracy: 0.7357\n",
            "Epoch 64/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5234 - accuracy: 0.7396\n",
            "Epoch 65/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5239 - accuracy: 0.7383\n",
            "Epoch 66/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5215 - accuracy: 0.7344\n",
            "Epoch 67/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5193 - accuracy: 0.7396\n",
            "Epoch 68/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5241 - accuracy: 0.7357\n",
            "Epoch 69/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5224 - accuracy: 0.7370\n",
            "Epoch 70/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5183 - accuracy: 0.7383\n",
            "Epoch 71/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5164 - accuracy: 0.7344\n",
            "Epoch 72/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5182 - accuracy: 0.7357\n",
            "Epoch 73/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5178 - accuracy: 0.7292\n",
            "Epoch 74/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5187 - accuracy: 0.7396\n",
            "Epoch 75/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5240 - accuracy: 0.7344\n",
            "Epoch 76/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5209 - accuracy: 0.7370\n",
            "Epoch 77/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5177 - accuracy: 0.7331\n",
            "Epoch 78/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5169 - accuracy: 0.7370\n",
            "Epoch 79/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5225 - accuracy: 0.7318\n",
            "Epoch 80/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7435\n",
            "Epoch 81/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7422\n",
            "Epoch 82/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5087 - accuracy: 0.7461\n",
            "Epoch 83/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.7344\n",
            "Epoch 84/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5120 - accuracy: 0.7305\n",
            "Epoch 85/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5088 - accuracy: 0.7461\n",
            "Epoch 86/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5046 - accuracy: 0.7513\n",
            "Epoch 87/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5061 - accuracy: 0.7500\n",
            "Epoch 88/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5073 - accuracy: 0.7500\n",
            "Epoch 89/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5080 - accuracy: 0.7474\n",
            "Epoch 90/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5111 - accuracy: 0.7357\n",
            "Epoch 91/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5293 - accuracy: 0.7305\n",
            "Epoch 92/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5144 - accuracy: 0.7513\n",
            "Epoch 93/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5008 - accuracy: 0.7513\n",
            "Epoch 94/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5034 - accuracy: 0.7461\n",
            "Epoch 95/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7448\n",
            "Epoch 96/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4999 - accuracy: 0.7318\n",
            "Epoch 97/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4978 - accuracy: 0.7552\n",
            "Epoch 98/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5050 - accuracy: 0.7344\n",
            "Epoch 99/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5071 - accuracy: 0.7500\n",
            "Epoch 100/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5004 - accuracy: 0.7513\n",
            "Epoch 101/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5015 - accuracy: 0.7448\n",
            "Epoch 102/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4960 - accuracy: 0.7526\n",
            "Epoch 103/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5054 - accuracy: 0.7409\n",
            "Epoch 104/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4959 - accuracy: 0.7552\n",
            "Epoch 105/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4943 - accuracy: 0.7487\n",
            "Epoch 106/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5006 - accuracy: 0.7526\n",
            "Epoch 107/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5023 - accuracy: 0.7539\n",
            "Epoch 108/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5055 - accuracy: 0.7526\n",
            "Epoch 109/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4918 - accuracy: 0.7487\n",
            "Epoch 110/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5006 - accuracy: 0.7578\n",
            "Epoch 111/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4911 - accuracy: 0.7617\n",
            "Epoch 112/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4916 - accuracy: 0.7500\n",
            "Epoch 113/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4930 - accuracy: 0.7539\n",
            "Epoch 114/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4937 - accuracy: 0.7578\n",
            "Epoch 115/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4932 - accuracy: 0.7565\n",
            "Epoch 116/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4913 - accuracy: 0.7474\n",
            "Epoch 117/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4969 - accuracy: 0.7539\n",
            "Epoch 118/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4920 - accuracy: 0.7474\n",
            "Epoch 119/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5022 - accuracy: 0.7552\n",
            "Epoch 120/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4908 - accuracy: 0.7461\n",
            "Epoch 121/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4847 - accuracy: 0.7526\n",
            "Epoch 122/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4915 - accuracy: 0.7461\n",
            "Epoch 123/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4918 - accuracy: 0.7461\n",
            "Epoch 124/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4932 - accuracy: 0.7487\n",
            "Epoch 125/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4904 - accuracy: 0.7539\n",
            "Epoch 126/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.7630\n",
            "Epoch 127/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4960 - accuracy: 0.7578\n",
            "Epoch 128/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4880 - accuracy: 0.7591\n",
            "Epoch 129/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4992 - accuracy: 0.7331\n",
            "Epoch 130/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4868 - accuracy: 0.7565\n",
            "Epoch 131/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4831 - accuracy: 0.7695\n",
            "Epoch 132/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4854 - accuracy: 0.7552\n",
            "Epoch 133/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4851 - accuracy: 0.7643\n",
            "Epoch 134/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4904 - accuracy: 0.7513\n",
            "Epoch 135/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4940 - accuracy: 0.7500\n",
            "Epoch 136/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.7435\n",
            "Epoch 137/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4884 - accuracy: 0.7539\n",
            "Epoch 138/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4964 - accuracy: 0.7539\n",
            "Epoch 139/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4841 - accuracy: 0.7578\n",
            "Epoch 140/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4849 - accuracy: 0.7695\n",
            "Epoch 141/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4792 - accuracy: 0.7539\n",
            "Epoch 142/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.7591\n",
            "Epoch 143/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4916 - accuracy: 0.7604\n",
            "Epoch 144/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.7578\n",
            "Epoch 145/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4941 - accuracy: 0.7526\n",
            "Epoch 146/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4922 - accuracy: 0.7604\n",
            "Epoch 147/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4797 - accuracy: 0.7513\n",
            "Epoch 148/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4808 - accuracy: 0.7513\n",
            "Epoch 149/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.7578\n",
            "Epoch 150/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4795 - accuracy: 0.7591\n",
            "Epoch 151/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4826 - accuracy: 0.7630\n",
            "Epoch 152/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4801 - accuracy: 0.7513\n",
            "Epoch 153/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4805 - accuracy: 0.7604\n",
            "Epoch 154/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4812 - accuracy: 0.7656\n",
            "Epoch 155/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4752 - accuracy: 0.7591\n",
            "Epoch 156/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4727 - accuracy: 0.7734\n",
            "Epoch 157/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4836 - accuracy: 0.7539\n",
            "Epoch 158/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4739 - accuracy: 0.7695\n",
            "Epoch 159/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4784 - accuracy: 0.7656\n",
            "Epoch 160/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4852 - accuracy: 0.7513\n",
            "Epoch 161/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4800 - accuracy: 0.7630\n",
            "Epoch 162/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4807 - accuracy: 0.7565\n",
            "Epoch 163/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4775 - accuracy: 0.7643\n",
            "Epoch 164/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4781 - accuracy: 0.7643\n",
            "Epoch 165/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4844 - accuracy: 0.7526\n",
            "Epoch 166/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4726 - accuracy: 0.7734\n",
            "Epoch 167/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4676 - accuracy: 0.7695\n",
            "Epoch 168/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4768 - accuracy: 0.7643\n",
            "Epoch 169/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4706 - accuracy: 0.7643\n",
            "Epoch 170/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4778 - accuracy: 0.7656\n",
            "Epoch 171/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4660 - accuracy: 0.7630\n",
            "Epoch 172/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4719 - accuracy: 0.7656\n",
            "Epoch 173/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4760 - accuracy: 0.7734\n",
            "Epoch 174/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4660 - accuracy: 0.7630\n",
            "Epoch 175/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4746 - accuracy: 0.7786\n",
            "Epoch 176/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4698 - accuracy: 0.7591\n",
            "Epoch 177/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4734 - accuracy: 0.7695\n",
            "Epoch 178/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4700 - accuracy: 0.7643\n",
            "Epoch 179/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4757 - accuracy: 0.7695\n",
            "Epoch 180/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4655 - accuracy: 0.7552\n",
            "Epoch 181/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4737 - accuracy: 0.7708\n",
            "Epoch 182/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4818 - accuracy: 0.7682\n",
            "Epoch 183/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4652 - accuracy: 0.7539\n",
            "Epoch 184/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4645 - accuracy: 0.7734\n",
            "Epoch 185/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4666 - accuracy: 0.7682\n",
            "Epoch 186/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4669 - accuracy: 0.7721\n",
            "Epoch 187/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4782 - accuracy: 0.7643\n",
            "Epoch 188/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4758 - accuracy: 0.7721\n",
            "Epoch 189/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4657 - accuracy: 0.7760\n",
            "Epoch 190/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4687 - accuracy: 0.7760\n",
            "Epoch 191/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.7669\n",
            "Epoch 192/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.7565\n",
            "Epoch 193/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4743 - accuracy: 0.7799\n",
            "Epoch 194/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4765 - accuracy: 0.7695\n",
            "Epoch 195/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4664 - accuracy: 0.7682\n",
            "Epoch 196/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4741 - accuracy: 0.7669\n",
            "Epoch 197/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4734 - accuracy: 0.7747\n",
            "Epoch 198/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4668 - accuracy: 0.7578\n",
            "Epoch 199/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4679 - accuracy: 0.7760\n",
            "Epoch 200/200\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4662 - accuracy: 0.7721\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc236bf1c50>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n accuracy :%.4f\"% (model.evaluate(x,y)[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx2N9HUdXjNE",
        "outputId": "4418dc87-f806-4b43-f4ef-00b3b1e83a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 0s 1ms/step - loss: 0.4587 - accuracy: 0.7708\n",
            "\n",
            " accuracy :0.7708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nFxYgYkUYrbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12장 다중 분류 문제 "
      ],
      "metadata": {
        "id": "T1xe6tP8lZA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset 폴더안에 iris.csv파일 넣기"
      ],
      "metadata": {
        "id": "lE5jljRslbDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('dataset/iris.csv', names=['sepal_length','sepal_wiedth','petal_length','petal_width','species'])\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQaSVTvIlmpg",
        "outputId": "48d49073-68ce-49a8-9ff1-ea129c66ccdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sepal_length  sepal_wiedth  petal_length  petal_width      species\n",
            "0           5.1           3.5           1.4          0.2  Iris-setosa\n",
            "1           4.9           3.0           1.4          0.2  Iris-setosa\n",
            "2           4.7           3.2           1.3          0.2  Iris-setosa\n",
            "3           4.6           3.1           1.5          0.2  Iris-setosa\n",
            "4           5.0           3.6           1.4          0.2  Iris-setosa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.pairplot(df, hue='species')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "id": "8DZ-YmLIl6wn",
        "outputId": "13921974-346e-423e-ae3f-3203278855e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 823.25x720 with 20 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 원핫 인코딩\n",
        "dataset= df.values"
      ],
      "metadata": {
        "id": "FojlQxfDmdKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNZP3CKt3uuv",
        "outputId": "9729bb7d-bb26-4537-ec3b-e84d638822d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hnB4ZXYmmwm",
        "outputId": "0048430f-5736-471d-a498-c33f694e4334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2, 'Iris-setosa'],\n",
              "       [4.9, 3.0, 1.4, 0.2, 'Iris-setosa'],\n",
              "       [4.7, 3.2, 1.3, 0.2, 'Iris-setosa'],\n",
              "       [4.6, 3.1, 1.5, 0.2, 'Iris-setosa'],\n",
              "       [5.0, 3.6, 1.4, 0.2, 'Iris-setosa'],\n",
              "       [5.4, 3.9, 1.7, 0.4, 'Iris-setosa'],\n",
              "       [4.6, 3.4, 1.4, 0.3, 'Iris-setosa'],\n",
              "       [5.0, 3.4, 1.5, 0.2, 'Iris-setosa'],\n",
              "       [4.4, 2.9, 1.4, 0.2, 'Iris-setosa'],\n",
              "       [4.9, 3.1, 1.5, 0.1, 'Iris-setosa'],\n",
              "       [5.4, 3.7, 1.5, 0.2, 'Iris-setosa'],\n",
              "       [4.8, 3.4, 1.6, 0.2, 'Iris-setosa'],\n",
              "       [4.8, 3.0, 1.4, 0.1, 'Iris-setosa'],\n",
              "       [4.3, 3.0, 1.1, 0.1, 'Iris-setosa'],\n",
              "       [5.8, 4.0, 1.2, 0.2, 'Iris-setosa'],\n",
              "       [5.7, 4.4, 1.5, 0.4, 'Iris-setosa'],\n",
              "       [5.4, 3.9, 1.3, 0.4, 'Iris-setosa'],\n",
              "       [5.1, 3.5, 1.4, 0.3, 'Iris-setosa'],\n",
              "       [5.7, 3.8, 1.7, 0.3, 'Iris-setosa'],\n",
              "       [5.1, 3.8, 1.5, 0.3, 'Iris-setosa'],\n",
              "       [5.4, 3.4, 1.7, 0.2, 'Iris-setosa'],\n",
              "       [5.1, 3.7, 1.5, 0.4, 'Iris-setosa'],\n",
              "       [4.6, 3.6, 1.0, 0.2, 'Iris-setosa'],\n",
              "       [5.1, 3.3, 1.7, 0.5, 'Iris-setosa'],\n",
              "       [4.8, 3.4, 1.9, 0.2, 'Iris-setosa'],\n",
              "       [5.0, 3.0, 1.6, 0.2, 'Iris-setosa'],\n",
              "       [5.0, 3.4, 1.6, 0.4, 'Iris-setosa'],\n",
              "       [5.2, 3.5, 1.5, 0.2, 'Iris-setosa'],\n",
              "       [5.2, 3.4, 1.4, 0.2, 'Iris-setosa'],\n",
              "       [4.7, 3.2, 1.6, 0.2, 'Iris-setosa'],\n",
              "       [4.8, 3.1, 1.6, 0.2, 'Iris-setosa'],\n",
              "       [5.4, 3.4, 1.5, 0.4, 'Iris-setosa'],\n",
              "       [5.2, 4.1, 1.5, 0.1, 'Iris-setosa'],\n",
              "       [5.5, 4.2, 1.4, 0.2, 'Iris-setosa'],\n",
              "       [4.9, 3.1, 1.5, 0.1, 'Iris-setosa'],\n",
              "       [5.0, 3.2, 1.2, 0.2, 'Iris-setosa'],\n",
              "       [5.5, 3.5, 1.3, 0.2, 'Iris-setosa'],\n",
              "       [4.9, 3.1, 1.5, 0.1, 'Iris-setosa'],\n",
              "       [4.4, 3.0, 1.3, 0.2, 'Iris-setosa'],\n",
              "       [5.1, 3.4, 1.5, 0.2, 'Iris-setosa'],\n",
              "       [5.0, 3.5, 1.3, 0.3, 'Iris-setosa'],\n",
              "       [4.5, 2.3, 1.3, 0.3, 'Iris-setosa'],\n",
              "       [4.4, 3.2, 1.3, 0.2, 'Iris-setosa'],\n",
              "       [5.0, 3.5, 1.6, 0.6, 'Iris-setosa'],\n",
              "       [5.1, 3.8, 1.9, 0.4, 'Iris-setosa'],\n",
              "       [4.8, 3.0, 1.4, 0.3, 'Iris-setosa'],\n",
              "       [5.1, 3.8, 1.6, 0.2, 'Iris-setosa'],\n",
              "       [4.6, 3.2, 1.4, 0.2, 'Iris-setosa'],\n",
              "       [5.3, 3.7, 1.5, 0.2, 'Iris-setosa'],\n",
              "       [5.0, 3.3, 1.4, 0.2, 'Iris-setosa'],\n",
              "       [7.0, 3.2, 4.7, 1.4, 'Iris-versicolor'],\n",
              "       [6.4, 3.2, 4.5, 1.5, 'Iris-versicolor'],\n",
              "       [6.9, 3.1, 4.9, 1.5, 'Iris-versicolor'],\n",
              "       [5.5, 2.3, 4.0, 1.3, 'Iris-versicolor'],\n",
              "       [6.5, 2.8, 4.6, 1.5, 'Iris-versicolor'],\n",
              "       [5.7, 2.8, 4.5, 1.3, 'Iris-versicolor'],\n",
              "       [6.3, 3.3, 4.7, 1.6, 'Iris-versicolor'],\n",
              "       [4.9, 2.4, 3.3, 1.0, 'Iris-versicolor'],\n",
              "       [6.6, 2.9, 4.6, 1.3, 'Iris-versicolor'],\n",
              "       [5.2, 2.7, 3.9, 1.4, 'Iris-versicolor'],\n",
              "       [5.0, 2.0, 3.5, 1.0, 'Iris-versicolor'],\n",
              "       [5.9, 3.0, 4.2, 1.5, 'Iris-versicolor'],\n",
              "       [6.0, 2.2, 4.0, 1.0, 'Iris-versicolor'],\n",
              "       [6.1, 2.9, 4.7, 1.4, 'Iris-versicolor'],\n",
              "       [5.6, 2.9, 3.6, 1.3, 'Iris-versicolor'],\n",
              "       [6.7, 3.1, 4.4, 1.4, 'Iris-versicolor'],\n",
              "       [5.6, 3.0, 4.5, 1.5, 'Iris-versicolor'],\n",
              "       [5.8, 2.7, 4.1, 1.0, 'Iris-versicolor'],\n",
              "       [6.2, 2.2, 4.5, 1.5, 'Iris-versicolor'],\n",
              "       [5.6, 2.5, 3.9, 1.1, 'Iris-versicolor'],\n",
              "       [5.9, 3.2, 4.8, 1.8, 'Iris-versicolor'],\n",
              "       [6.1, 2.8, 4.0, 1.3, 'Iris-versicolor'],\n",
              "       [6.3, 2.5, 4.9, 1.5, 'Iris-versicolor'],\n",
              "       [6.1, 2.8, 4.7, 1.2, 'Iris-versicolor'],\n",
              "       [6.4, 2.9, 4.3, 1.3, 'Iris-versicolor'],\n",
              "       [6.6, 3.0, 4.4, 1.4, 'Iris-versicolor'],\n",
              "       [6.8, 2.8, 4.8, 1.4, 'Iris-versicolor'],\n",
              "       [6.7, 3.0, 5.0, 1.7, 'Iris-versicolor'],\n",
              "       [6.0, 2.9, 4.5, 1.5, 'Iris-versicolor'],\n",
              "       [5.7, 2.6, 3.5, 1.0, 'Iris-versicolor'],\n",
              "       [5.5, 2.4, 3.8, 1.1, 'Iris-versicolor'],\n",
              "       [5.5, 2.4, 3.7, 1.0, 'Iris-versicolor'],\n",
              "       [5.8, 2.7, 3.9, 1.2, 'Iris-versicolor'],\n",
              "       [6.0, 2.7, 5.1, 1.6, 'Iris-versicolor'],\n",
              "       [5.4, 3.0, 4.5, 1.5, 'Iris-versicolor'],\n",
              "       [6.0, 3.4, 4.5, 1.6, 'Iris-versicolor'],\n",
              "       [6.7, 3.1, 4.7, 1.5, 'Iris-versicolor'],\n",
              "       [6.3, 2.3, 4.4, 1.3, 'Iris-versicolor'],\n",
              "       [5.6, 3.0, 4.1, 1.3, 'Iris-versicolor'],\n",
              "       [5.5, 2.5, 4.0, 1.3, 'Iris-versicolor'],\n",
              "       [5.5, 2.6, 4.4, 1.2, 'Iris-versicolor'],\n",
              "       [6.1, 3.0, 4.6, 1.4, 'Iris-versicolor'],\n",
              "       [5.8, 2.6, 4.0, 1.2, 'Iris-versicolor'],\n",
              "       [5.0, 2.3, 3.3, 1.0, 'Iris-versicolor'],\n",
              "       [5.6, 2.7, 4.2, 1.3, 'Iris-versicolor'],\n",
              "       [5.7, 3.0, 4.2, 1.2, 'Iris-versicolor'],\n",
              "       [5.7, 2.9, 4.2, 1.3, 'Iris-versicolor'],\n",
              "       [6.2, 2.9, 4.3, 1.3, 'Iris-versicolor'],\n",
              "       [5.1, 2.5, 3.0, 1.1, 'Iris-versicolor'],\n",
              "       [5.7, 2.8, 4.1, 1.3, 'Iris-versicolor'],\n",
              "       [6.3, 3.3, 6.0, 2.5, 'Iris-virginica'],\n",
              "       [5.8, 2.7, 5.1, 1.9, 'Iris-virginica'],\n",
              "       [7.1, 3.0, 5.9, 2.1, 'Iris-virginica'],\n",
              "       [6.3, 2.9, 5.6, 1.8, 'Iris-virginica'],\n",
              "       [6.5, 3.0, 5.8, 2.2, 'Iris-virginica'],\n",
              "       [7.6, 3.0, 6.6, 2.1, 'Iris-virginica'],\n",
              "       [4.9, 2.5, 4.5, 1.7, 'Iris-virginica'],\n",
              "       [7.3, 2.9, 6.3, 1.8, 'Iris-virginica'],\n",
              "       [6.7, 2.5, 5.8, 1.8, 'Iris-virginica'],\n",
              "       [7.2, 3.6, 6.1, 2.5, 'Iris-virginica'],\n",
              "       [6.5, 3.2, 5.1, 2.0, 'Iris-virginica'],\n",
              "       [6.4, 2.7, 5.3, 1.9, 'Iris-virginica'],\n",
              "       [6.8, 3.0, 5.5, 2.1, 'Iris-virginica'],\n",
              "       [5.7, 2.5, 5.0, 2.0, 'Iris-virginica'],\n",
              "       [5.8, 2.8, 5.1, 2.4, 'Iris-virginica'],\n",
              "       [6.4, 3.2, 5.3, 2.3, 'Iris-virginica'],\n",
              "       [6.5, 3.0, 5.5, 1.8, 'Iris-virginica'],\n",
              "       [7.7, 3.8, 6.7, 2.2, 'Iris-virginica'],\n",
              "       [7.7, 2.6, 6.9, 2.3, 'Iris-virginica'],\n",
              "       [6.0, 2.2, 5.0, 1.5, 'Iris-virginica'],\n",
              "       [6.9, 3.2, 5.7, 2.3, 'Iris-virginica'],\n",
              "       [5.6, 2.8, 4.9, 2.0, 'Iris-virginica'],\n",
              "       [7.7, 2.8, 6.7, 2.0, 'Iris-virginica'],\n",
              "       [6.3, 2.7, 4.9, 1.8, 'Iris-virginica'],\n",
              "       [6.7, 3.3, 5.7, 2.1, 'Iris-virginica'],\n",
              "       [7.2, 3.2, 6.0, 1.8, 'Iris-virginica'],\n",
              "       [6.2, 2.8, 4.8, 1.8, 'Iris-virginica'],\n",
              "       [6.1, 3.0, 4.9, 1.8, 'Iris-virginica'],\n",
              "       [6.4, 2.8, 5.6, 2.1, 'Iris-virginica'],\n",
              "       [7.2, 3.0, 5.8, 1.6, 'Iris-virginica'],\n",
              "       [7.4, 2.8, 6.1, 1.9, 'Iris-virginica'],\n",
              "       [7.9, 3.8, 6.4, 2.0, 'Iris-virginica'],\n",
              "       [6.4, 2.8, 5.6, 2.2, 'Iris-virginica'],\n",
              "       [6.3, 2.8, 5.1, 1.5, 'Iris-virginica'],\n",
              "       [6.1, 2.6, 5.6, 1.4, 'Iris-virginica'],\n",
              "       [7.7, 3.0, 6.1, 2.3, 'Iris-virginica'],\n",
              "       [6.3, 3.4, 5.6, 2.4, 'Iris-virginica'],\n",
              "       [6.4, 3.1, 5.5, 1.8, 'Iris-virginica'],\n",
              "       [6.0, 3.0, 4.8, 1.8, 'Iris-virginica'],\n",
              "       [6.9, 3.1, 5.4, 2.1, 'Iris-virginica'],\n",
              "       [6.7, 3.1, 5.6, 2.4, 'Iris-virginica'],\n",
              "       [6.9, 3.1, 5.1, 2.3, 'Iris-virginica'],\n",
              "       [5.8, 2.7, 5.1, 1.9, 'Iris-virginica'],\n",
              "       [6.8, 3.2, 5.9, 2.3, 'Iris-virginica'],\n",
              "       [6.7, 3.3, 5.7, 2.5, 'Iris-virginica'],\n",
              "       [6.7, 3.0, 5.2, 2.3, 'Iris-virginica'],\n",
              "       [6.3, 2.5, 5.0, 1.9, 'Iris-virginica'],\n",
              "       [6.5, 3.0, 5.2, 2.0, 'Iris-virginica'],\n",
              "       [6.2, 3.4, 5.4, 2.3, 'Iris-virginica'],\n",
              "       [5.9, 3.0, 5.1, 1.8, 'Iris-virginica']], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=dataset[:,0:4].astype(float)\n",
        "Y_obj = dataset[:,4]"
      ],
      "metadata": {
        "id": "_JbOIVlomojU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder#문자열을 숫자로 바꿔주는 함수\n",
        "e= LabelEncoder()\n",
        "e.fit(Y_obj)\n",
        "Y=e.transform(Y_obj)"
      ],
      "metadata": {
        "id": "gIYrjvNwmwms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDIduNzFm9eS",
        "outputId": "d2c74e43-7edf-49bf-aae7-4b6b6efc4644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8qSpz5qm97U",
        "outputId": "50d4e107-ed3d-419a-c785-1d41a3febdf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install np_utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnxfHxY2xsQQ",
        "outputId": "0eb97884-ea7f-49ac-cdab-97c77d0931b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting np_utils\n",
            "  Downloading np_utils-0.6.0.tar.gz (61 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 20 kB 32.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 30 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 40 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 51 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 61 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 604 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from np_utils) (1.19.5)\n",
            "Building wheels for collected packages: np-utils\n",
            "  Building wheel for np-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for np-utils: filename=np_utils-0.6.0-py3-none-any.whl size=56459 sha256=615ff717f2c0e2a3fc42c742f40becbc7d0581df3e96137f5eeb34fcabc36600\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/83/71/a781667865955ae7dc18e5a4038401deb56d96eb85d3a5f1c0\n",
            "Successfully built np-utils\n",
            "Installing collected packages: np-utils\n",
            "Successfully installed np-utils-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from tensorflow.keras.utils import np_utils \n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n",
        "Y_encoded = to_categorical(Y)"
      ],
      "metadata": {
        "id": "lEOKkQH1m-tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlM37f_5xjUh",
        "outputId": "9153d2d8-2565-43d6-f7b5-b25a83749f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=4, activation='relu'))\n",
        "model.add(Dense(3,activation = 'softmax'))   # 범주가 3개이상이니깐 softmax\n"
      ],
      "metadata": {
        "id": "iIs_zJZfxkBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(3)\n",
        "tf.random.set_seed(3)\n",
        "model.compile(loss='categorical_crossentropy',optimizer ='adam', metrics=['accuracy'])\n",
        "model.fit(X,Y_encoded, epochs=50, batch_size=1) \n",
        "# batch_size 1 인것으로보아 146개의 품종을 한번씩 테스트한 결과 146개의 품종을 정확히 맞히는 확률"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6m4VATyyC-x",
        "outputId": "7d623353-e695-4042-96a5-43b7649d473c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "150/150 [==============================] - 1s 1ms/step - loss: 0.9965 - accuracy: 0.3467\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.7693 - accuracy: 0.6267\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.6275 - accuracy: 0.8200\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.5452 - accuracy: 0.8200\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4886 - accuracy: 0.8533\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4432 - accuracy: 0.8933\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4096 - accuracy: 0.9200\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3911 - accuracy: 0.9133\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3686 - accuracy: 0.9400\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3492 - accuracy: 0.9467\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3288 - accuracy: 0.9333\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3093 - accuracy: 0.9267\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3042 - accuracy: 0.9600\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2833 - accuracy: 0.9333\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2698 - accuracy: 0.9667\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2596 - accuracy: 0.9333\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2499 - accuracy: 0.9533\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2469 - accuracy: 0.9533\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2277 - accuracy: 0.9533\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2185 - accuracy: 0.9667\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2094 - accuracy: 0.9667\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2050 - accuracy: 0.9600\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1936 - accuracy: 0.9667\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1905 - accuracy: 0.9667\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1786 - accuracy: 0.9733\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1722 - accuracy: 0.9733\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1699 - accuracy: 0.9800\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.9933\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1607 - accuracy: 0.9667\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1563 - accuracy: 0.9667\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1493 - accuracy: 0.9733\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1453 - accuracy: 0.9600\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1401 - accuracy: 0.9733\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1361 - accuracy: 0.9600\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1341 - accuracy: 0.9667\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1326 - accuracy: 0.9667\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1229 - accuracy: 0.9667\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1272 - accuracy: 0.9667\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1175 - accuracy: 0.9800\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9600\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9733\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9800\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.9733\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.9733\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.9533\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9800\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1081 - accuracy: 0.9600\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.9667\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.9667\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9533\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb96890cd90>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n accuracy: %.4f\" % (model.evaluate(X,Y_encoded)[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1d6qY4hyZuD",
        "outputId": "5e163578-9a58-4982-fdd7-d3f08e460456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 2ms/step - loss: 0.0969 - accuracy: 0.9733\n",
            "\n",
            " accuracy: 0.9733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rffPQElXyxLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13장 과적합 "
      ],
      "metadata": {
        "id": "vgT4xj9QzHu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#  sonar 데이터  dataset 폴더에 넣기"
      ],
      "metadata": {
        "id": "Kx7vvp8AzJed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('dataset/sonar.csv', header=None)"
      ],
      "metadata": {
        "id": "IJpQriu00Fwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLAuOfKf0L9w",
        "outputId": "f431346e-4cf9-4cfa-eb3b-e4ba4481fd3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 208 entries, 0 to 207\n",
            "Data columns (total 61 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       208 non-null    float64\n",
            " 1   1       208 non-null    float64\n",
            " 2   2       208 non-null    float64\n",
            " 3   3       208 non-null    float64\n",
            " 4   4       208 non-null    float64\n",
            " 5   5       208 non-null    float64\n",
            " 6   6       208 non-null    float64\n",
            " 7   7       208 non-null    float64\n",
            " 8   8       208 non-null    float64\n",
            " 9   9       208 non-null    float64\n",
            " 10  10      208 non-null    float64\n",
            " 11  11      208 non-null    float64\n",
            " 12  12      208 non-null    float64\n",
            " 13  13      208 non-null    float64\n",
            " 14  14      208 non-null    float64\n",
            " 15  15      208 non-null    float64\n",
            " 16  16      208 non-null    float64\n",
            " 17  17      208 non-null    float64\n",
            " 18  18      208 non-null    float64\n",
            " 19  19      208 non-null    float64\n",
            " 20  20      208 non-null    float64\n",
            " 21  21      208 non-null    float64\n",
            " 22  22      208 non-null    float64\n",
            " 23  23      208 non-null    float64\n",
            " 24  24      208 non-null    float64\n",
            " 25  25      208 non-null    float64\n",
            " 26  26      208 non-null    float64\n",
            " 27  27      208 non-null    float64\n",
            " 28  28      208 non-null    float64\n",
            " 29  29      208 non-null    float64\n",
            " 30  30      208 non-null    float64\n",
            " 31  31      208 non-null    float64\n",
            " 32  32      208 non-null    float64\n",
            " 33  33      208 non-null    float64\n",
            " 34  34      208 non-null    float64\n",
            " 35  35      208 non-null    float64\n",
            " 36  36      208 non-null    float64\n",
            " 37  37      208 non-null    float64\n",
            " 38  38      208 non-null    float64\n",
            " 39  39      208 non-null    float64\n",
            " 40  40      208 non-null    float64\n",
            " 41  41      208 non-null    float64\n",
            " 42  42      208 non-null    float64\n",
            " 43  43      208 non-null    float64\n",
            " 44  44      208 non-null    float64\n",
            " 45  45      208 non-null    float64\n",
            " 46  46      208 non-null    float64\n",
            " 47  47      208 non-null    float64\n",
            " 48  48      208 non-null    float64\n",
            " 49  49      208 non-null    float64\n",
            " 50  50      208 non-null    float64\n",
            " 51  51      208 non-null    float64\n",
            " 52  52      208 non-null    float64\n",
            " 53  53      208 non-null    float64\n",
            " 54  54      208 non-null    float64\n",
            " 55  55      208 non-null    float64\n",
            " 56  56      208 non-null    float64\n",
            " 57  57      208 non-null    float64\n",
            " 58  58      208 non-null    float64\n",
            " 59  59      208 non-null    float64\n",
            " 60  60      208 non-null    object \n",
            "dtypes: float64(60), object(1)\n",
            "memory usage: 99.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#샘플의 개수는 208개 , 컬럼수가 61개\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "iWNb_0Nl0OVg",
        "outputId": "e1e0b7de-4180-4449-d9ba-83a652823268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2dcd5013-4b86-4336-a5b7-b4a123de0795\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.0371</td>\n",
              "      <td>0.0428</td>\n",
              "      <td>0.0207</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0986</td>\n",
              "      <td>0.1539</td>\n",
              "      <td>0.1601</td>\n",
              "      <td>0.3109</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.1609</td>\n",
              "      <td>0.1582</td>\n",
              "      <td>0.2238</td>\n",
              "      <td>0.0645</td>\n",
              "      <td>0.0660</td>\n",
              "      <td>0.2273</td>\n",
              "      <td>0.3100</td>\n",
              "      <td>0.2999</td>\n",
              "      <td>0.5078</td>\n",
              "      <td>0.4797</td>\n",
              "      <td>0.5783</td>\n",
              "      <td>0.5071</td>\n",
              "      <td>0.4328</td>\n",
              "      <td>0.5550</td>\n",
              "      <td>0.6711</td>\n",
              "      <td>0.6415</td>\n",
              "      <td>0.7104</td>\n",
              "      <td>0.8080</td>\n",
              "      <td>0.6791</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>0.1307</td>\n",
              "      <td>0.2604</td>\n",
              "      <td>0.5121</td>\n",
              "      <td>0.7547</td>\n",
              "      <td>0.8537</td>\n",
              "      <td>0.8507</td>\n",
              "      <td>0.6692</td>\n",
              "      <td>0.6097</td>\n",
              "      <td>0.4943</td>\n",
              "      <td>0.2744</td>\n",
              "      <td>0.0510</td>\n",
              "      <td>0.2834</td>\n",
              "      <td>0.2825</td>\n",
              "      <td>0.4256</td>\n",
              "      <td>0.2641</td>\n",
              "      <td>0.1386</td>\n",
              "      <td>0.1051</td>\n",
              "      <td>0.1343</td>\n",
              "      <td>0.0383</td>\n",
              "      <td>0.0324</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0159</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0167</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>0.0843</td>\n",
              "      <td>0.0689</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.2583</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.3481</td>\n",
              "      <td>0.3337</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>0.4918</td>\n",
              "      <td>0.6552</td>\n",
              "      <td>0.6919</td>\n",
              "      <td>0.7797</td>\n",
              "      <td>0.7464</td>\n",
              "      <td>0.9444</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8874</td>\n",
              "      <td>0.8024</td>\n",
              "      <td>0.7818</td>\n",
              "      <td>0.5212</td>\n",
              "      <td>0.4052</td>\n",
              "      <td>0.3957</td>\n",
              "      <td>0.3914</td>\n",
              "      <td>0.3250</td>\n",
              "      <td>0.3200</td>\n",
              "      <td>0.3271</td>\n",
              "      <td>0.2767</td>\n",
              "      <td>0.4423</td>\n",
              "      <td>0.2028</td>\n",
              "      <td>0.3788</td>\n",
              "      <td>0.2947</td>\n",
              "      <td>0.1984</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.1306</td>\n",
              "      <td>0.4182</td>\n",
              "      <td>0.3835</td>\n",
              "      <td>0.1057</td>\n",
              "      <td>0.1840</td>\n",
              "      <td>0.1970</td>\n",
              "      <td>0.1674</td>\n",
              "      <td>0.0583</td>\n",
              "      <td>0.1401</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.0621</td>\n",
              "      <td>0.0203</td>\n",
              "      <td>0.0530</td>\n",
              "      <td>0.0742</td>\n",
              "      <td>0.0409</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0125</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0.0191</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>0.1099</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0974</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>0.3771</td>\n",
              "      <td>0.5598</td>\n",
              "      <td>0.6194</td>\n",
              "      <td>0.6333</td>\n",
              "      <td>0.7060</td>\n",
              "      <td>0.5544</td>\n",
              "      <td>0.5320</td>\n",
              "      <td>0.6479</td>\n",
              "      <td>0.6931</td>\n",
              "      <td>0.6759</td>\n",
              "      <td>0.7551</td>\n",
              "      <td>0.8929</td>\n",
              "      <td>0.8619</td>\n",
              "      <td>0.7974</td>\n",
              "      <td>0.6737</td>\n",
              "      <td>0.4293</td>\n",
              "      <td>0.3648</td>\n",
              "      <td>0.5331</td>\n",
              "      <td>0.2413</td>\n",
              "      <td>0.5070</td>\n",
              "      <td>0.8533</td>\n",
              "      <td>0.6036</td>\n",
              "      <td>0.8514</td>\n",
              "      <td>0.8512</td>\n",
              "      <td>0.5045</td>\n",
              "      <td>0.1862</td>\n",
              "      <td>0.2709</td>\n",
              "      <td>0.4232</td>\n",
              "      <td>0.3043</td>\n",
              "      <td>0.6116</td>\n",
              "      <td>0.6756</td>\n",
              "      <td>0.5375</td>\n",
              "      <td>0.4719</td>\n",
              "      <td>0.4647</td>\n",
              "      <td>0.2587</td>\n",
              "      <td>0.2129</td>\n",
              "      <td>0.2222</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.0176</td>\n",
              "      <td>0.1348</td>\n",
              "      <td>0.0744</td>\n",
              "      <td>0.0130</td>\n",
              "      <td>0.0106</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0078</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1276</td>\n",
              "      <td>0.0598</td>\n",
              "      <td>0.1264</td>\n",
              "      <td>0.0881</td>\n",
              "      <td>0.1992</td>\n",
              "      <td>0.0184</td>\n",
              "      <td>0.2261</td>\n",
              "      <td>0.1729</td>\n",
              "      <td>0.2131</td>\n",
              "      <td>0.0693</td>\n",
              "      <td>0.2281</td>\n",
              "      <td>0.4060</td>\n",
              "      <td>0.3973</td>\n",
              "      <td>0.2741</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>0.5556</td>\n",
              "      <td>0.4846</td>\n",
              "      <td>0.3140</td>\n",
              "      <td>0.5334</td>\n",
              "      <td>0.5256</td>\n",
              "      <td>0.2520</td>\n",
              "      <td>0.2090</td>\n",
              "      <td>0.3559</td>\n",
              "      <td>0.6260</td>\n",
              "      <td>0.7340</td>\n",
              "      <td>0.6120</td>\n",
              "      <td>0.3497</td>\n",
              "      <td>0.3953</td>\n",
              "      <td>0.3012</td>\n",
              "      <td>0.5408</td>\n",
              "      <td>0.8814</td>\n",
              "      <td>0.9857</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>0.6121</td>\n",
              "      <td>0.5006</td>\n",
              "      <td>0.3210</td>\n",
              "      <td>0.3202</td>\n",
              "      <td>0.4295</td>\n",
              "      <td>0.3654</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>0.1576</td>\n",
              "      <td>0.0681</td>\n",
              "      <td>0.0294</td>\n",
              "      <td>0.0241</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0117</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0762</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.0394</td>\n",
              "      <td>0.0590</td>\n",
              "      <td>0.0649</td>\n",
              "      <td>0.1209</td>\n",
              "      <td>0.2467</td>\n",
              "      <td>0.3564</td>\n",
              "      <td>0.4459</td>\n",
              "      <td>0.4152</td>\n",
              "      <td>0.3952</td>\n",
              "      <td>0.4256</td>\n",
              "      <td>0.4135</td>\n",
              "      <td>0.4528</td>\n",
              "      <td>0.5326</td>\n",
              "      <td>0.7306</td>\n",
              "      <td>0.6193</td>\n",
              "      <td>0.2032</td>\n",
              "      <td>0.4636</td>\n",
              "      <td>0.4148</td>\n",
              "      <td>0.4292</td>\n",
              "      <td>0.5730</td>\n",
              "      <td>0.5399</td>\n",
              "      <td>0.3161</td>\n",
              "      <td>0.2285</td>\n",
              "      <td>0.6995</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.7262</td>\n",
              "      <td>0.4724</td>\n",
              "      <td>0.5103</td>\n",
              "      <td>0.5459</td>\n",
              "      <td>0.2881</td>\n",
              "      <td>0.0981</td>\n",
              "      <td>0.1951</td>\n",
              "      <td>0.4181</td>\n",
              "      <td>0.4604</td>\n",
              "      <td>0.3217</td>\n",
              "      <td>0.2828</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.1979</td>\n",
              "      <td>0.2444</td>\n",
              "      <td>0.1847</td>\n",
              "      <td>0.0841</td>\n",
              "      <td>0.0692</td>\n",
              "      <td>0.0528</td>\n",
              "      <td>0.0357</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.0046</td>\n",
              "      <td>0.0156</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0107</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2dcd5013-4b86-4336-a5b7-b4a123de0795')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2dcd5013-4b86-4336-a5b7-b4a123de0795 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2dcd5013-4b86-4336-a5b7-b4a123de0795');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       0       1       2       3       4   ...      56      57      58      59  60\n",
              "0  0.0200  0.0371  0.0428  0.0207  0.0954  ...  0.0180  0.0084  0.0090  0.0032   R\n",
              "1  0.0453  0.0523  0.0843  0.0689  0.1183  ...  0.0140  0.0049  0.0052  0.0044   R\n",
              "2  0.0262  0.0582  0.1099  0.1083  0.0974  ...  0.0316  0.0164  0.0095  0.0078   R\n",
              "3  0.0100  0.0171  0.0623  0.0205  0.0205  ...  0.0050  0.0044  0.0040  0.0117   R\n",
              "4  0.0762  0.0666  0.0481  0.0394  0.0590  ...  0.0072  0.0048  0.0107  0.0094   R\n",
              "\n",
              "[5 rows x 61 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "\n",
        "numpy.random.seed(3)\n",
        "tf.random.set_seed(3)\n",
        "\n"
      ],
      "metadata": {
        "id": "WgguYng00Vue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = df.values # numpy.ndarray\n",
        "X=dataset[:,0:60].astype(float) # 60컬럼까지\n",
        "Y_obj = dataset[:,60] # 61컬럼"
      ],
      "metadata": {
        "id": "cWNmFaeF0rqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cX0kSFK3UMq",
        "outputId": "ff3e604a-b1d9-4ac9-99af-1fb45cd6823c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02  , 0.0371, 0.0428, ..., 0.0084, 0.009 , 0.0032],\n",
              "       [0.0453, 0.0523, 0.0843, ..., 0.0049, 0.0052, 0.0044],\n",
              "       [0.0262, 0.0582, 0.1099, ..., 0.0164, 0.0095, 0.0078],\n",
              "       ...,\n",
              "       [0.0522, 0.0437, 0.018 , ..., 0.0138, 0.0077, 0.0031],\n",
              "       [0.0303, 0.0353, 0.049 , ..., 0.0079, 0.0036, 0.0048],\n",
              "       [0.026 , 0.0363, 0.0136, ..., 0.0036, 0.0061, 0.0115]])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e68BXBd423-s",
        "outputId": "9e2b0876-2130-4dff-8370-ff2e283d392e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자열 변환\n",
        "e= LabelEncoder()\n",
        "e.fit(Y_obj)  # Y_obj 장착 \n",
        "Y=e.transform(Y_obj)  #변환"
      ],
      "metadata": {
        "id": "GmzGnbjb0yis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtb9JZWL1joE",
        "outputId": "5e8ee3d4-5ead-4663-bfd2-3fe09ea76c8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(24, input_dim=60, activation ='relu'))\n",
        "model.add(Dense(10, activation ='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "8ceEKBnT07Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCNJYsF53GA2",
        "outputId": "59f3e02d-33a5-444b-980e-d41b909486dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 24)                1464      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                250       \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,725\n",
            "Trainable params: 1,725\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer ='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Qy1kShoG1KQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, Y, epochs=200, batch_size=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wGPku8j1YeL",
        "outputId": "31e5355a-9385-4706-9897-e23275e1c44b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "42/42 [==============================] - 1s 2ms/step - loss: 0.6795 - accuracy: 0.5913\n",
            "Epoch 2/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.6517 - accuracy: 0.6250\n",
            "Epoch 3/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.6328 - accuracy: 0.6587\n",
            "Epoch 4/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.5994 - accuracy: 0.7067\n",
            "Epoch 5/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.5664 - accuracy: 0.7644\n",
            "Epoch 6/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.5420 - accuracy: 0.7500\n",
            "Epoch 7/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7644\n",
            "Epoch 8/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.4829 - accuracy: 0.7740\n",
            "Epoch 9/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.4707 - accuracy: 0.7788\n",
            "Epoch 10/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.4490 - accuracy: 0.7788\n",
            "Epoch 11/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.4448 - accuracy: 0.8029\n",
            "Epoch 12/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.4182 - accuracy: 0.7981\n",
            "Epoch 13/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.4082 - accuracy: 0.8221\n",
            "Epoch 14/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.4029 - accuracy: 0.8077\n",
            "Epoch 15/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.3934 - accuracy: 0.7933\n",
            "Epoch 16/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8317\n",
            "Epoch 17/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.3793 - accuracy: 0.8413\n",
            "Epoch 18/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.3734 - accuracy: 0.8125\n",
            "Epoch 19/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.3787 - accuracy: 0.8413\n",
            "Epoch 20/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.3660 - accuracy: 0.8125\n",
            "Epoch 21/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.3573 - accuracy: 0.8365\n",
            "Epoch 22/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.3578 - accuracy: 0.8558\n",
            "Epoch 23/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.3419 - accuracy: 0.8510\n",
            "Epoch 24/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.3388 - accuracy: 0.8606\n",
            "Epoch 25/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.3367 - accuracy: 0.8510\n",
            "Epoch 26/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.3287 - accuracy: 0.8510\n",
            "Epoch 27/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.3406 - accuracy: 0.8413\n",
            "Epoch 28/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.3336 - accuracy: 0.8654\n",
            "Epoch 29/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.3196 - accuracy: 0.8606\n",
            "Epoch 30/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.3097 - accuracy: 0.8750\n",
            "Epoch 31/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.3032 - accuracy: 0.8654\n",
            "Epoch 32/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.3231 - accuracy: 0.8510\n",
            "Epoch 33/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2880 - accuracy: 0.8894\n",
            "Epoch 34/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.3018 - accuracy: 0.8462\n",
            "Epoch 35/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2927 - accuracy: 0.8750\n",
            "Epoch 36/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2884 - accuracy: 0.8654\n",
            "Epoch 37/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2824 - accuracy: 0.8990\n",
            "Epoch 38/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2861 - accuracy: 0.8846\n",
            "Epoch 39/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2645 - accuracy: 0.9038\n",
            "Epoch 40/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2625 - accuracy: 0.8990\n",
            "Epoch 41/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2716 - accuracy: 0.8894\n",
            "Epoch 42/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2572 - accuracy: 0.9038\n",
            "Epoch 43/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2678 - accuracy: 0.8846\n",
            "Epoch 44/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2736 - accuracy: 0.8942\n",
            "Epoch 45/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2565 - accuracy: 0.8894\n",
            "Epoch 46/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2363 - accuracy: 0.9327\n",
            "Epoch 47/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2366 - accuracy: 0.9279\n",
            "Epoch 48/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2417 - accuracy: 0.9183\n",
            "Epoch 49/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2414 - accuracy: 0.9231\n",
            "Epoch 50/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2376 - accuracy: 0.9279\n",
            "Epoch 51/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2237 - accuracy: 0.9135\n",
            "Epoch 52/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2195 - accuracy: 0.9375\n",
            "Epoch 53/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2340 - accuracy: 0.9038\n",
            "Epoch 54/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2180 - accuracy: 0.9327\n",
            "Epoch 55/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2205 - accuracy: 0.9087\n",
            "Epoch 56/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2139 - accuracy: 0.9231\n",
            "Epoch 57/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2017 - accuracy: 0.9375\n",
            "Epoch 58/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2030 - accuracy: 0.9231\n",
            "Epoch 59/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2060 - accuracy: 0.9279\n",
            "Epoch 60/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2093 - accuracy: 0.9231\n",
            "Epoch 61/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1930 - accuracy: 0.9231\n",
            "Epoch 62/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2053 - accuracy: 0.9038\n",
            "Epoch 63/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1835 - accuracy: 0.9567\n",
            "Epoch 64/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1813 - accuracy: 0.9423\n",
            "Epoch 65/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1737 - accuracy: 0.9519\n",
            "Epoch 66/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1747 - accuracy: 0.9471\n",
            "Epoch 67/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1833 - accuracy: 0.9279\n",
            "Epoch 68/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1665 - accuracy: 0.9471\n",
            "Epoch 69/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1693 - accuracy: 0.9471\n",
            "Epoch 70/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1587 - accuracy: 0.9567\n",
            "Epoch 71/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1605 - accuracy: 0.9471\n",
            "Epoch 72/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1563 - accuracy: 0.9615\n",
            "Epoch 73/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1564 - accuracy: 0.9519\n",
            "Epoch 74/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1568 - accuracy: 0.9615\n",
            "Epoch 75/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1577 - accuracy: 0.9471\n",
            "Epoch 76/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1534 - accuracy: 0.9423\n",
            "Epoch 77/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1501 - accuracy: 0.9519\n",
            "Epoch 78/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9663\n",
            "Epoch 79/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1482 - accuracy: 0.9471\n",
            "Epoch 80/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1305 - accuracy: 0.9567\n",
            "Epoch 81/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1334 - accuracy: 0.9471\n",
            "Epoch 82/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1357 - accuracy: 0.9567\n",
            "Epoch 83/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1252 - accuracy: 0.9615\n",
            "Epoch 84/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1269 - accuracy: 0.9471\n",
            "Epoch 85/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1247 - accuracy: 0.9567\n",
            "Epoch 86/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1225 - accuracy: 0.9567\n",
            "Epoch 87/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1137 - accuracy: 0.9663\n",
            "Epoch 88/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1556 - accuracy: 0.9327\n",
            "Epoch 89/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1131 - accuracy: 0.9712\n",
            "Epoch 90/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1113 - accuracy: 0.9615\n",
            "Epoch 91/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1100 - accuracy: 0.9663\n",
            "Epoch 92/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1097 - accuracy: 0.9519\n",
            "Epoch 93/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1060 - accuracy: 0.9663\n",
            "Epoch 94/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1030 - accuracy: 0.9760\n",
            "Epoch 95/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0988 - accuracy: 0.9808\n",
            "Epoch 96/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1002 - accuracy: 0.9663\n",
            "Epoch 97/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1106 - accuracy: 0.9615\n",
            "Epoch 98/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1007 - accuracy: 0.9663\n",
            "Epoch 99/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0988 - accuracy: 0.9663\n",
            "Epoch 100/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0928 - accuracy: 0.9808\n",
            "Epoch 101/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0901 - accuracy: 0.9808\n",
            "Epoch 102/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0868 - accuracy: 0.9808\n",
            "Epoch 103/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0878 - accuracy: 0.9712\n",
            "Epoch 104/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0996 - accuracy: 0.9712\n",
            "Epoch 105/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0831 - accuracy: 0.9856\n",
            "Epoch 106/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0924 - accuracy: 0.9712\n",
            "Epoch 107/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0797 - accuracy: 0.9760\n",
            "Epoch 108/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0795 - accuracy: 0.9808\n",
            "Epoch 109/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0759 - accuracy: 0.9760\n",
            "Epoch 110/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0791 - accuracy: 0.9856\n",
            "Epoch 111/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0845 - accuracy: 0.9615\n",
            "Epoch 112/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0692 - accuracy: 0.9808\n",
            "Epoch 113/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0720 - accuracy: 0.9808\n",
            "Epoch 114/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0700 - accuracy: 0.9856\n",
            "Epoch 115/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0647 - accuracy: 0.9856\n",
            "Epoch 116/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0643 - accuracy: 0.9808\n",
            "Epoch 117/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0769 - accuracy: 0.9712\n",
            "Epoch 118/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0721 - accuracy: 0.9760\n",
            "Epoch 119/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0682 - accuracy: 0.9760\n",
            "Epoch 120/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0616 - accuracy: 0.9856\n",
            "Epoch 121/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0556 - accuracy: 0.9952\n",
            "Epoch 122/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0555 - accuracy: 0.9904\n",
            "Epoch 123/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0561 - accuracy: 0.9856\n",
            "Epoch 124/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0560 - accuracy: 0.9952\n",
            "Epoch 125/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0464 - accuracy: 0.9952\n",
            "Epoch 126/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0551 - accuracy: 0.9904\n",
            "Epoch 127/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0569 - accuracy: 0.9904\n",
            "Epoch 128/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0506 - accuracy: 0.9856\n",
            "Epoch 129/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0482 - accuracy: 0.9904\n",
            "Epoch 130/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0439 - accuracy: 0.9904\n",
            "Epoch 131/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0447 - accuracy: 0.9952\n",
            "Epoch 132/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0413 - accuracy: 0.9952\n",
            "Epoch 133/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0436 - accuracy: 0.9904\n",
            "Epoch 134/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0429 - accuracy: 0.9952\n",
            "Epoch 135/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0375 - accuracy: 0.9952\n",
            "Epoch 136/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0357 - accuracy: 0.9952\n",
            "Epoch 137/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0355 - accuracy: 0.9952\n",
            "Epoch 138/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0353 - accuracy: 0.9952\n",
            "Epoch 139/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0341 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0391 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0355 - accuracy: 0.9952\n",
            "Epoch 142/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0356 - accuracy: 0.9952\n",
            "Epoch 143/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0314 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0345 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0399 - accuracy: 0.9904\n",
            "Epoch 146/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0301 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0306 - accuracy: 0.9952\n",
            "Epoch 149/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.9952\n",
            "Epoch 150/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0259 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0237 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0243 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0234 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 0.9952\n",
            "Epoch 155/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0210 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0245 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0224 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0194 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 0.9952\n",
            "Epoch 163/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0295 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 0.9952\n",
            "Epoch 165/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0175 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0166 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0240 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0453 - accuracy: 0.9904\n",
            "Epoch 178/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.9952\n",
            "Epoch 179/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0086 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb967dcb390>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정말 100%의 정확도가 맞을까? 과적합이 아닐까?\n"
      ],
      "metadata": {
        "id": "d76sReRU3LMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습셋과 테스트셋을 나눔"
      ],
      "metadata": {
        "id": "CgfoJ5WP4HVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size =0.3)"
      ],
      "metadata": {
        "id": "b7e61unw1cWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(24, input_dim=60, activation ='relu'))\n",
        "model.add(Dense(10, activation ='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer ='adam',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X_train,Y_train, epochs=130, batch_size=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K3wa4RT6X6n",
        "outputId": "3d7529a9-334f-417d-a714-cdd2f4ec062b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.7224 - accuracy: 0.5034\n",
            "Epoch 2/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.6948 - accuracy: 0.5103\n",
            "Epoch 3/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.6885 - accuracy: 0.5103\n",
            "Epoch 4/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.6746 - accuracy: 0.5655\n",
            "Epoch 5/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.6667 - accuracy: 0.5931\n",
            "Epoch 6/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.6552 - accuracy: 0.6345\n",
            "Epoch 7/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.6425 - accuracy: 0.6483\n",
            "Epoch 8/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.6311 - accuracy: 0.6483\n",
            "Epoch 9/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.6175 - accuracy: 0.6552\n",
            "Epoch 10/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.6044 - accuracy: 0.6966\n",
            "Epoch 11/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.5898 - accuracy: 0.7103\n",
            "Epoch 12/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.5756 - accuracy: 0.6621\n",
            "Epoch 13/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.5608 - accuracy: 0.7310\n",
            "Epoch 14/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.5462 - accuracy: 0.7241\n",
            "Epoch 15/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.5304 - accuracy: 0.7448\n",
            "Epoch 16/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.5160 - accuracy: 0.7310\n",
            "Epoch 17/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7793\n",
            "Epoch 18/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.7862\n",
            "Epoch 19/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.4726 - accuracy: 0.7931\n",
            "Epoch 20/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.4597 - accuracy: 0.7931\n",
            "Epoch 21/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.4559 - accuracy: 0.7724\n",
            "Epoch 22/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.4384 - accuracy: 0.7793\n",
            "Epoch 23/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.4309 - accuracy: 0.7655\n",
            "Epoch 24/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.4249 - accuracy: 0.8000\n",
            "Epoch 25/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.4110 - accuracy: 0.7862\n",
            "Epoch 26/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.3986 - accuracy: 0.8069\n",
            "Epoch 27/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8069\n",
            "Epoch 28/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.3906 - accuracy: 0.8069\n",
            "Epoch 29/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.3755 - accuracy: 0.8207\n",
            "Epoch 30/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.3740 - accuracy: 0.7862\n",
            "Epoch 31/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.3593 - accuracy: 0.8621\n",
            "Epoch 32/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.3568 - accuracy: 0.8483\n",
            "Epoch 33/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.3574 - accuracy: 0.8276\n",
            "Epoch 34/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.3482 - accuracy: 0.8345\n",
            "Epoch 35/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.3415 - accuracy: 0.8552\n",
            "Epoch 36/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.3303 - accuracy: 0.8690\n",
            "Epoch 37/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.3219 - accuracy: 0.8552\n",
            "Epoch 38/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.3187 - accuracy: 0.8621\n",
            "Epoch 39/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.3046 - accuracy: 0.8759\n",
            "Epoch 40/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.3053 - accuracy: 0.8759\n",
            "Epoch 41/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.3078 - accuracy: 0.8552\n",
            "Epoch 42/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.2921 - accuracy: 0.8759\n",
            "Epoch 43/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.2868 - accuracy: 0.8759\n",
            "Epoch 44/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.2874 - accuracy: 0.8966\n",
            "Epoch 45/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.2781 - accuracy: 0.8828\n",
            "Epoch 46/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.2760 - accuracy: 0.8897\n",
            "Epoch 47/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.2892 - accuracy: 0.8621\n",
            "Epoch 48/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.2817 - accuracy: 0.8690\n",
            "Epoch 49/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.2667 - accuracy: 0.9034\n",
            "Epoch 50/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.2503 - accuracy: 0.9172\n",
            "Epoch 51/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.2541 - accuracy: 0.9103\n",
            "Epoch 52/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.2464 - accuracy: 0.9241\n",
            "Epoch 53/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.2377 - accuracy: 0.9103\n",
            "Epoch 54/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.2605 - accuracy: 0.8966\n",
            "Epoch 55/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.2383 - accuracy: 0.9379\n",
            "Epoch 56/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.2299 - accuracy: 0.9241\n",
            "Epoch 57/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.2275 - accuracy: 0.8966\n",
            "Epoch 58/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.2328 - accuracy: 0.8966\n",
            "Epoch 59/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.2269 - accuracy: 0.9241\n",
            "Epoch 60/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.2175 - accuracy: 0.9379\n",
            "Epoch 61/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.2091 - accuracy: 0.9586\n",
            "Epoch 62/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.2076 - accuracy: 0.9586\n",
            "Epoch 63/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.2023 - accuracy: 0.9448\n",
            "Epoch 64/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.2126 - accuracy: 0.9241\n",
            "Epoch 65/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.2157 - accuracy: 0.9379\n",
            "Epoch 66/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1997 - accuracy: 0.9586\n",
            "Epoch 67/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1951 - accuracy: 0.9379\n",
            "Epoch 68/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1832 - accuracy: 0.9517\n",
            "Epoch 69/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1891 - accuracy: 0.9517\n",
            "Epoch 70/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1770 - accuracy: 0.9793\n",
            "Epoch 71/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1810 - accuracy: 0.9379\n",
            "Epoch 72/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1766 - accuracy: 0.9517\n",
            "Epoch 73/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1671 - accuracy: 0.9655\n",
            "Epoch 74/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1689 - accuracy: 0.9586\n",
            "Epoch 75/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1618 - accuracy: 0.9586\n",
            "Epoch 76/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1592 - accuracy: 0.9586\n",
            "Epoch 77/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1638 - accuracy: 0.9379\n",
            "Epoch 78/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1690 - accuracy: 0.9517\n",
            "Epoch 79/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1494 - accuracy: 0.9724\n",
            "Epoch 80/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1540 - accuracy: 0.9517\n",
            "Epoch 81/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1597 - accuracy: 0.9586\n",
            "Epoch 82/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1487 - accuracy: 0.9586\n",
            "Epoch 83/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1504 - accuracy: 0.9655\n",
            "Epoch 84/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1387 - accuracy: 0.9724\n",
            "Epoch 85/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1363 - accuracy: 0.9793\n",
            "Epoch 86/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1343 - accuracy: 0.9724\n",
            "Epoch 87/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1358 - accuracy: 0.9724\n",
            "Epoch 88/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1312 - accuracy: 0.9724\n",
            "Epoch 89/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1259 - accuracy: 0.9724\n",
            "Epoch 90/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1245 - accuracy: 0.9724\n",
            "Epoch 91/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1241 - accuracy: 0.9793\n",
            "Epoch 92/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9724\n",
            "Epoch 93/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1245 - accuracy: 0.9724\n",
            "Epoch 94/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1114 - accuracy: 0.9862\n",
            "Epoch 95/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1167 - accuracy: 0.9793\n",
            "Epoch 96/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1130 - accuracy: 0.9724\n",
            "Epoch 97/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1113 - accuracy: 0.9724\n",
            "Epoch 98/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1063 - accuracy: 0.9793\n",
            "Epoch 99/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1098 - accuracy: 0.9862\n",
            "Epoch 100/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1027 - accuracy: 0.9793\n",
            "Epoch 101/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.1043 - accuracy: 0.9793\n",
            "Epoch 102/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.9862\n",
            "Epoch 103/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9862\n",
            "Epoch 104/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.9862\n",
            "Epoch 105/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0900 - accuracy: 0.9862\n",
            "Epoch 106/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0922 - accuracy: 0.9793\n",
            "Epoch 107/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0935 - accuracy: 0.9862\n",
            "Epoch 108/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0985 - accuracy: 0.9724\n",
            "Epoch 109/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0855 - accuracy: 0.9862\n",
            "Epoch 110/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0810 - accuracy: 0.9862\n",
            "Epoch 111/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0787 - accuracy: 0.9862\n",
            "Epoch 112/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0757 - accuracy: 0.9862\n",
            "Epoch 113/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0741 - accuracy: 0.9862\n",
            "Epoch 114/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0727 - accuracy: 0.9862\n",
            "Epoch 115/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0732 - accuracy: 0.9862\n",
            "Epoch 116/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0673 - accuracy: 0.9931\n",
            "Epoch 117/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0696 - accuracy: 0.9931\n",
            "Epoch 118/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0720 - accuracy: 0.9931\n",
            "Epoch 119/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.9862\n",
            "Epoch 120/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0627 - accuracy: 0.9862\n",
            "Epoch 121/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0701 - accuracy: 0.9862\n",
            "Epoch 122/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0620 - accuracy: 0.9931\n",
            "Epoch 123/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0625 - accuracy: 0.9931\n",
            "Epoch 124/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0591 - accuracy: 0.9931\n",
            "Epoch 125/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0574 - accuracy: 0.9931\n",
            "Epoch 126/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0558 - accuracy: 0.9931\n",
            "Epoch 127/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0552 - accuracy: 0.9931\n",
            "Epoch 128/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0517 - accuracy: 0.9931\n",
            "Epoch 129/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0593 - accuracy: 0.9931\n",
            "Epoch 130/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0530 - accuracy: 0.9931\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb9663beb50>"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n test accuracy: %.4f\" % model.evaluate(X_test,Y_test)[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOILHOkV6eYn",
        "outputId": "ad10e5f1-8939-4e27-eff1-8d954a4c4138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5371 - accuracy: 0.8095\n",
            "\n",
            " test accuracy: 0.8095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장과 재사용\n",
        "from keras.models import load_model\n",
        "model.save('mymodel.h5') # content안에 h5확장자파일이 저장된것을 확인가능\n"
      ],
      "metadata": {
        "id": "RffdESit6v39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "mkUvu7bL7XoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "B8rdb7Mq7Yqm",
        "outputId": "63f25ac6-0192-494e-8128-edbceef6d0bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-113-1f8a688cae5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model =load_model('mymodel.h5')"
      ],
      "metadata": {
        "id": "D3itATIi7JZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5-9WO_m7Q4A",
        "outputId": "5eeebc9b-7bf3-4e46-e843-89110e1f3cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7fb9662a19d0>"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test,Y_test)[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E6sz0GL7RSF",
        "outputId": "771d0d40-f3e0-41b6-eccb-95c957c2ecb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5371 - accuracy: 0.8095\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8095238208770752"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# k겹 교차검증"
      ],
      "metadata": {
        "id": "pRCU5d0x7hlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "metadata": {
        "id": "Hm_AFW0n7okj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_fold=10\n",
        "seed=0\n",
        "skf=StratifiedKFold(n_splits = n_fold, shuffle=True, random_state=seed)\n",
        "accuracy = []"
      ],
      "metadata": {
        "id": "ctky1am0-t5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for train, test in skf.split(X,Y):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(24, input_dim=60, activation='relu'))\n",
        "  model.add(Dense(10, activation = 'relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='mean_squared_error', optimizer = 'adam', metrics=['accuracy'])\n",
        "  model.fit(X[train],Y[train], epochs=100, batch_size=5)\n",
        "  k_accuracy = \"%.4f\" % (model.evaluate(X[test],Y[test])[1])\n",
        "  accuracy.append(k_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK6uUD17-5c_",
        "outputId": "9592914e-9dfc-4ca0-926b-0128e1286c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2488 - accuracy: 0.5294\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2368 - accuracy: 0.5241\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2299 - accuracy: 0.5401\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2220 - accuracy: 0.5829\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2131 - accuracy: 0.6524\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2028 - accuracy: 0.7487\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1931 - accuracy: 0.7433\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1814 - accuracy: 0.7754\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1719 - accuracy: 0.7754\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1610 - accuracy: 0.7807\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1562 - accuracy: 0.7914\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1457 - accuracy: 0.7701\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1448 - accuracy: 0.8235\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1366 - accuracy: 0.8182\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1313 - accuracy: 0.8182\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1328 - accuracy: 0.8075\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1273 - accuracy: 0.8342\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1252 - accuracy: 0.8342\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1214 - accuracy: 0.8610\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1184 - accuracy: 0.8449\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1168 - accuracy: 0.8396\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1469 - accuracy: 0.7701\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1147 - accuracy: 0.8449\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1148 - accuracy: 0.8610\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1078 - accuracy: 0.8610\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1050 - accuracy: 0.8717\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1036 - accuracy: 0.8610\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0985 - accuracy: 0.8503\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1009 - accuracy: 0.8610\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0970 - accuracy: 0.8770\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1003 - accuracy: 0.8717\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0964 - accuracy: 0.8717\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0964 - accuracy: 0.8824\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0921 - accuracy: 0.8717\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0919 - accuracy: 0.8770\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0928 - accuracy: 0.8770\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0903 - accuracy: 0.8984\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0867 - accuracy: 0.8877\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0854 - accuracy: 0.8770\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0819 - accuracy: 0.9144\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0867 - accuracy: 0.8877\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0833 - accuracy: 0.8877\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9091\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0797 - accuracy: 0.9198\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0741 - accuracy: 0.9144\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0754 - accuracy: 0.9091\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.9037\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0707 - accuracy: 0.9198\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.9198\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0696 - accuracy: 0.9144\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0666 - accuracy: 0.9412\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0667 - accuracy: 0.9198\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0649 - accuracy: 0.9412\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0638 - accuracy: 0.9358\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0621 - accuracy: 0.9358\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0616 - accuracy: 0.9412\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0620 - accuracy: 0.9198\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0591 - accuracy: 0.9412\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0595 - accuracy: 0.9465\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0637 - accuracy: 0.9198\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0580 - accuracy: 0.9465\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0550 - accuracy: 0.9519\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0550 - accuracy: 0.9465\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0572 - accuracy: 0.9412\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0547 - accuracy: 0.9358\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0514 - accuracy: 0.9412\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0519 - accuracy: 0.9465\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0499 - accuracy: 0.9572\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0489 - accuracy: 0.9519\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0496 - accuracy: 0.9465\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0583 - accuracy: 0.9305\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0447 - accuracy: 0.9626\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0480 - accuracy: 0.9519\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0474 - accuracy: 0.9412\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0448 - accuracy: 0.9572\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0532 - accuracy: 0.9412\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0457 - accuracy: 0.9465\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0477 - accuracy: 0.9519\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0423 - accuracy: 0.9679\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0389 - accuracy: 0.9733\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 0.9626\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0404 - accuracy: 0.9626\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0367 - accuracy: 0.9572\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0366 - accuracy: 0.9679\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0461 - accuracy: 0.9626\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0394 - accuracy: 0.9572\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0416 - accuracy: 0.9519\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0349 - accuracy: 0.9626\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0342 - accuracy: 0.9733\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0338 - accuracy: 0.9733\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0317 - accuracy: 0.9786\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0300 - accuracy: 0.9786\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0325 - accuracy: 0.9626\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0305 - accuracy: 0.9786\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0305 - accuracy: 0.9786\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0301 - accuracy: 0.9626\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.9733\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0286 - accuracy: 0.9786\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 0.9840\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0282 - accuracy: 0.9786\n",
            "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb9688899e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.2187 - accuracy: 0.7619\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2497 - accuracy: 0.5294\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2357 - accuracy: 0.6043\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2256 - accuracy: 0.6791\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2119 - accuracy: 0.7112\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1988 - accuracy: 0.7380\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1844 - accuracy: 0.7540\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1760 - accuracy: 0.7540\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1632 - accuracy: 0.7861\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1539 - accuracy: 0.7861\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1469 - accuracy: 0.8075\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1505 - accuracy: 0.7968\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1391 - accuracy: 0.7968\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1477 - accuracy: 0.7807\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1367 - accuracy: 0.8128\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1358 - accuracy: 0.8128\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1315 - accuracy: 0.8342\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1332 - accuracy: 0.8128\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1338 - accuracy: 0.8182\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1280 - accuracy: 0.7968\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1281 - accuracy: 0.8021\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1308 - accuracy: 0.8021\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1335 - accuracy: 0.7861\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1190 - accuracy: 0.8289\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1315 - accuracy: 0.8235\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1161 - accuracy: 0.8289\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.8396\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1153 - accuracy: 0.8235\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1122 - accuracy: 0.8289\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1103 - accuracy: 0.8396\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1095 - accuracy: 0.8449\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1071 - accuracy: 0.8449\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1084 - accuracy: 0.8556\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1145 - accuracy: 0.8289\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0995 - accuracy: 0.8770\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1107 - accuracy: 0.8342\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1004 - accuracy: 0.8824\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0991 - accuracy: 0.8503\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0990 - accuracy: 0.8770\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0973 - accuracy: 0.8877\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0942 - accuracy: 0.8770\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0906 - accuracy: 0.8877\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0899 - accuracy: 0.8877\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0838 - accuracy: 0.8984\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0826 - accuracy: 0.9091\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0813 - accuracy: 0.8930\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0822 - accuracy: 0.8930\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0846 - accuracy: 0.8877\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0791 - accuracy: 0.9037\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0752 - accuracy: 0.9251\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0736 - accuracy: 0.9091\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0720 - accuracy: 0.9037\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0719 - accuracy: 0.9037\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0703 - accuracy: 0.9198\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0664 - accuracy: 0.9251\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0665 - accuracy: 0.9144\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0673 - accuracy: 0.9198\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0622 - accuracy: 0.9251\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0607 - accuracy: 0.9465\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0610 - accuracy: 0.9198\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0592 - accuracy: 0.9465\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0589 - accuracy: 0.9519\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0544 - accuracy: 0.9519\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0527 - accuracy: 0.9465\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0673 - accuracy: 0.8930\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0606 - accuracy: 0.9305\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0520 - accuracy: 0.9519\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0556 - accuracy: 0.9412\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0479 - accuracy: 0.9733\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0455 - accuracy: 0.9626\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0452 - accuracy: 0.9679\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0486 - accuracy: 0.9465\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 0.9733\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0407 - accuracy: 0.9733\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0405 - accuracy: 0.9679\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0416 - accuracy: 0.9733\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0462 - accuracy: 0.9465\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0438 - accuracy: 0.9519\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0350 - accuracy: 0.9733\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0348 - accuracy: 0.9786\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0342 - accuracy: 0.9786\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0363 - accuracy: 0.9679\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 0.9786\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0304 - accuracy: 0.9840\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 0.9733\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0412 - accuracy: 0.9679\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0316 - accuracy: 0.9840\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.9840\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0282 - accuracy: 0.9893\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0296 - accuracy: 0.9786\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0341 - accuracy: 0.9679\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0300 - accuracy: 0.9893\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.9893\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 0.9786\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0239 - accuracy: 0.9893\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0293 - accuracy: 0.9679\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0243 - accuracy: 0.9947\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0228 - accuracy: 0.9893\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0202 - accuracy: 0.9893\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0198 - accuracy: 0.9893\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 0.9840\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.1917 - accuracy: 0.7619\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2457 - accuracy: 0.5401\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2360 - accuracy: 0.5989\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2299 - accuracy: 0.6203\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2214 - accuracy: 0.7059\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2121 - accuracy: 0.7594\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2017 - accuracy: 0.7594\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1917 - accuracy: 0.7433\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1808 - accuracy: 0.7968\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1703 - accuracy: 0.7807\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1601 - accuracy: 0.8075\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1637 - accuracy: 0.7807\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1503 - accuracy: 0.8182\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1507 - accuracy: 0.7968\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1407 - accuracy: 0.8235\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1391 - accuracy: 0.8128\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1359 - accuracy: 0.8342\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1336 - accuracy: 0.8396\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1285 - accuracy: 0.8235\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1253 - accuracy: 0.8610\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1211 - accuracy: 0.8342\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1198 - accuracy: 0.8556\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1301 - accuracy: 0.8235\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1139 - accuracy: 0.8770\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1207 - accuracy: 0.8235\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1084 - accuracy: 0.8610\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1059 - accuracy: 0.8610\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1036 - accuracy: 0.9037\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0999 - accuracy: 0.8717\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0993 - accuracy: 0.8877\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0955 - accuracy: 0.9037\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0931 - accuracy: 0.9144\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0979 - accuracy: 0.8877\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0966 - accuracy: 0.8717\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0873 - accuracy: 0.9251\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0866 - accuracy: 0.9037\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0883 - accuracy: 0.9144\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0830 - accuracy: 0.9091\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0781 - accuracy: 0.9358\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0781 - accuracy: 0.9305\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0816 - accuracy: 0.9198\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0742 - accuracy: 0.9412\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0709 - accuracy: 0.9465\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0703 - accuracy: 0.9572\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0688 - accuracy: 0.9358\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0692 - accuracy: 0.9412\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0713 - accuracy: 0.9412\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0686 - accuracy: 0.9412\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0622 - accuracy: 0.9519\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0645 - accuracy: 0.9412\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0597 - accuracy: 0.9572\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0596 - accuracy: 0.9358\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0698 - accuracy: 0.9037\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0574 - accuracy: 0.9626\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0556 - accuracy: 0.9572\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0544 - accuracy: 0.9572\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0599 - accuracy: 0.9519\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0522 - accuracy: 0.9572\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0535 - accuracy: 0.9519\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0524 - accuracy: 0.9519\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0503 - accuracy: 0.9572\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0467 - accuracy: 0.9572\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0456 - accuracy: 0.9626\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0442 - accuracy: 0.9679\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0544 - accuracy: 0.9305\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0422 - accuracy: 0.9679\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0437 - accuracy: 0.9679\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0442 - accuracy: 0.9679\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0397 - accuracy: 0.9679\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0387 - accuracy: 0.9626\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0393 - accuracy: 0.9733\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0394 - accuracy: 0.9679\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0351 - accuracy: 0.9679\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0349 - accuracy: 0.9733\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0412 - accuracy: 0.9626\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0333 - accuracy: 0.9733\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0416 - accuracy: 0.9519\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0334 - accuracy: 0.9679\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0346 - accuracy: 0.9733\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0327 - accuracy: 0.9786\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0307 - accuracy: 0.9733\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0299 - accuracy: 0.9733\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0278 - accuracy: 0.9786\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0293 - accuracy: 0.9679\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 0.9786\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.9786\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0256 - accuracy: 0.9786\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0294 - accuracy: 0.9786\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.9786\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0238 - accuracy: 0.9786\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 0.9840\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 0.9840\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 0.9840\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0197 - accuracy: 0.9840\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0210 - accuracy: 0.9840\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0204 - accuracy: 0.9840\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0182 - accuracy: 0.9840\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0186 - accuracy: 0.9840\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 0.9840\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0177 - accuracy: 0.9840\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0159 - accuracy: 0.9893\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.2064 - accuracy: 0.7619\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2709 - accuracy: 0.4332\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2438 - accuracy: 0.5561\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2362 - accuracy: 0.6043\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2277 - accuracy: 0.6845\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2178 - accuracy: 0.7380\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2079 - accuracy: 0.7540\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2000 - accuracy: 0.7701\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1896 - accuracy: 0.7807\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1806 - accuracy: 0.7968\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1708 - accuracy: 0.8021\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1674 - accuracy: 0.7968\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1576 - accuracy: 0.8235\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1582 - accuracy: 0.8021\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1471 - accuracy: 0.8289\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1419 - accuracy: 0.8342\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1437 - accuracy: 0.8075\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1385 - accuracy: 0.8128\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1384 - accuracy: 0.8128\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1303 - accuracy: 0.8396\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1259 - accuracy: 0.8556\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1273 - accuracy: 0.8075\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1421 - accuracy: 0.7914\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1175 - accuracy: 0.8556\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1248 - accuracy: 0.8342\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1111 - accuracy: 0.8717\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1125 - accuracy: 0.8610\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1126 - accuracy: 0.8770\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1136 - accuracy: 0.8663\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1055 - accuracy: 0.8503\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1048 - accuracy: 0.8717\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1010 - accuracy: 0.8770\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1128 - accuracy: 0.8556\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1052 - accuracy: 0.8503\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0962 - accuracy: 0.8877\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0957 - accuracy: 0.8770\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0920 - accuracy: 0.8770\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0913 - accuracy: 0.8930\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0906 - accuracy: 0.8877\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0925 - accuracy: 0.8663\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0904 - accuracy: 0.8877\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0878 - accuracy: 0.8930\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0827 - accuracy: 0.9037\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0824 - accuracy: 0.8877\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0791 - accuracy: 0.8877\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0778 - accuracy: 0.8930\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0804 - accuracy: 0.8930\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0772 - accuracy: 0.9144\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0729 - accuracy: 0.9198\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0711 - accuracy: 0.9251\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0730 - accuracy: 0.9198\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0672 - accuracy: 0.9198\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0726 - accuracy: 0.9198\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0711 - accuracy: 0.9198\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0646 - accuracy: 0.9358\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0645 - accuracy: 0.9358\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0609 - accuracy: 0.9412\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0595 - accuracy: 0.9519\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0594 - accuracy: 0.9519\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0592 - accuracy: 0.9412\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0553 - accuracy: 0.9519\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0555 - accuracy: 0.9626\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0527 - accuracy: 0.9572\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0580 - accuracy: 0.9305\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0509 - accuracy: 0.9679\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0536 - accuracy: 0.9572\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0505 - accuracy: 0.9572\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0526 - accuracy: 0.9519\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0470 - accuracy: 0.9733\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0449 - accuracy: 0.9626\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0457 - accuracy: 0.9626\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0496 - accuracy: 0.9626\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0422 - accuracy: 0.9679\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0416 - accuracy: 0.9733\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0443 - accuracy: 0.9626\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 0.9786\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0402 - accuracy: 0.9626\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0425 - accuracy: 0.9733\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0364 - accuracy: 0.9786\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0334 - accuracy: 0.9786\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0366 - accuracy: 0.9786\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0332 - accuracy: 0.9840\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0337 - accuracy: 0.9786\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0320 - accuracy: 0.9840\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 0.9786\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0344 - accuracy: 0.9786\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0311 - accuracy: 0.9840\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 0.9840\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0290 - accuracy: 0.9786\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0288 - accuracy: 0.9733\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0330 - accuracy: 0.9733\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.9840\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0259 - accuracy: 0.9840\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0250 - accuracy: 0.9840\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0253 - accuracy: 0.9840\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.9840\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0255 - accuracy: 0.9893\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0257 - accuracy: 0.9786\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0257 - accuracy: 0.9786\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.9893\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0232 - accuracy: 0.9840\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.0551 - accuracy: 0.9524\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2518 - accuracy: 0.5348\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2403 - accuracy: 0.5829\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2315 - accuracy: 0.6150\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2226 - accuracy: 0.6364\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2137 - accuracy: 0.7166\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2023 - accuracy: 0.7807\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1955 - accuracy: 0.7701\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1856 - accuracy: 0.8128\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1779 - accuracy: 0.8128\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1695 - accuracy: 0.7861\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1715 - accuracy: 0.7754\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1576 - accuracy: 0.7968\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1580 - accuracy: 0.8182\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1475 - accuracy: 0.8449\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1446 - accuracy: 0.8289\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1434 - accuracy: 0.8128\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1403 - accuracy: 0.8235\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1383 - accuracy: 0.7968\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1340 - accuracy: 0.8128\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1283 - accuracy: 0.8449\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1278 - accuracy: 0.8503\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1328 - accuracy: 0.7968\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1200 - accuracy: 0.8396\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1284 - accuracy: 0.8396\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1140 - accuracy: 0.8610\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1124 - accuracy: 0.8663\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1093 - accuracy: 0.8824\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1055 - accuracy: 0.8610\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1068 - accuracy: 0.8930\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1047 - accuracy: 0.8610\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1018 - accuracy: 0.8663\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1044 - accuracy: 0.8396\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1031 - accuracy: 0.8770\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0974 - accuracy: 0.8770\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0990 - accuracy: 0.8610\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0924 - accuracy: 0.8824\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0917 - accuracy: 0.8770\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0970 - accuracy: 0.8824\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0917 - accuracy: 0.8824\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0940 - accuracy: 0.8717\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0900 - accuracy: 0.8770\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0842 - accuracy: 0.9091\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0824 - accuracy: 0.8877\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0821 - accuracy: 0.8930\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0797 - accuracy: 0.9037\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0836 - accuracy: 0.8877\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0804 - accuracy: 0.8877\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0750 - accuracy: 0.9144\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0736 - accuracy: 0.8984\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0752 - accuracy: 0.9091\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0731 - accuracy: 0.9037\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0718 - accuracy: 0.9198\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0672 - accuracy: 0.9251\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0658 - accuracy: 0.9305\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0690 - accuracy: 0.9144\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0745 - accuracy: 0.9037\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0623 - accuracy: 0.9251\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0653 - accuracy: 0.9251\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0657 - accuracy: 0.9358\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0624 - accuracy: 0.9305\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0581 - accuracy: 0.9465\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0572 - accuracy: 0.9465\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0620 - accuracy: 0.9198\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0613 - accuracy: 0.9358\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0558 - accuracy: 0.9572\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0537 - accuracy: 0.9358\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0569 - accuracy: 0.9519\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0518 - accuracy: 0.9519\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0524 - accuracy: 0.9572\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0543 - accuracy: 0.9519\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0534 - accuracy: 0.9465\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0473 - accuracy: 0.9626\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0491 - accuracy: 0.9626\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0469 - accuracy: 0.9679\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0474 - accuracy: 0.9679\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0492 - accuracy: 0.9572\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0461 - accuracy: 0.9679\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0458 - accuracy: 0.9626\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0450 - accuracy: 0.9679\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0419 - accuracy: 0.9733\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0398 - accuracy: 0.9679\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0388 - accuracy: 0.9786\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0378 - accuracy: 0.9786\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0367 - accuracy: 0.9733\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0402 - accuracy: 0.9679\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0383 - accuracy: 0.9786\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0368 - accuracy: 0.9786\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0386 - accuracy: 0.9679\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0347 - accuracy: 0.9786\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0361 - accuracy: 0.9733\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0362 - accuracy: 0.9786\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0316 - accuracy: 0.9786\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0375 - accuracy: 0.9679\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0319 - accuracy: 0.9786\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0306 - accuracy: 0.9786\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0309 - accuracy: 0.9786\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0317 - accuracy: 0.9733\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0328 - accuracy: 0.9840\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0298 - accuracy: 0.9786\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0288 - accuracy: 0.9786\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.1443 - accuracy: 0.8095\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2518 - accuracy: 0.5348\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2465 - accuracy: 0.5508\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2402 - accuracy: 0.6096\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2306 - accuracy: 0.6578\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2186 - accuracy: 0.7059\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2081 - accuracy: 0.7166\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1933 - accuracy: 0.7433\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1771 - accuracy: 0.7861\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1682 - accuracy: 0.8021\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1568 - accuracy: 0.8289\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1567 - accuracy: 0.7861\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1437 - accuracy: 0.8021\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1468 - accuracy: 0.8182\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1321 - accuracy: 0.8396\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1310 - accuracy: 0.8449\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1281 - accuracy: 0.8449\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1224 - accuracy: 0.8503\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1222 - accuracy: 0.8128\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1243 - accuracy: 0.8449\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1137 - accuracy: 0.8824\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1147 - accuracy: 0.8610\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1363 - accuracy: 0.7754\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1100 - accuracy: 0.8717\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1113 - accuracy: 0.8556\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1050 - accuracy: 0.8663\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0996 - accuracy: 0.8930\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0966 - accuracy: 0.8984\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0923 - accuracy: 0.9037\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0924 - accuracy: 0.8930\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0913 - accuracy: 0.8930\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0896 - accuracy: 0.9037\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0890 - accuracy: 0.9037\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0874 - accuracy: 0.9037\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0851 - accuracy: 0.9091\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0869 - accuracy: 0.8984\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0920 - accuracy: 0.8663\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0810 - accuracy: 0.8984\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0790 - accuracy: 0.9144\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0772 - accuracy: 0.9144\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0751 - accuracy: 0.9412\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0746 - accuracy: 0.9305\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0711 - accuracy: 0.9412\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0701 - accuracy: 0.9305\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0675 - accuracy: 0.9358\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0663 - accuracy: 0.9412\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0688 - accuracy: 0.9305\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0691 - accuracy: 0.9144\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0638 - accuracy: 0.9412\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0640 - accuracy: 0.9465\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0749 - accuracy: 0.9037\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0596 - accuracy: 0.9412\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0638 - accuracy: 0.9198\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0569 - accuracy: 0.9572\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0572 - accuracy: 0.9412\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0527 - accuracy: 0.9626\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0537 - accuracy: 0.9519\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0510 - accuracy: 0.9626\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0509 - accuracy: 0.9519\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0498 - accuracy: 0.9519\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0490 - accuracy: 0.9465\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0500 - accuracy: 0.9412\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0457 - accuracy: 0.9572\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0476 - accuracy: 0.9572\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0477 - accuracy: 0.9626\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0431 - accuracy: 0.9679\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0423 - accuracy: 0.9626\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0449 - accuracy: 0.9626\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0416 - accuracy: 0.9679\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0402 - accuracy: 0.9626\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0376 - accuracy: 0.9733\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0389 - accuracy: 0.9626\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0364 - accuracy: 0.9786\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0432 - accuracy: 0.9519\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0357 - accuracy: 0.9733\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0370 - accuracy: 0.9679\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0368 - accuracy: 0.9679\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0323 - accuracy: 0.9733\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0404 - accuracy: 0.9626\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 0.9733\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0314 - accuracy: 0.9733\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0355 - accuracy: 0.9679\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0300 - accuracy: 0.9840\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0289 - accuracy: 0.9840\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.9786\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.9840\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.9840\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0251 - accuracy: 0.9893\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.9893\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0233 - accuracy: 0.9840\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0230 - accuracy: 0.9840\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0257 - accuracy: 0.9786\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.9840\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.9893\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0229 - accuracy: 0.9786\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0227 - accuracy: 0.9840\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 0.9893\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 0.9893\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0186 - accuracy: 0.9893\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 0.9840\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 0.9840\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.1163 - accuracy: 0.8571\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 1s 2ms/step - loss: 0.2475 - accuracy: 0.5187\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2395 - accuracy: 0.5455\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2339 - accuracy: 0.5722\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2284 - accuracy: 0.6257\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2210 - accuracy: 0.6845\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2129 - accuracy: 0.7059\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2058 - accuracy: 0.7059\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1974 - accuracy: 0.7540\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1887 - accuracy: 0.7487\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1807 - accuracy: 0.7754\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1844 - accuracy: 0.7273\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1718 - accuracy: 0.7487\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1701 - accuracy: 0.7807\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1605 - accuracy: 0.8021\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1573 - accuracy: 0.7914\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - accuracy: 0.7968\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1548 - accuracy: 0.7968\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1472 - accuracy: 0.8128\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1446 - accuracy: 0.8182\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1469 - accuracy: 0.8021\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1442 - accuracy: 0.7968\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1461 - accuracy: 0.7968\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1356 - accuracy: 0.8128\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1351 - accuracy: 0.8289\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1272 - accuracy: 0.8342\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1335 - accuracy: 0.7968\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1306 - accuracy: 0.8235\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1250 - accuracy: 0.8182\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1195 - accuracy: 0.8396\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1202 - accuracy: 0.8235\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1191 - accuracy: 0.8342\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1190 - accuracy: 0.8396\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1174 - accuracy: 0.8556\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1142 - accuracy: 0.8449\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1128 - accuracy: 0.8396\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1109 - accuracy: 0.8503\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1088 - accuracy: 0.8556\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1107 - accuracy: 0.8449\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1078 - accuracy: 0.8663\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1055 - accuracy: 0.8717\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1021 - accuracy: 0.8824\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1034 - accuracy: 0.8717\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1017 - accuracy: 0.8503\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1002 - accuracy: 0.8770\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0958 - accuracy: 0.8824\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0953 - accuracy: 0.8930\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0947 - accuracy: 0.8877\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0918 - accuracy: 0.8824\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0918 - accuracy: 0.8984\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0935 - accuracy: 0.8984\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0883 - accuracy: 0.8930\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0914 - accuracy: 0.8770\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0889 - accuracy: 0.8877\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0852 - accuracy: 0.9091\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0855 - accuracy: 0.9091\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0911 - accuracy: 0.8984\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0826 - accuracy: 0.9037\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0804 - accuracy: 0.9251\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0832 - accuracy: 0.9037\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0816 - accuracy: 0.8984\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0804 - accuracy: 0.9305\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0797 - accuracy: 0.9091\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0744 - accuracy: 0.9198\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0790 - accuracy: 0.8930\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0735 - accuracy: 0.9305\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0706 - accuracy: 0.9358\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0737 - accuracy: 0.9358\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0679 - accuracy: 0.9412\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0684 - accuracy: 0.9412\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0689 - accuracy: 0.9305\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0684 - accuracy: 0.9412\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0662 - accuracy: 0.9358\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0655 - accuracy: 0.9465\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0623 - accuracy: 0.9519\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0651 - accuracy: 0.9572\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0658 - accuracy: 0.9519\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0605 - accuracy: 0.9412\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0633 - accuracy: 0.9465\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0592 - accuracy: 0.9572\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0558 - accuracy: 0.9572\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0557 - accuracy: 0.9572\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0550 - accuracy: 0.9626\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0535 - accuracy: 0.9626\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0532 - accuracy: 0.9572\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0523 - accuracy: 0.9572\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0580 - accuracy: 0.9358\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0543 - accuracy: 0.9626\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0625 - accuracy: 0.9251\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0502 - accuracy: 0.9572\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0502 - accuracy: 0.9572\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0490 - accuracy: 0.9572\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0483 - accuracy: 0.9626\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0474 - accuracy: 0.9572\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0461 - accuracy: 0.9626\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0483 - accuracy: 0.9626\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0457 - accuracy: 0.9572\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0449 - accuracy: 0.9572\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0462 - accuracy: 0.9679\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0450 - accuracy: 0.9572\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0448 - accuracy: 0.9626\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.0966 - accuracy: 0.8571\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2691 - accuracy: 0.4759\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2422 - accuracy: 0.6096\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2370 - accuracy: 0.5936\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2302 - accuracy: 0.6524\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2222 - accuracy: 0.7005\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2141 - accuracy: 0.7112\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2044 - accuracy: 0.7166\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1941 - accuracy: 0.7807\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1837 - accuracy: 0.7380\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1732 - accuracy: 0.7540\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1769 - accuracy: 0.7380\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1609 - accuracy: 0.7914\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1615 - accuracy: 0.7754\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1504 - accuracy: 0.8235\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1458 - accuracy: 0.8289\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1403 - accuracy: 0.8182\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1394 - accuracy: 0.8556\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1367 - accuracy: 0.7861\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1312 - accuracy: 0.8396\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1288 - accuracy: 0.8289\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1272 - accuracy: 0.8342\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1317 - accuracy: 0.7914\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1248 - accuracy: 0.8182\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1298 - accuracy: 0.8182\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1147 - accuracy: 0.8503\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1160 - accuracy: 0.8449\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1125 - accuracy: 0.8717\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1125 - accuracy: 0.8503\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1104 - accuracy: 0.8663\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1091 - accuracy: 0.8663\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1013 - accuracy: 0.8610\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1033 - accuracy: 0.8717\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1060 - accuracy: 0.8503\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1008 - accuracy: 0.8770\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0991 - accuracy: 0.8449\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0997 - accuracy: 0.8824\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0963 - accuracy: 0.8663\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0945 - accuracy: 0.8770\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.8717\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0928 - accuracy: 0.8824\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0915 - accuracy: 0.8717\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0861 - accuracy: 0.8984\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0841 - accuracy: 0.9091\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0833 - accuracy: 0.9144\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0823 - accuracy: 0.9144\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0818 - accuracy: 0.9198\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0828 - accuracy: 0.8877\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0774 - accuracy: 0.8930\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0782 - accuracy: 0.8930\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0791 - accuracy: 0.8930\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0734 - accuracy: 0.9251\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0748 - accuracy: 0.9251\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0719 - accuracy: 0.9144\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0689 - accuracy: 0.9305\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0704 - accuracy: 0.8984\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0696 - accuracy: 0.9412\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0649 - accuracy: 0.9358\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0687 - accuracy: 0.9091\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0619 - accuracy: 0.9251\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0625 - accuracy: 0.9412\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0616 - accuracy: 0.9251\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0635 - accuracy: 0.9251\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0590 - accuracy: 0.9305\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0583 - accuracy: 0.9465\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0604 - accuracy: 0.9358\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0554 - accuracy: 0.9465\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0585 - accuracy: 0.9465\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0509 - accuracy: 0.9465\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9519\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0522 - accuracy: 0.9412\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0533 - accuracy: 0.9519\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0478 - accuracy: 0.9465\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0457 - accuracy: 0.9412\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0451 - accuracy: 0.9519\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0487 - accuracy: 0.9465\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0430 - accuracy: 0.9572\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0450 - accuracy: 0.9626\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0427 - accuracy: 0.9465\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0415 - accuracy: 0.9626\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0428 - accuracy: 0.9465\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0422 - accuracy: 0.9626\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0357 - accuracy: 0.9679\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0361 - accuracy: 0.9786\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0343 - accuracy: 0.9733\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0404 - accuracy: 0.9572\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0354 - accuracy: 0.9626\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0323 - accuracy: 0.9786\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0325 - accuracy: 0.9733\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0305 - accuracy: 0.9786\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0325 - accuracy: 0.9679\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0307 - accuracy: 0.9840\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.9786\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0281 - accuracy: 0.9786\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.9840\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0282 - accuracy: 0.9786\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.9786\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0258 - accuracy: 0.9840\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 0.9840\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0234 - accuracy: 0.9947\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.9893\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.1036 - accuracy: 0.8095\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2565 - accuracy: 0.4894\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2432 - accuracy: 0.5372\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2371 - accuracy: 0.6064\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2274 - accuracy: 0.6223\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2192 - accuracy: 0.6436\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2121 - accuracy: 0.7074\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2032 - accuracy: 0.7181\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1925 - accuracy: 0.7181\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1878 - accuracy: 0.7500\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1782 - accuracy: 0.7553\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1715 - accuracy: 0.7713\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1656 - accuracy: 0.7872\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1605 - accuracy: 0.7872\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1560 - accuracy: 0.8032\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1519 - accuracy: 0.8191\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1498 - accuracy: 0.8032\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1440 - accuracy: 0.8245\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1391 - accuracy: 0.8245\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1387 - accuracy: 0.8032\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1320 - accuracy: 0.8245\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1263 - accuracy: 0.8457\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1297 - accuracy: 0.8245\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1224 - accuracy: 0.8457\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1192 - accuracy: 0.8298\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1204 - accuracy: 0.8617\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1140 - accuracy: 0.8670\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1127 - accuracy: 0.8511\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1080 - accuracy: 0.8670\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1100 - accuracy: 0.8617\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1076 - accuracy: 0.8564\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1037 - accuracy: 0.8723\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1003 - accuracy: 0.8670\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1025 - accuracy: 0.8670\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0976 - accuracy: 0.8777\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0946 - accuracy: 0.8883\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0968 - accuracy: 0.8777\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0915 - accuracy: 0.8936\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0938 - accuracy: 0.8883\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0890 - accuracy: 0.9043\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0879 - accuracy: 0.9149\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0874 - accuracy: 0.9149\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0897 - accuracy: 0.8777\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0845 - accuracy: 0.9043\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0816 - accuracy: 0.9096\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0797 - accuracy: 0.9149\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0828 - accuracy: 0.8989\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0784 - accuracy: 0.8989\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0794 - accuracy: 0.9255\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0776 - accuracy: 0.8989\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0751 - accuracy: 0.9202\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0729 - accuracy: 0.9149\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0719 - accuracy: 0.9149\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0690 - accuracy: 0.9309\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0698 - accuracy: 0.9362\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0654 - accuracy: 0.9309\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0659 - accuracy: 0.9362\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0697 - accuracy: 0.9202\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0677 - accuracy: 0.9255\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0636 - accuracy: 0.9255\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0658 - accuracy: 0.9521\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0618 - accuracy: 0.9149\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0603 - accuracy: 0.9415\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0596 - accuracy: 0.9362\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0576 - accuracy: 0.9521\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0579 - accuracy: 0.9468\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0598 - accuracy: 0.9415\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0542 - accuracy: 0.9628\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0557 - accuracy: 0.9468\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0552 - accuracy: 0.9521\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0509 - accuracy: 0.9574\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0534 - accuracy: 0.9521\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0500 - accuracy: 0.9574\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0542 - accuracy: 0.9521\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0494 - accuracy: 0.9521\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0595 - accuracy: 0.9202\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0543 - accuracy: 0.9468\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0437 - accuracy: 0.9628\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0430 - accuracy: 0.9628\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0474 - accuracy: 0.9521\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0469 - accuracy: 0.9628\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0410 - accuracy: 0.9681\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0411 - accuracy: 0.9734\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0421 - accuracy: 0.9681\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0427 - accuracy: 0.9681\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0394 - accuracy: 0.9681\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0373 - accuracy: 0.9734\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0364 - accuracy: 0.9681\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0398 - accuracy: 0.9628\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0365 - accuracy: 0.9734\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0338 - accuracy: 0.9734\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0332 - accuracy: 0.9734\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0329 - accuracy: 0.9734\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0334 - accuracy: 0.9787\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0355 - accuracy: 0.9787\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.9681\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0292 - accuracy: 0.9787\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0342 - accuracy: 0.9734\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0311 - accuracy: 0.9734\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0307 - accuracy: 0.9787\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.9840\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0219 - accuracy: 1.0000\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2412 - accuracy: 0.5638\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2274 - accuracy: 0.6223\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2176 - accuracy: 0.7074\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2074 - accuracy: 0.6915\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1950 - accuracy: 0.7340\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1857 - accuracy: 0.7766\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1792 - accuracy: 0.7553\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1661 - accuracy: 0.7766\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1605 - accuracy: 0.7979\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1516 - accuracy: 0.7819\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1463 - accuracy: 0.8138\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1400 - accuracy: 0.8191\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1385 - accuracy: 0.8298\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1315 - accuracy: 0.8298\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1324 - accuracy: 0.8191\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1268 - accuracy: 0.8351\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1236 - accuracy: 0.8564\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1176 - accuracy: 0.8723\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1215 - accuracy: 0.8404\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1114 - accuracy: 0.8723\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1116 - accuracy: 0.8564\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1104 - accuracy: 0.8617\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1090 - accuracy: 0.8617\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1058 - accuracy: 0.8564\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1030 - accuracy: 0.8564\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0984 - accuracy: 0.8777\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0979 - accuracy: 0.8723\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0913 - accuracy: 0.9202\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0931 - accuracy: 0.8936\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0888 - accuracy: 0.8830\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0900 - accuracy: 0.8883\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0867 - accuracy: 0.8989\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0872 - accuracy: 0.9043\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0852 - accuracy: 0.8989\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0816 - accuracy: 0.8989\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0824 - accuracy: 0.8830\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0851 - accuracy: 0.9202\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0761 - accuracy: 0.9096\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0791 - accuracy: 0.8989\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0730 - accuracy: 0.9255\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0737 - accuracy: 0.8989\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0704 - accuracy: 0.9309\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0682 - accuracy: 0.9202\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0663 - accuracy: 0.9362\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0657 - accuracy: 0.9362\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0639 - accuracy: 0.9415\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0631 - accuracy: 0.9149\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0692 - accuracy: 0.9255\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0606 - accuracy: 0.9521\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0604 - accuracy: 0.9309\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0621 - accuracy: 0.9255\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0558 - accuracy: 0.9574\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0536 - accuracy: 0.9521\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0556 - accuracy: 0.9468\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0582 - accuracy: 0.9468\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0520 - accuracy: 0.9521\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0491 - accuracy: 0.9468\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0503 - accuracy: 0.9574\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0482 - accuracy: 0.9574\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0487 - accuracy: 0.9521\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0476 - accuracy: 0.9521\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0434 - accuracy: 0.9628\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0432 - accuracy: 0.9521\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0429 - accuracy: 0.9574\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0398 - accuracy: 0.9681\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0452 - accuracy: 0.9628\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0404 - accuracy: 0.9787\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0367 - accuracy: 0.9734\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0426 - accuracy: 0.9468\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 0.9574\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0383 - accuracy: 0.9681\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0338 - accuracy: 0.9840\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0379 - accuracy: 0.9574\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0353 - accuracy: 0.9734\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0349 - accuracy: 0.9574\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0341 - accuracy: 0.9787\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0295 - accuracy: 0.9734\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0295 - accuracy: 0.9787\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.9787\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0325 - accuracy: 0.9787\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0278 - accuracy: 0.9840\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0288 - accuracy: 0.9734\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.9787\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0309 - accuracy: 0.9681\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0245 - accuracy: 0.9840\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0234 - accuracy: 0.9840\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0226 - accuracy: 0.9840\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0248 - accuracy: 0.9894\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0244 - accuracy: 0.9787\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0201 - accuracy: 0.9894\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 0.9840\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 0.9947\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 0.9840\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0197 - accuracy: 0.9894\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 0.9894\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0177 - accuracy: 0.9947\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0169 - accuracy: 0.9947\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.9894\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0163 - accuracy: 0.9947\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 0.9894\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.1309 - accuracy: 0.8500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n %.f fold accuracy:\" % n_fold, accuracy) # 실행후 10번의 값들"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDK25J57_bDo",
        "outputId": "d4affc69-5233-44c8-ffda-531e27d5eb4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 10 fold accuracy: ['0.7619', '0.7619', '0.7619', '0.9524', '0.8095', '0.8571', '0.8571', '0.8095', '1.0000', '0.8500']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tNcyURMWADlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DoWCSZ3uADou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14장 베스트 모델 만들기 "
      ],
      "metadata": {
        "id": "mMM78orPAE0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_pre = pd.read_csv('dataset/wine.csv',header=None)  # 레드와인과 화이트 와인 구분하는 테스크\n",
        "#df = df_pre.sample(frac=1)\n",
        "df = df_pre.sample(frac=0.15)"
      ],
      "metadata": {
        "id": "OxvTHN-lAJ6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Xb8ld1HKJTcp",
        "outputId": "e16ee067-3051-48e3-b1e7-e48c34b3e0e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a4830f6a-8dd4-4114-a3fd-f956b1254a64\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5015</th>\n",
              "      <td>7.2</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.40</td>\n",
              "      <td>11.6</td>\n",
              "      <td>0.032</td>\n",
              "      <td>34.0</td>\n",
              "      <td>214.0</td>\n",
              "      <td>0.99630</td>\n",
              "      <td>3.10</td>\n",
              "      <td>0.51</td>\n",
              "      <td>9.8</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5799</th>\n",
              "      <td>8.0</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.33</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.044</td>\n",
              "      <td>28.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>0.99035</td>\n",
              "      <td>3.03</td>\n",
              "      <td>0.43</td>\n",
              "      <td>12.5</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4051</th>\n",
              "      <td>7.3</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.27</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.027</td>\n",
              "      <td>35.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>0.99248</td>\n",
              "      <td>3.38</td>\n",
              "      <td>0.54</td>\n",
              "      <td>11.0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1974</th>\n",
              "      <td>5.1</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.22</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.027</td>\n",
              "      <td>18.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>0.98930</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.38</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2368</th>\n",
              "      <td>6.7</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.34</td>\n",
              "      <td>8.8</td>\n",
              "      <td>0.043</td>\n",
              "      <td>41.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>0.99620</td>\n",
              "      <td>3.42</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.3</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4830f6a-8dd4-4114-a3fd-f956b1254a64')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a4830f6a-8dd4-4114-a3fd-f956b1254a64 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a4830f6a-8dd4-4114-a3fd-f956b1254a64');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       0     1     2     3      4     5   ...       7     8     9     10  11  12\n",
              "5015  7.2  0.37  0.40  11.6  0.032  34.0  ...  0.99630  3.10  0.51   9.8   6   0\n",
              "5799  8.0  0.24  0.33   1.2  0.044  28.0  ...  0.99035  3.03  0.43  12.5   6   0\n",
              "4051  7.3  0.19  0.27   1.6  0.027  35.0  ...  0.99248  3.38  0.54  11.0   7   0\n",
              "1974  5.1  0.33  0.22   1.6  0.027  18.0  ...  0.98930  3.51  0.38  12.5   7   0\n",
              "2368  6.7  0.11  0.34   8.8  0.043  41.0  ...  0.99620  3.42  0.40   9.3   7   0\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIUP-qXTAKPZ",
        "outputId": "497f8b07-6e5f-4848-c388-365170fc3afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 6497 entries, 3553 to 5267\n",
            "Data columns (total 13 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       6497 non-null   float64\n",
            " 1   1       6497 non-null   float64\n",
            " 2   2       6497 non-null   float64\n",
            " 3   3       6497 non-null   float64\n",
            " 4   4       6497 non-null   float64\n",
            " 5   5       6497 non-null   float64\n",
            " 6   6       6497 non-null   float64\n",
            " 7   7       6497 non-null   float64\n",
            " 8   8       6497 non-null   float64\n",
            " 9   9       6497 non-null   float64\n",
            " 10  10      6497 non-null   float64\n",
            " 11  11      6497 non-null   int64  \n",
            " 12  12      6497 non-null   int64  \n",
            "dtypes: float64(11), int64(2)\n",
            "memory usage: 710.6 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=df.values\n",
        "X=dataset[:,0:12]\n",
        "Y=dataset[:,12]\n"
      ],
      "metadata": {
        "id": "6woHqqlzAKR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "seed=0\n",
        "numpy.random.seed(3)\n",
        "tf.random.set_seed(3)\n"
      ],
      "metadata": {
        "id": "FdqmayAaJfKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= Sequential()\n",
        "model.add(Dense(30, input_dim=12, activation='relu'))\n",
        "model.add(Dense(12,activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n"
      ],
      "metadata": {
        "id": "fj5qLI5uJzDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer ='adam',metrics = ['accuracy'])\n"
      ],
      "metadata": {
        "id": "J0n8gF84Kyts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,Y, epochs=200, batch_size=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Peu6ItXdK5Ng",
        "outputId": "f8900906-3c3c-474e-d96f-6cbc7993ac70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "33/33 [==============================] - 1s 2ms/step - loss: 0.6388 - accuracy: 0.7599\n",
            "Epoch 2/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.3008 - accuracy: 0.8969\n",
            "Epoch 3/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2475 - accuracy: 0.9217\n",
            "Epoch 4/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2283 - accuracy: 0.9263\n",
            "Epoch 5/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2204 - accuracy: 0.9274\n",
            "Epoch 6/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2112 - accuracy: 0.9295\n",
            "Epoch 7/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.2042 - accuracy: 0.9318\n",
            "Epoch 8/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1953 - accuracy: 0.9344\n",
            "Epoch 9/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1917 - accuracy: 0.9334\n",
            "Epoch 10/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1854 - accuracy: 0.9364\n",
            "Epoch 11/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1838 - accuracy: 0.9384\n",
            "Epoch 12/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1790 - accuracy: 0.9386\n",
            "Epoch 13/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1739 - accuracy: 0.9387\n",
            "Epoch 14/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1697 - accuracy: 0.9406\n",
            "Epoch 15/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1661 - accuracy: 0.9410\n",
            "Epoch 16/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1634 - accuracy: 0.9417\n",
            "Epoch 17/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1613 - accuracy: 0.9440\n",
            "Epoch 18/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1565 - accuracy: 0.9434\n",
            "Epoch 19/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1554 - accuracy: 0.9438\n",
            "Epoch 20/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1473 - accuracy: 0.9449\n",
            "Epoch 21/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1437 - accuracy: 0.9455\n",
            "Epoch 22/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1426 - accuracy: 0.9481\n",
            "Epoch 23/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1354 - accuracy: 0.9517\n",
            "Epoch 24/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1309 - accuracy: 0.9527\n",
            "Epoch 25/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1252 - accuracy: 0.9529\n",
            "Epoch 26/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1266 - accuracy: 0.9558\n",
            "Epoch 27/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1183 - accuracy: 0.9583\n",
            "Epoch 28/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1155 - accuracy: 0.9597\n",
            "Epoch 29/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1174 - accuracy: 0.9577\n",
            "Epoch 30/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1105 - accuracy: 0.9628\n",
            "Epoch 31/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1069 - accuracy: 0.9640\n",
            "Epoch 32/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1047 - accuracy: 0.9644\n",
            "Epoch 33/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1023 - accuracy: 0.9658\n",
            "Epoch 34/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1026 - accuracy: 0.9660\n",
            "Epoch 35/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.1002 - accuracy: 0.9671\n",
            "Epoch 36/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0980 - accuracy: 0.9686\n",
            "Epoch 37/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0941 - accuracy: 0.9700\n",
            "Epoch 38/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0932 - accuracy: 0.9701\n",
            "Epoch 39/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0909 - accuracy: 0.9717\n",
            "Epoch 40/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0904 - accuracy: 0.9709\n",
            "Epoch 41/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0893 - accuracy: 0.9721\n",
            "Epoch 42/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0875 - accuracy: 0.9720\n",
            "Epoch 43/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0867 - accuracy: 0.9735\n",
            "Epoch 44/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0855 - accuracy: 0.9735\n",
            "Epoch 45/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0829 - accuracy: 0.9731\n",
            "Epoch 46/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0809 - accuracy: 0.9746\n",
            "Epoch 47/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0810 - accuracy: 0.9752\n",
            "Epoch 48/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0801 - accuracy: 0.9757\n",
            "Epoch 49/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0775 - accuracy: 0.9761\n",
            "Epoch 50/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0813 - accuracy: 0.9751\n",
            "Epoch 51/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0756 - accuracy: 0.9769\n",
            "Epoch 52/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0775 - accuracy: 0.9758\n",
            "Epoch 53/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0777 - accuracy: 0.9749\n",
            "Epoch 54/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0746 - accuracy: 0.9769\n",
            "Epoch 55/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9774\n",
            "Epoch 56/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0771 - accuracy: 0.9765\n",
            "Epoch 57/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0726 - accuracy: 0.9766\n",
            "Epoch 58/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.9757\n",
            "Epoch 59/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0736 - accuracy: 0.9763\n",
            "Epoch 60/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0732 - accuracy: 0.9769\n",
            "Epoch 61/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0713 - accuracy: 0.9778\n",
            "Epoch 62/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0711 - accuracy: 0.9775\n",
            "Epoch 63/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0696 - accuracy: 0.9786\n",
            "Epoch 64/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0675 - accuracy: 0.9780\n",
            "Epoch 65/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0678 - accuracy: 0.9800\n",
            "Epoch 66/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0701 - accuracy: 0.9777\n",
            "Epoch 67/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0791 - accuracy: 0.9738\n",
            "Epoch 68/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0687 - accuracy: 0.9789\n",
            "Epoch 69/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0673 - accuracy: 0.9785\n",
            "Epoch 70/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0668 - accuracy: 0.9788\n",
            "Epoch 71/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0647 - accuracy: 0.9800\n",
            "Epoch 72/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0710 - accuracy: 0.9789\n",
            "Epoch 73/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0651 - accuracy: 0.9792\n",
            "Epoch 74/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0668 - accuracy: 0.9789\n",
            "Epoch 75/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0617 - accuracy: 0.9800\n",
            "Epoch 76/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0628 - accuracy: 0.9814\n",
            "Epoch 77/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0668 - accuracy: 0.9794\n",
            "Epoch 78/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0663 - accuracy: 0.9797\n",
            "Epoch 79/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0668 - accuracy: 0.9795\n",
            "Epoch 80/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0690 - accuracy: 0.9783\n",
            "Epoch 81/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0640 - accuracy: 0.9800\n",
            "Epoch 82/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0653 - accuracy: 0.9803\n",
            "Epoch 83/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0602 - accuracy: 0.9818\n",
            "Epoch 84/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0630 - accuracy: 0.9811\n",
            "Epoch 85/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0645 - accuracy: 0.9798\n",
            "Epoch 86/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0683 - accuracy: 0.9785\n",
            "Epoch 87/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0605 - accuracy: 0.9808\n",
            "Epoch 88/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0605 - accuracy: 0.9817\n",
            "Epoch 89/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0620 - accuracy: 0.9826\n",
            "Epoch 90/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0619 - accuracy: 0.9794\n",
            "Epoch 91/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0662 - accuracy: 0.9803\n",
            "Epoch 92/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0620 - accuracy: 0.9811\n",
            "Epoch 93/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0656 - accuracy: 0.9781\n",
            "Epoch 94/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0659 - accuracy: 0.9803\n",
            "Epoch 95/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0613 - accuracy: 0.9820\n",
            "Epoch 96/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0585 - accuracy: 0.9815\n",
            "Epoch 97/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0578 - accuracy: 0.9829\n",
            "Epoch 98/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0589 - accuracy: 0.9832\n",
            "Epoch 99/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0636 - accuracy: 0.9798\n",
            "Epoch 100/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0619 - accuracy: 0.9803\n",
            "Epoch 101/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0574 - accuracy: 0.9829\n",
            "Epoch 102/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0577 - accuracy: 0.9818\n",
            "Epoch 103/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0556 - accuracy: 0.9828\n",
            "Epoch 104/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0606 - accuracy: 0.9815\n",
            "Epoch 105/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0592 - accuracy: 0.9823\n",
            "Epoch 106/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0624 - accuracy: 0.9821\n",
            "Epoch 107/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0610 - accuracy: 0.9806\n",
            "Epoch 108/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0552 - accuracy: 0.9826\n",
            "Epoch 109/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0727 - accuracy: 0.9781\n",
            "Epoch 110/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0762 - accuracy: 0.9772\n",
            "Epoch 111/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0711 - accuracy: 0.9774\n",
            "Epoch 112/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0581 - accuracy: 0.9828\n",
            "Epoch 113/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0558 - accuracy: 0.9831\n",
            "Epoch 114/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0602 - accuracy: 0.9815\n",
            "Epoch 115/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.9828\n",
            "Epoch 116/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0587 - accuracy: 0.9828\n",
            "Epoch 117/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0558 - accuracy: 0.9835\n",
            "Epoch 118/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0561 - accuracy: 0.9834\n",
            "Epoch 119/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0547 - accuracy: 0.9835\n",
            "Epoch 120/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0547 - accuracy: 0.9837\n",
            "Epoch 121/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0550 - accuracy: 0.9831\n",
            "Epoch 122/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0555 - accuracy: 0.9829\n",
            "Epoch 123/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0531 - accuracy: 0.9835\n",
            "Epoch 124/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0577 - accuracy: 0.9829\n",
            "Epoch 125/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0597 - accuracy: 0.9820\n",
            "Epoch 126/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0553 - accuracy: 0.9831\n",
            "Epoch 127/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0572 - accuracy: 0.9821\n",
            "Epoch 128/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0521 - accuracy: 0.9846\n",
            "Epoch 129/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0552 - accuracy: 0.9840\n",
            "Epoch 130/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0563 - accuracy: 0.9825\n",
            "Epoch 131/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0526 - accuracy: 0.9851\n",
            "Epoch 132/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0525 - accuracy: 0.9849\n",
            "Epoch 133/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0544 - accuracy: 0.9841\n",
            "Epoch 134/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0565 - accuracy: 0.9815\n",
            "Epoch 135/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0532 - accuracy: 0.9829\n",
            "Epoch 136/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0609 - accuracy: 0.9823\n",
            "Epoch 137/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0534 - accuracy: 0.9835\n",
            "Epoch 138/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0565 - accuracy: 0.9834\n",
            "Epoch 139/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0583 - accuracy: 0.9825\n",
            "Epoch 140/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0637 - accuracy: 0.9794\n",
            "Epoch 141/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0564 - accuracy: 0.9828\n",
            "Epoch 142/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0546 - accuracy: 0.9831\n",
            "Epoch 143/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0521 - accuracy: 0.9837\n",
            "Epoch 144/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0559 - accuracy: 0.9841\n",
            "Epoch 145/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0544 - accuracy: 0.9831\n",
            "Epoch 146/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0544 - accuracy: 0.9840\n",
            "Epoch 147/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0563 - accuracy: 0.9815\n",
            "Epoch 148/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0515 - accuracy: 0.9846\n",
            "Epoch 149/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0516 - accuracy: 0.9843\n",
            "Epoch 150/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.9835\n",
            "Epoch 151/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0530 - accuracy: 0.9838\n",
            "Epoch 152/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0564 - accuracy: 0.9823\n",
            "Epoch 153/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0571 - accuracy: 0.9829\n",
            "Epoch 154/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0549 - accuracy: 0.9837\n",
            "Epoch 155/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0548 - accuracy: 0.9834\n",
            "Epoch 156/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0566 - accuracy: 0.9825\n",
            "Epoch 157/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0653 - accuracy: 0.9806\n",
            "Epoch 158/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0510 - accuracy: 0.9852\n",
            "Epoch 159/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0522 - accuracy: 0.9845\n",
            "Epoch 160/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0637 - accuracy: 0.9791\n",
            "Epoch 161/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0543 - accuracy: 0.9825\n",
            "Epoch 162/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0531 - accuracy: 0.9841\n",
            "Epoch 163/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0521 - accuracy: 0.9841\n",
            "Epoch 164/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0503 - accuracy: 0.9846\n",
            "Epoch 165/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0505 - accuracy: 0.9854\n",
            "Epoch 166/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0524 - accuracy: 0.9834\n",
            "Epoch 167/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0648 - accuracy: 0.9783\n",
            "Epoch 168/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0543 - accuracy: 0.9828\n",
            "Epoch 169/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0514 - accuracy: 0.9849\n",
            "Epoch 170/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0493 - accuracy: 0.9860\n",
            "Epoch 171/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0507 - accuracy: 0.9846\n",
            "Epoch 172/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0520 - accuracy: 0.9841\n",
            "Epoch 173/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0502 - accuracy: 0.9845\n",
            "Epoch 174/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0519 - accuracy: 0.9831\n",
            "Epoch 175/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0502 - accuracy: 0.9848\n",
            "Epoch 176/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0505 - accuracy: 0.9852\n",
            "Epoch 177/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0502 - accuracy: 0.9837\n",
            "Epoch 178/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0577 - accuracy: 0.9829\n",
            "Epoch 179/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0547 - accuracy: 0.9821\n",
            "Epoch 180/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0504 - accuracy: 0.9848\n",
            "Epoch 181/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0508 - accuracy: 0.9848\n",
            "Epoch 182/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0522 - accuracy: 0.9846\n",
            "Epoch 183/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0557 - accuracy: 0.9826\n",
            "Epoch 184/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0517 - accuracy: 0.9835\n",
            "Epoch 185/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.9818\n",
            "Epoch 186/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0499 - accuracy: 0.9857\n",
            "Epoch 187/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0503 - accuracy: 0.9854\n",
            "Epoch 188/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0501 - accuracy: 0.9845\n",
            "Epoch 189/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0496 - accuracy: 0.9855\n",
            "Epoch 190/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0516 - accuracy: 0.9837\n",
            "Epoch 191/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0648 - accuracy: 0.9794\n",
            "Epoch 192/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0530 - accuracy: 0.9837\n",
            "Epoch 193/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0555 - accuracy: 0.9820\n",
            "Epoch 194/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0565 - accuracy: 0.9812\n",
            "Epoch 195/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0539 - accuracy: 0.9828\n",
            "Epoch 196/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0495 - accuracy: 0.9848\n",
            "Epoch 197/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0501 - accuracy: 0.9837\n",
            "Epoch 198/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0493 - accuracy: 0.9861\n",
            "Epoch 199/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0496 - accuracy: 0.9845\n",
            "Epoch 200/200\n",
            "33/33 [==============================] - 0s 2ms/step - loss: 0.0534 - accuracy: 0.9837\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb965690490>"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X,Y)[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkSDCheAK9Gy",
        "outputId": "474b6dd9-b797-442e-a8c4-53fe38f706b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "204/204 [==============================] - 0s 1ms/step - loss: 0.0498 - accuracy: 0.9860\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9859935641288757"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result=model.evaluate(X,Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXxpB1cYLBPj",
        "outputId": "229d6076-fbcc-4d5e-bac5-85a810c95de7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "204/204 [==============================] - 0s 1ms/step - loss: 0.0498 - accuracy: 0.9860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgyvGLgpLKF-",
        "outputId": "be101f8f-bd6e-40be-9690-d3199a8eda2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.049805670976638794"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsJt0CNtLLzB",
        "outputId": "899f5330-4f8c-479b-e0fa-c1d4b498cdc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9859935641288757"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 업데이트하기\n",
        "# 모델을 저장하고, 재사용하는 방법 : save(), load_model()\n",
        "# 모델 저장은 hdf5라는 확장자로 저장됨"
      ],
      "metadata": {
        "id": "lMnU59GDLKhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "MODEL_DIR ='/content/model'"
      ],
      "metadata": {
        "id": "QfUrkdmcv2fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(MODEL_DIR):\n",
        "  os.mkdir(MODEL_DIR)"
      ],
      "metadata": {
        "id": "J0WbBbxzv6yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelpath='/content/model/{epoch:02d}-{val_loss:.4f}.hdf5'"
      ],
      "metadata": {
        "id": "qc5P8Rfbw3Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint #  체크포인트라는 변수를 만들어 이곳에 모니터할 값을 지정\n",
        "# verbose =1 : 해당함수의 진행사항이 출력되고, 0으로 정하면 출력안됨\n"
      ],
      "metadata": {
        "id": "TwPBQbfBwCFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1)"
      ],
      "metadata": {
        "id": "jlZDJ8XCxIV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,Y, validation_split=0.2, epochs=200, batch_size=200, callbacks=[checkpointer]) \n",
        "# model 폴더에 0-199까지 총 200개의 모델이 저장됨\n",
        "# 이때 함수에 모델이 앞서 저장한 모델보다 나아졌을때만 저장 하게끔 하려면 save_best_only 설정"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6GoNLbVxO7C",
        "outputId": "e9e19273-be21-42b6-d79a-0524ae075958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0117 - accuracy: 1.0000\n",
            "Epoch 00001: saving model to /content/model/01-0.0546.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0392 - accuracy: 0.9892 - val_loss: 0.0546 - val_accuracy: 0.9869\n",
            "Epoch 2/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 0.9896\n",
            "Epoch 00002: saving model to /content/model/02-0.0547.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0378 - accuracy: 0.9896 - val_loss: 0.0547 - val_accuracy: 0.9862\n",
            "Epoch 3/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9894\n",
            "Epoch 00003: saving model to /content/model/03-0.0533.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0400 - accuracy: 0.9894 - val_loss: 0.0533 - val_accuracy: 0.9854\n",
            "Epoch 4/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0686 - accuracy: 0.9900\n",
            "Epoch 00004: saving model to /content/model/04-0.0585.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0372 - accuracy: 0.9913 - val_loss: 0.0585 - val_accuracy: 0.9877\n",
            "Epoch 5/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0246 - accuracy: 0.9950\n",
            "Epoch 00005: saving model to /content/model/05-0.0571.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0384 - accuracy: 0.9900 - val_loss: 0.0571 - val_accuracy: 0.9885\n",
            "Epoch 6/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0212 - accuracy: 0.9950\n",
            "Epoch 00006: saving model to /content/model/06-0.0550.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0438 - accuracy: 0.9871 - val_loss: 0.0550 - val_accuracy: 0.9862\n",
            "Epoch 7/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0425 - accuracy: 0.9900\n",
            "Epoch 00007: saving model to /content/model/07-0.0616.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0378 - accuracy: 0.9904 - val_loss: 0.0616 - val_accuracy: 0.9869\n",
            "Epoch 8/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9898\n",
            "Epoch 00008: saving model to /content/model/08-0.0539.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0404 - accuracy: 0.9898 - val_loss: 0.0539 - val_accuracy: 0.9862\n",
            "Epoch 9/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0700 - accuracy: 0.9750\n",
            "Epoch 00009: saving model to /content/model/09-0.0586.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0380 - accuracy: 0.9890 - val_loss: 0.0586 - val_accuracy: 0.9877\n",
            "Epoch 10/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0389 - accuracy: 0.9887\n",
            "Epoch 00010: saving model to /content/model/10-0.0552.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0373 - accuracy: 0.9890 - val_loss: 0.0552 - val_accuracy: 0.9869\n",
            "Epoch 11/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9908\n",
            "Epoch 00011: saving model to /content/model/11-0.0551.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0373 - accuracy: 0.9908 - val_loss: 0.0551 - val_accuracy: 0.9877\n",
            "Epoch 12/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0382 - accuracy: 0.9893\n",
            "Epoch 00012: saving model to /content/model/12-0.0655.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0382 - accuracy: 0.9892 - val_loss: 0.0655 - val_accuracy: 0.9838\n",
            "Epoch 13/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0387 - accuracy: 0.9896\n",
            "Epoch 00013: saving model to /content/model/13-0.0542.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0374 - accuracy: 0.9898 - val_loss: 0.0542 - val_accuracy: 0.9854\n",
            "Epoch 14/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0404 - accuracy: 0.9882\n",
            "Epoch 00014: saving model to /content/model/14-0.0560.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0394 - accuracy: 0.9885 - val_loss: 0.0560 - val_accuracy: 0.9838\n",
            "Epoch 15/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0371 - accuracy: 0.9898\n",
            "Epoch 00015: saving model to /content/model/15-0.0542.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0373 - accuracy: 0.9898 - val_loss: 0.0542 - val_accuracy: 0.9869\n",
            "Epoch 16/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0402 - accuracy: 0.9892\n",
            "Epoch 00016: saving model to /content/model/16-0.0640.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0398 - accuracy: 0.9894 - val_loss: 0.0640 - val_accuracy: 0.9854\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9888\n",
            "Epoch 00017: saving model to /content/model/17-0.0566.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0392 - accuracy: 0.9888 - val_loss: 0.0566 - val_accuracy: 0.9831\n",
            "Epoch 18/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 00018: saving model to /content/model/18-0.0539.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0383 - accuracy: 0.9900 - val_loss: 0.0539 - val_accuracy: 0.9877\n",
            "Epoch 19/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0385 - accuracy: 0.9885\n",
            "Epoch 00019: saving model to /content/model/19-0.0566.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0376 - accuracy: 0.9892 - val_loss: 0.0566 - val_accuracy: 0.9877\n",
            "Epoch 20/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0381 - accuracy: 0.9889\n",
            "Epoch 00020: saving model to /content/model/20-0.0534.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0372 - accuracy: 0.9894 - val_loss: 0.0534 - val_accuracy: 0.9854\n",
            "Epoch 21/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0370 - accuracy: 0.9898\n",
            "Epoch 00021: saving model to /content/model/21-0.0584.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0386 - accuracy: 0.9894 - val_loss: 0.0584 - val_accuracy: 0.9877\n",
            "Epoch 22/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0461 - accuracy: 0.9883\n",
            "Epoch 00022: saving model to /content/model/22-0.0606.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0449 - accuracy: 0.9883 - val_loss: 0.0606 - val_accuracy: 0.9815\n",
            "Epoch 23/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0885 - accuracy: 0.9750\n",
            "Epoch 00023: saving model to /content/model/23-0.0563.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0417 - accuracy: 0.9881 - val_loss: 0.0563 - val_accuracy: 0.9846\n",
            "Epoch 24/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0435 - accuracy: 0.9862\n",
            "Epoch 00024: saving model to /content/model/24-0.0634.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0424 - accuracy: 0.9869 - val_loss: 0.0634 - val_accuracy: 0.9838\n",
            "Epoch 25/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0194 - accuracy: 0.9950\n",
            "Epoch 00025: saving model to /content/model/25-0.0530.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0401 - accuracy: 0.9892 - val_loss: 0.0530 - val_accuracy: 0.9854\n",
            "Epoch 26/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0382 - accuracy: 0.9850\n",
            "Epoch 00026: saving model to /content/model/26-0.0596.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0383 - accuracy: 0.9890 - val_loss: 0.0596 - val_accuracy: 0.9869\n",
            "Epoch 27/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0677 - accuracy: 0.9750\n",
            "Epoch 00027: saving model to /content/model/27-0.0568.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0384 - accuracy: 0.9888 - val_loss: 0.0568 - val_accuracy: 0.9877\n",
            "Epoch 28/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0365 - accuracy: 0.9887\n",
            "Epoch 00028: saving model to /content/model/28-0.0593.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0384 - accuracy: 0.9883 - val_loss: 0.0593 - val_accuracy: 0.9877\n",
            "Epoch 29/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0862 - accuracy: 0.9800\n",
            "Epoch 00029: saving model to /content/model/29-0.0552.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0406 - accuracy: 0.9886 - val_loss: 0.0552 - val_accuracy: 0.9885\n",
            "Epoch 30/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0359 - accuracy: 0.9895\n",
            "Epoch 00030: saving model to /content/model/30-0.0539.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0383 - accuracy: 0.9896 - val_loss: 0.0539 - val_accuracy: 0.9854\n",
            "Epoch 31/200\n",
            "20/26 [======================>.......] - ETA: 0s - loss: 0.0408 - accuracy: 0.9895\n",
            "Epoch 00031: saving model to /content/model/31-0.0579.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0412 - accuracy: 0.9888 - val_loss: 0.0579 - val_accuracy: 0.9885\n",
            "Epoch 32/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0237 - accuracy: 0.9950\n",
            "Epoch 00032: saving model to /content/model/32-0.0621.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0376 - accuracy: 0.9894 - val_loss: 0.0621 - val_accuracy: 0.9831\n",
            "Epoch 33/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0426 - accuracy: 0.9882\n",
            "Epoch 00033: saving model to /content/model/33-0.0559.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0409 - accuracy: 0.9879 - val_loss: 0.0559 - val_accuracy: 0.9892\n",
            "Epoch 34/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9865\n",
            "Epoch 00034: saving model to /content/model/34-0.0682.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0444 - accuracy: 0.9865 - val_loss: 0.0682 - val_accuracy: 0.9846\n",
            "Epoch 35/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0355 - accuracy: 0.9950\n",
            "Epoch 00035: saving model to /content/model/35-0.0539.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0398 - accuracy: 0.9883 - val_loss: 0.0539 - val_accuracy: 0.9854\n",
            "Epoch 36/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0479 - accuracy: 0.9863\n",
            "Epoch 00036: saving model to /content/model/36-0.0617.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0451 - accuracy: 0.9871 - val_loss: 0.0617 - val_accuracy: 0.9846\n",
            "Epoch 37/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9886\n",
            "Epoch 00037: saving model to /content/model/37-0.0539.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0386 - accuracy: 0.9886 - val_loss: 0.0539 - val_accuracy: 0.9854\n",
            "Epoch 38/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0228 - accuracy: 0.9850\n",
            "Epoch 00038: saving model to /content/model/38-0.0557.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0414 - accuracy: 0.9871 - val_loss: 0.0557 - val_accuracy: 0.9869\n",
            "Epoch 39/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0394 - accuracy: 0.9880\n",
            "Epoch 00039: saving model to /content/model/39-0.0604.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0395 - accuracy: 0.9877 - val_loss: 0.0604 - val_accuracy: 0.9823\n",
            "Epoch 40/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0387 - accuracy: 0.9900\n",
            "Epoch 00040: saving model to /content/model/40-0.0544.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0387 - accuracy: 0.9898 - val_loss: 0.0544 - val_accuracy: 0.9877\n",
            "Epoch 41/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0403 - accuracy: 0.9890\n",
            "Epoch 00041: saving model to /content/model/41-0.0616.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0391 - accuracy: 0.9892 - val_loss: 0.0616 - val_accuracy: 0.9869\n",
            "Epoch 42/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0385 - accuracy: 0.9900\n",
            "Epoch 00042: saving model to /content/model/42-0.0614.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0356 - accuracy: 0.9908 - val_loss: 0.0614 - val_accuracy: 0.9869\n",
            "Epoch 43/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0423 - accuracy: 0.9861\n",
            "Epoch 00043: saving model to /content/model/43-0.0544.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0423 - accuracy: 0.9861 - val_loss: 0.0544 - val_accuracy: 0.9846\n",
            "Epoch 44/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9877\n",
            "Epoch 00044: saving model to /content/model/44-0.0546.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0442 - accuracy: 0.9877 - val_loss: 0.0546 - val_accuracy: 0.9869\n",
            "Epoch 45/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 00045: saving model to /content/model/45-0.0702.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0372 - accuracy: 0.9892 - val_loss: 0.0702 - val_accuracy: 0.9831\n",
            "Epoch 46/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9896\n",
            "Epoch 00046: saving model to /content/model/46-0.0560.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0384 - accuracy: 0.9896 - val_loss: 0.0560 - val_accuracy: 0.9862\n",
            "Epoch 47/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0119 - accuracy: 1.0000\n",
            "Epoch 00047: saving model to /content/model/47-0.0576.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0384 - accuracy: 0.9892 - val_loss: 0.0576 - val_accuracy: 0.9892\n",
            "Epoch 48/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0282 - accuracy: 0.9900\n",
            "Epoch 00048: saving model to /content/model/48-0.0618.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0422 - accuracy: 0.9892 - val_loss: 0.0618 - val_accuracy: 0.9823\n",
            "Epoch 49/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0116 - accuracy: 0.9950\n",
            "Epoch 00049: saving model to /content/model/49-0.0607.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0415 - accuracy: 0.9871 - val_loss: 0.0607 - val_accuracy: 0.9877\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9890\n",
            "Epoch 00050: saving model to /content/model/50-0.0549.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0387 - accuracy: 0.9890 - val_loss: 0.0549 - val_accuracy: 0.9885\n",
            "Epoch 51/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0379 - accuracy: 0.9900\n",
            "Epoch 00051: saving model to /content/model/51-0.0547.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0360 - accuracy: 0.9900 - val_loss: 0.0547 - val_accuracy: 0.9877\n",
            "Epoch 52/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0117 - accuracy: 0.9950\n",
            "Epoch 00052: saving model to /content/model/52-0.0540.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0346 - accuracy: 0.9902 - val_loss: 0.0540 - val_accuracy: 0.9869\n",
            "Epoch 53/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0366 - accuracy: 0.9900\n",
            "Epoch 00053: saving model to /content/model/53-0.0576.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0383 - accuracy: 0.9894 - val_loss: 0.0576 - val_accuracy: 0.9877\n",
            "Epoch 54/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0598 - accuracy: 0.9815\n",
            "Epoch 00054: saving model to /content/model/54-0.1022.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0606 - accuracy: 0.9810 - val_loss: 0.1022 - val_accuracy: 0.9646\n",
            "Epoch 55/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0515 - accuracy: 0.9854\n",
            "Epoch 00055: saving model to /content/model/55-0.0661.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0509 - accuracy: 0.9858 - val_loss: 0.0661 - val_accuracy: 0.9838\n",
            "Epoch 56/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0352 - accuracy: 0.9900\n",
            "Epoch 00056: saving model to /content/model/56-0.0664.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0409 - accuracy: 0.9875 - val_loss: 0.0664 - val_accuracy: 0.9838\n",
            "Epoch 57/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0357 - accuracy: 0.9900\n",
            "Epoch 00057: saving model to /content/model/57-0.0598.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0353 - accuracy: 0.9902 - val_loss: 0.0598 - val_accuracy: 0.9885\n",
            "Epoch 58/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0292 - accuracy: 0.9850\n",
            "Epoch 00058: saving model to /content/model/58-0.0593.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0349 - accuracy: 0.9898 - val_loss: 0.0593 - val_accuracy: 0.9892\n",
            "Epoch 59/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0195 - accuracy: 0.9950\n",
            "Epoch 00059: saving model to /content/model/59-0.0717.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0360 - accuracy: 0.9892 - val_loss: 0.0717 - val_accuracy: 0.9831\n",
            "Epoch 60/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0231 - accuracy: 0.9950\n",
            "Epoch 00060: saving model to /content/model/60-0.0567.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0371 - accuracy: 0.9900 - val_loss: 0.0567 - val_accuracy: 0.9885\n",
            "Epoch 61/200\n",
            "19/26 [====================>.........] - ETA: 0s - loss: 0.0379 - accuracy: 0.9887\n",
            "Epoch 00061: saving model to /content/model/61-0.0565.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0352 - accuracy: 0.9898 - val_loss: 0.0565 - val_accuracy: 0.9892\n",
            "Epoch 62/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0399 - accuracy: 0.9950\n",
            "Epoch 00062: saving model to /content/model/62-0.0617.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0375 - accuracy: 0.9900 - val_loss: 0.0617 - val_accuracy: 0.9877\n",
            "Epoch 63/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0160 - accuracy: 1.0000\n",
            "Epoch 00063: saving model to /content/model/63-0.0546.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0351 - accuracy: 0.9910 - val_loss: 0.0546 - val_accuracy: 0.9869\n",
            "Epoch 64/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0420 - accuracy: 0.9878\n",
            "Epoch 00064: saving model to /content/model/64-0.0709.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0431 - accuracy: 0.9879 - val_loss: 0.0709 - val_accuracy: 0.9854\n",
            "Epoch 65/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0301 - accuracy: 0.9900\n",
            "Epoch 00065: saving model to /content/model/65-0.0861.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0413 - accuracy: 0.9877 - val_loss: 0.0861 - val_accuracy: 0.9792\n",
            "Epoch 66/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9877\n",
            "Epoch 00066: saving model to /content/model/66-0.0574.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0415 - accuracy: 0.9877 - val_loss: 0.0574 - val_accuracy: 0.9838\n",
            "Epoch 67/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9883\n",
            "Epoch 00067: saving model to /content/model/67-0.0561.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0396 - accuracy: 0.9883 - val_loss: 0.0561 - val_accuracy: 0.9854\n",
            "Epoch 68/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0353 - accuracy: 0.9800\n",
            "Epoch 00068: saving model to /content/model/68-0.0548.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0346 - accuracy: 0.9908 - val_loss: 0.0548 - val_accuracy: 0.9869\n",
            "Epoch 69/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0354 - accuracy: 0.9900\n",
            "Epoch 00069: saving model to /content/model/69-0.0563.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0343 - accuracy: 0.9902 - val_loss: 0.0563 - val_accuracy: 0.9862\n",
            "Epoch 70/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0246 - accuracy: 0.9900\n",
            "Epoch 00070: saving model to /content/model/70-0.0595.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0347 - accuracy: 0.9908 - val_loss: 0.0595 - val_accuracy: 0.9877\n",
            "Epoch 71/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0528 - accuracy: 0.9850\n",
            "Epoch 00071: saving model to /content/model/71-0.0552.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0359 - accuracy: 0.9904 - val_loss: 0.0552 - val_accuracy: 0.9854\n",
            "Epoch 72/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0391 - accuracy: 0.9880\n",
            "Epoch 00072: saving model to /content/model/72-0.0533.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0393 - accuracy: 0.9881 - val_loss: 0.0533 - val_accuracy: 0.9885\n",
            "Epoch 73/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9900\n",
            "Epoch 00073: saving model to /content/model/73-0.0556.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0351 - accuracy: 0.9900 - val_loss: 0.0556 - val_accuracy: 0.9854\n",
            "Epoch 74/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9888\n",
            "Epoch 00074: saving model to /content/model/74-0.0780.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0389 - accuracy: 0.9888 - val_loss: 0.0780 - val_accuracy: 0.9831\n",
            "Epoch 75/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0415 - accuracy: 0.9874\n",
            "Epoch 00075: saving model to /content/model/75-0.0554.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0404 - accuracy: 0.9879 - val_loss: 0.0554 - val_accuracy: 0.9869\n",
            "Epoch 76/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0234 - accuracy: 0.9900\n",
            "Epoch 00076: saving model to /content/model/76-0.0561.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0394 - accuracy: 0.9886 - val_loss: 0.0561 - val_accuracy: 0.9854\n",
            "Epoch 77/200\n",
            "19/26 [====================>.........] - ETA: 0s - loss: 0.0367 - accuracy: 0.9895\n",
            "Epoch 00077: saving model to /content/model/77-0.0607.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0406 - accuracy: 0.9888 - val_loss: 0.0607 - val_accuracy: 0.9885\n",
            "Epoch 78/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0321 - accuracy: 0.9926\n",
            "Epoch 00078: saving model to /content/model/78-0.0571.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0357 - accuracy: 0.9911 - val_loss: 0.0571 - val_accuracy: 0.9846\n",
            "Epoch 79/200\n",
            "15/26 [================>.............] - ETA: 0s - loss: 0.0437 - accuracy: 0.9877\n",
            "Epoch 00079: saving model to /content/model/79-0.0662.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0385 - accuracy: 0.9890 - val_loss: 0.0662 - val_accuracy: 0.9846\n",
            "Epoch 80/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9894\n",
            "Epoch 00080: saving model to /content/model/80-0.0549.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0388 - accuracy: 0.9894 - val_loss: 0.0549 - val_accuracy: 0.9862\n",
            "Epoch 81/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0143 - accuracy: 1.0000\n",
            "Epoch 00081: saving model to /content/model/81-0.0599.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0361 - accuracy: 0.9902 - val_loss: 0.0599 - val_accuracy: 0.9885\n",
            "Epoch 82/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0372 - accuracy: 0.9892\n",
            "Epoch 00082: saving model to /content/model/82-0.0597.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0368 - accuracy: 0.9892 - val_loss: 0.0597 - val_accuracy: 0.9877\n",
            "Epoch 83/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0364 - accuracy: 0.9904\n",
            "Epoch 00083: saving model to /content/model/83-0.0553.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0356 - accuracy: 0.9904 - val_loss: 0.0553 - val_accuracy: 0.9877\n",
            "Epoch 84/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0144 - accuracy: 0.9950\n",
            "Epoch 00084: saving model to /content/model/84-0.0551.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0357 - accuracy: 0.9900 - val_loss: 0.0551 - val_accuracy: 0.9869\n",
            "Epoch 85/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0330 - accuracy: 0.9914\n",
            "Epoch 00085: saving model to /content/model/85-0.0568.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0340 - accuracy: 0.9911 - val_loss: 0.0568 - val_accuracy: 0.9892\n",
            "Epoch 86/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0344 - accuracy: 0.9904\n",
            "Epoch 00086: saving model to /content/model/86-0.0558.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0347 - accuracy: 0.9906 - val_loss: 0.0558 - val_accuracy: 0.9862\n",
            "Epoch 87/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0172 - accuracy: 0.9950\n",
            "Epoch 00087: saving model to /content/model/87-0.0572.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0345 - accuracy: 0.9913 - val_loss: 0.0572 - val_accuracy: 0.9885\n",
            "Epoch 88/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 00088: saving model to /content/model/88-0.0598.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0339 - accuracy: 0.9906 - val_loss: 0.0598 - val_accuracy: 0.9900\n",
            "Epoch 89/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0118 - accuracy: 1.0000\n",
            "Epoch 00089: saving model to /content/model/89-0.0557.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0352 - accuracy: 0.9898 - val_loss: 0.0557 - val_accuracy: 0.9854\n",
            "Epoch 90/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0336 - accuracy: 0.9904\n",
            "Epoch 00090: saving model to /content/model/90-0.0550.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0357 - accuracy: 0.9902 - val_loss: 0.0550 - val_accuracy: 0.9869\n",
            "Epoch 91/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0388 - accuracy: 0.9879\n",
            "Epoch 00091: saving model to /content/model/91-0.0597.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0386 - accuracy: 0.9883 - val_loss: 0.0597 - val_accuracy: 0.9892\n",
            "Epoch 92/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0359 - accuracy: 0.9898\n",
            "Epoch 00092: saving model to /content/model/92-0.0561.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0367 - accuracy: 0.9890 - val_loss: 0.0561 - val_accuracy: 0.9892\n",
            "Epoch 93/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0316 - accuracy: 0.9914\n",
            "Epoch 00093: saving model to /content/model/93-0.0615.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0344 - accuracy: 0.9904 - val_loss: 0.0615 - val_accuracy: 0.9885\n",
            "Epoch 94/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0356 - accuracy: 0.9902\n",
            "Epoch 00094: saving model to /content/model/94-0.0553.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0361 - accuracy: 0.9900 - val_loss: 0.0553 - val_accuracy: 0.9877\n",
            "Epoch 95/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9892\n",
            "Epoch 00095: saving model to /content/model/95-0.0566.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0375 - accuracy: 0.9892 - val_loss: 0.0566 - val_accuracy: 0.9877\n",
            "Epoch 96/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0397 - accuracy: 0.9883\n",
            "Epoch 00096: saving model to /content/model/96-0.0571.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0390 - accuracy: 0.9886 - val_loss: 0.0571 - val_accuracy: 0.9846\n",
            "Epoch 97/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0436 - accuracy: 0.9872\n",
            "Epoch 00097: saving model to /content/model/97-0.0566.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0457 - accuracy: 0.9861 - val_loss: 0.0566 - val_accuracy: 0.9869\n",
            "Epoch 98/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0541 - accuracy: 0.9850\n",
            "Epoch 00098: saving model to /content/model/98-0.0546.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0410 - accuracy: 0.9885 - val_loss: 0.0546 - val_accuracy: 0.9869\n",
            "Epoch 99/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0370 - accuracy: 0.9896\n",
            "Epoch 00099: saving model to /content/model/99-0.0577.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0371 - accuracy: 0.9898 - val_loss: 0.0577 - val_accuracy: 0.9862\n",
            "Epoch 100/200\n",
            "19/26 [====================>.........] - ETA: 0s - loss: 0.0387 - accuracy: 0.9905\n",
            "Epoch 00100: saving model to /content/model/100-0.0763.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0384 - accuracy: 0.9898 - val_loss: 0.0763 - val_accuracy: 0.9831\n",
            "Epoch 101/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0364 - accuracy: 0.9900\n",
            "Epoch 00101: saving model to /content/model/101-0.0559.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0381 - accuracy: 0.9888 - val_loss: 0.0559 - val_accuracy: 0.9885\n",
            "Epoch 102/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0435 - accuracy: 0.9850\n",
            "Epoch 00102: saving model to /content/model/102-0.0549.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0457 - accuracy: 0.9877 - val_loss: 0.0549 - val_accuracy: 0.9846\n",
            "Epoch 103/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0370 - accuracy: 0.9896\n",
            "Epoch 00103: saving model to /content/model/103-0.0652.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0365 - accuracy: 0.9896 - val_loss: 0.0652 - val_accuracy: 0.9854\n",
            "Epoch 104/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0269 - accuracy: 0.9950\n",
            "Epoch 00104: saving model to /content/model/104-0.0590.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0351 - accuracy: 0.9898 - val_loss: 0.0590 - val_accuracy: 0.9885\n",
            "Epoch 105/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9904\n",
            "Epoch 00105: saving model to /content/model/105-0.0562.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0333 - accuracy: 0.9904 - val_loss: 0.0562 - val_accuracy: 0.9877\n",
            "Epoch 106/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0363 - accuracy: 0.9902\n",
            "Epoch 00106: saving model to /content/model/106-0.0682.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0383 - accuracy: 0.9892 - val_loss: 0.0682 - val_accuracy: 0.9838\n",
            "Epoch 107/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9896\n",
            "Epoch 00107: saving model to /content/model/107-0.0600.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0364 - accuracy: 0.9896 - val_loss: 0.0600 - val_accuracy: 0.9877\n",
            "Epoch 108/200\n",
            "18/26 [===================>..........] - ETA: 0s - loss: 0.0419 - accuracy: 0.9892\n",
            "Epoch 00108: saving model to /content/model/108-0.0587.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0372 - accuracy: 0.9896 - val_loss: 0.0587 - val_accuracy: 0.9892\n",
            "Epoch 109/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0236 - accuracy: 0.9900\n",
            "Epoch 00109: saving model to /content/model/109-0.0549.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0357 - accuracy: 0.9902 - val_loss: 0.0549 - val_accuracy: 0.9862\n",
            "Epoch 110/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0342 - accuracy: 0.9908\n",
            "Epoch 00110: saving model to /content/model/110-0.0572.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0333 - accuracy: 0.9911 - val_loss: 0.0572 - val_accuracy: 0.9885\n",
            "Epoch 111/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0343 - accuracy: 0.9909\n",
            "Epoch 00111: saving model to /content/model/111-0.0589.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0343 - accuracy: 0.9908 - val_loss: 0.0589 - val_accuracy: 0.9885\n",
            "Epoch 112/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0400 - accuracy: 0.9900\n",
            "Epoch 00112: saving model to /content/model/112-0.0594.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0365 - accuracy: 0.9898 - val_loss: 0.0594 - val_accuracy: 0.9885\n",
            "Epoch 113/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9896\n",
            "Epoch 00113: saving model to /content/model/113-0.0594.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0337 - accuracy: 0.9896 - val_loss: 0.0594 - val_accuracy: 0.9885\n",
            "Epoch 114/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0375 - accuracy: 0.9885\n",
            "Epoch 00114: saving model to /content/model/114-0.0581.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0386 - accuracy: 0.9885 - val_loss: 0.0581 - val_accuracy: 0.9877\n",
            "Epoch 115/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0166 - accuracy: 0.9950\n",
            "Epoch 00115: saving model to /content/model/115-0.0624.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0337 - accuracy: 0.9911 - val_loss: 0.0624 - val_accuracy: 0.9831\n",
            "Epoch 116/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0301 - accuracy: 0.9907\n",
            "Epoch 00116: saving model to /content/model/116-0.0677.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0332 - accuracy: 0.9906 - val_loss: 0.0677 - val_accuracy: 0.9854\n",
            "Epoch 117/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9883\n",
            "Epoch 00117: saving model to /content/model/117-0.0561.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0382 - accuracy: 0.9883 - val_loss: 0.0561 - val_accuracy: 0.9869\n",
            "Epoch 118/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0370 - accuracy: 0.9884\n",
            "Epoch 00118: saving model to /content/model/118-0.0602.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0380 - accuracy: 0.9883 - val_loss: 0.0602 - val_accuracy: 0.9885\n",
            "Epoch 119/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0230 - accuracy: 0.9950\n",
            "Epoch 00119: saving model to /content/model/119-0.0558.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0426 - accuracy: 0.9877 - val_loss: 0.0558 - val_accuracy: 0.9862\n",
            "Epoch 120/200\n",
            "19/26 [====================>.........] - ETA: 0s - loss: 0.0360 - accuracy: 0.9887\n",
            "Epoch 00120: saving model to /content/model/120-0.0667.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0365 - accuracy: 0.9883 - val_loss: 0.0667 - val_accuracy: 0.9854\n",
            "Epoch 121/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0311 - accuracy: 0.9850\n",
            "Epoch 00121: saving model to /content/model/121-0.0570.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0352 - accuracy: 0.9904 - val_loss: 0.0570 - val_accuracy: 0.9846\n",
            "Epoch 122/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0427 - accuracy: 0.9870\n",
            "Epoch 00122: saving model to /content/model/122-0.0574.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0438 - accuracy: 0.9869 - val_loss: 0.0574 - val_accuracy: 0.9877\n",
            "Epoch 123/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0344 - accuracy: 0.9921\n",
            "Epoch 00123: saving model to /content/model/123-0.0568.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0370 - accuracy: 0.9906 - val_loss: 0.0568 - val_accuracy: 0.9885\n",
            "Epoch 124/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0364 - accuracy: 0.9902\n",
            "Epoch 00124: saving model to /content/model/124-0.0574.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0364 - accuracy: 0.9900 - val_loss: 0.0574 - val_accuracy: 0.9862\n",
            "Epoch 125/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0392 - accuracy: 0.9878\n",
            "Epoch 00125: saving model to /content/model/125-0.1075.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0391 - accuracy: 0.9879 - val_loss: 0.1075 - val_accuracy: 0.9738\n",
            "Epoch 126/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0402 - accuracy: 0.9889\n",
            "Epoch 00126: saving model to /content/model/126-0.0642.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0408 - accuracy: 0.9886 - val_loss: 0.0642 - val_accuracy: 0.9869\n",
            "Epoch 127/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0352 - accuracy: 0.9898\n",
            "Epoch 00127: saving model to /content/model/127-0.0610.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0361 - accuracy: 0.9894 - val_loss: 0.0610 - val_accuracy: 0.9885\n",
            "Epoch 128/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0402 - accuracy: 0.9800\n",
            "Epoch 00128: saving model to /content/model/128-0.0582.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0369 - accuracy: 0.9896 - val_loss: 0.0582 - val_accuracy: 0.9885\n",
            "Epoch 129/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0338 - accuracy: 0.9887\n",
            "Epoch 00129: saving model to /content/model/129-0.0644.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0363 - accuracy: 0.9886 - val_loss: 0.0644 - val_accuracy: 0.9877\n",
            "Epoch 130/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0409 - accuracy: 0.9880\n",
            "Epoch 00130: saving model to /content/model/130-0.0561.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0416 - accuracy: 0.9877 - val_loss: 0.0561 - val_accuracy: 0.9869\n",
            "Epoch 131/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0347 - accuracy: 0.9905\n",
            "Epoch 00131: saving model to /content/model/131-0.0574.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0344 - accuracy: 0.9898 - val_loss: 0.0574 - val_accuracy: 0.9854\n",
            "Epoch 132/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0483 - accuracy: 0.9841\n",
            "Epoch 00132: saving model to /content/model/132-0.0692.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0466 - accuracy: 0.9848 - val_loss: 0.0692 - val_accuracy: 0.9854\n",
            "Epoch 133/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0256 - accuracy: 0.9950\n",
            "Epoch 00133: saving model to /content/model/133-0.0558.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0349 - accuracy: 0.9908 - val_loss: 0.0558 - val_accuracy: 0.9877\n",
            "Epoch 134/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0337 - accuracy: 0.9921\n",
            "Epoch 00134: saving model to /content/model/134-0.0597.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0347 - accuracy: 0.9917 - val_loss: 0.0597 - val_accuracy: 0.9846\n",
            "Epoch 135/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0369 - accuracy: 0.9895\n",
            "Epoch 00135: saving model to /content/model/135-0.0605.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0348 - accuracy: 0.9900 - val_loss: 0.0605 - val_accuracy: 0.9831\n",
            "Epoch 136/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9894\n",
            "Epoch 00136: saving model to /content/model/136-0.0571.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0376 - accuracy: 0.9894 - val_loss: 0.0571 - val_accuracy: 0.9862\n",
            "Epoch 137/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0329 - accuracy: 0.9902\n",
            "Epoch 00137: saving model to /content/model/137-0.0605.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0330 - accuracy: 0.9900 - val_loss: 0.0605 - val_accuracy: 0.9877\n",
            "Epoch 138/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9908\n",
            "Epoch 00138: saving model to /content/model/138-0.0628.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0341 - accuracy: 0.9908 - val_loss: 0.0628 - val_accuracy: 0.9877\n",
            "Epoch 139/200\n",
            "17/26 [==================>...........] - ETA: 0s - loss: 0.0357 - accuracy: 0.9894\n",
            "Epoch 00139: saving model to /content/model/139-0.0841.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0370 - accuracy: 0.9890 - val_loss: 0.0841 - val_accuracy: 0.9769\n",
            "Epoch 140/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0368 - accuracy: 0.9893\n",
            "Epoch 00140: saving model to /content/model/140-0.0567.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0374 - accuracy: 0.9890 - val_loss: 0.0567 - val_accuracy: 0.9854\n",
            "Epoch 141/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9900\n",
            "Epoch 00141: saving model to /content/model/141-0.0709.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0358 - accuracy: 0.9900 - val_loss: 0.0709 - val_accuracy: 0.9846\n",
            "Epoch 142/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0450 - accuracy: 0.9846\n",
            "Epoch 00142: saving model to /content/model/142-0.0669.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0437 - accuracy: 0.9846 - val_loss: 0.0669 - val_accuracy: 0.9815\n",
            "Epoch 143/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0417 - accuracy: 0.9871\n",
            "Epoch 00143: saving model to /content/model/143-0.0571.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0431 - accuracy: 0.9867 - val_loss: 0.0571 - val_accuracy: 0.9838\n",
            "Epoch 144/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0364 - accuracy: 0.9896\n",
            "Epoch 00144: saving model to /content/model/144-0.0600.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0374 - accuracy: 0.9894 - val_loss: 0.0600 - val_accuracy: 0.9885\n",
            "Epoch 145/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0371 - accuracy: 0.9893\n",
            "Epoch 00145: saving model to /content/model/145-0.0563.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0368 - accuracy: 0.9896 - val_loss: 0.0563 - val_accuracy: 0.9869\n",
            "Epoch 146/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0355 - accuracy: 0.9889\n",
            "Epoch 00146: saving model to /content/model/146-0.0657.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0353 - accuracy: 0.9892 - val_loss: 0.0657 - val_accuracy: 0.9846\n",
            "Epoch 147/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0392 - accuracy: 0.9898\n",
            "Epoch 00147: saving model to /content/model/147-0.0585.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0365 - accuracy: 0.9908 - val_loss: 0.0585 - val_accuracy: 0.9877\n",
            "Epoch 148/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0346 - accuracy: 0.9908\n",
            "Epoch 00148: saving model to /content/model/148-0.0567.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0340 - accuracy: 0.9911 - val_loss: 0.0567 - val_accuracy: 0.9877\n",
            "Epoch 149/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0312 - accuracy: 0.9915\n",
            "Epoch 00149: saving model to /content/model/149-0.0565.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0316 - accuracy: 0.9913 - val_loss: 0.0565 - val_accuracy: 0.9877\n",
            "Epoch 150/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0364 - accuracy: 0.9880\n",
            "Epoch 00150: saving model to /content/model/150-0.0633.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0379 - accuracy: 0.9879 - val_loss: 0.0633 - val_accuracy: 0.9885\n",
            "Epoch 151/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0439 - accuracy: 0.9850\n",
            "Epoch 00151: saving model to /content/model/151-0.0632.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0352 - accuracy: 0.9906 - val_loss: 0.0632 - val_accuracy: 0.9854\n",
            "Epoch 152/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9906\n",
            "Epoch 00152: saving model to /content/model/152-0.0810.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0340 - accuracy: 0.9906 - val_loss: 0.0810 - val_accuracy: 0.9831\n",
            "Epoch 153/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0360 - accuracy: 0.9887\n",
            "Epoch 00153: saving model to /content/model/153-0.0693.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0377 - accuracy: 0.9886 - val_loss: 0.0693 - val_accuracy: 0.9854\n",
            "Epoch 154/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9898\n",
            "Epoch 00154: saving model to /content/model/154-0.0680.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0359 - accuracy: 0.9898 - val_loss: 0.0680 - val_accuracy: 0.9846\n",
            "Epoch 155/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0365 - accuracy: 0.9886\n",
            "Epoch 00155: saving model to /content/model/155-0.0635.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0377 - accuracy: 0.9886 - val_loss: 0.0635 - val_accuracy: 0.9838\n",
            "Epoch 156/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0335 - accuracy: 0.9902\n",
            "Epoch 00156: saving model to /content/model/156-0.0580.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0337 - accuracy: 0.9902 - val_loss: 0.0580 - val_accuracy: 0.9862\n",
            "Epoch 157/200\n",
            "20/26 [======================>.......] - ETA: 0s - loss: 0.0338 - accuracy: 0.9905\n",
            "Epoch 00157: saving model to /content/model/157-0.0624.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0337 - accuracy: 0.9902 - val_loss: 0.0624 - val_accuracy: 0.9877\n",
            "Epoch 158/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0347 - accuracy: 0.9914\n",
            "Epoch 00158: saving model to /content/model/158-0.0598.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0331 - accuracy: 0.9919 - val_loss: 0.0598 - val_accuracy: 0.9877\n",
            "Epoch 159/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0329 - accuracy: 0.9916\n",
            "Epoch 00159: saving model to /content/model/159-0.0647.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0329 - accuracy: 0.9917 - val_loss: 0.0647 - val_accuracy: 0.9862\n",
            "Epoch 160/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0392 - accuracy: 0.9880\n",
            "Epoch 00160: saving model to /content/model/160-0.0562.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0394 - accuracy: 0.9875 - val_loss: 0.0562 - val_accuracy: 0.9854\n",
            "Epoch 161/200\n",
            "17/26 [==================>...........] - ETA: 0s - loss: 0.0308 - accuracy: 0.9918\n",
            "Epoch 00161: saving model to /content/model/161-0.0584.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0322 - accuracy: 0.9915 - val_loss: 0.0584 - val_accuracy: 0.9885\n",
            "Epoch 162/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0318 - accuracy: 0.9906\n",
            "Epoch 00162: saving model to /content/model/162-0.0588.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0319 - accuracy: 0.9908 - val_loss: 0.0588 - val_accuracy: 0.9900\n",
            "Epoch 163/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0289 - accuracy: 0.9925\n",
            "Epoch 00163: saving model to /content/model/163-0.0599.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0324 - accuracy: 0.9910 - val_loss: 0.0599 - val_accuracy: 0.9877\n",
            "Epoch 164/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9898\n",
            "Epoch 00164: saving model to /content/model/164-0.0602.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0321 - accuracy: 0.9898 - val_loss: 0.0602 - val_accuracy: 0.9854\n",
            "Epoch 165/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9902\n",
            "Epoch 00165: saving model to /content/model/165-0.0586.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0323 - accuracy: 0.9902 - val_loss: 0.0586 - val_accuracy: 0.9862\n",
            "Epoch 166/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0361 - accuracy: 0.9902\n",
            "Epoch 00166: saving model to /content/model/166-0.0579.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0352 - accuracy: 0.9900 - val_loss: 0.0579 - val_accuracy: 0.9892\n",
            "Epoch 167/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0333 - accuracy: 0.9902\n",
            "Epoch 00167: saving model to /content/model/167-0.0673.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0332 - accuracy: 0.9906 - val_loss: 0.0673 - val_accuracy: 0.9862\n",
            "Epoch 168/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0318 - accuracy: 0.9907\n",
            "Epoch 00168: saving model to /content/model/168-0.0577.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0335 - accuracy: 0.9908 - val_loss: 0.0577 - val_accuracy: 0.9862\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9906\n",
            "Epoch 00169: saving model to /content/model/169-0.0584.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0344 - accuracy: 0.9906 - val_loss: 0.0584 - val_accuracy: 0.9862\n",
            "Epoch 170/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9915\n",
            "Epoch 00170: saving model to /content/model/170-0.0642.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0344 - accuracy: 0.9915 - val_loss: 0.0642 - val_accuracy: 0.9885\n",
            "Epoch 171/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9892\n",
            "Epoch 00171: saving model to /content/model/171-0.0865.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0365 - accuracy: 0.9892 - val_loss: 0.0865 - val_accuracy: 0.9823\n",
            "Epoch 172/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0490 - accuracy: 0.9842\n",
            "Epoch 00172: saving model to /content/model/172-0.0612.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0477 - accuracy: 0.9846 - val_loss: 0.0612 - val_accuracy: 0.9838\n",
            "Epoch 173/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0408 - accuracy: 0.9877\n",
            "Epoch 00173: saving model to /content/model/173-0.0582.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0396 - accuracy: 0.9879 - val_loss: 0.0582 - val_accuracy: 0.9877\n",
            "Epoch 174/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0341 - accuracy: 0.9912\n",
            "Epoch 00174: saving model to /content/model/174-0.0560.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0333 - accuracy: 0.9913 - val_loss: 0.0560 - val_accuracy: 0.9877\n",
            "Epoch 175/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0332 - accuracy: 0.9909\n",
            "Epoch 00175: saving model to /content/model/175-0.0592.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0326 - accuracy: 0.9913 - val_loss: 0.0592 - val_accuracy: 0.9854\n",
            "Epoch 176/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0360 - accuracy: 0.9886\n",
            "Epoch 00176: saving model to /content/model/176-0.0730.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0350 - accuracy: 0.9894 - val_loss: 0.0730 - val_accuracy: 0.9869\n",
            "Epoch 177/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0370 - accuracy: 0.9887\n",
            "Epoch 00177: saving model to /content/model/177-0.0629.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0385 - accuracy: 0.9886 - val_loss: 0.0629 - val_accuracy: 0.9877\n",
            "Epoch 178/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0342 - accuracy: 0.9908\n",
            "Epoch 00178: saving model to /content/model/178-0.0581.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0340 - accuracy: 0.9910 - val_loss: 0.0581 - val_accuracy: 0.9900\n",
            "Epoch 179/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0298 - accuracy: 0.9924\n",
            "Epoch 00179: saving model to /content/model/179-0.0572.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0308 - accuracy: 0.9919 - val_loss: 0.0572 - val_accuracy: 0.9900\n",
            "Epoch 180/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0333 - accuracy: 0.9918\n",
            "Epoch 00180: saving model to /content/model/180-0.0577.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0321 - accuracy: 0.9923 - val_loss: 0.0577 - val_accuracy: 0.9892\n",
            "Epoch 181/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9915\n",
            "Epoch 00181: saving model to /content/model/181-0.0606.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0323 - accuracy: 0.9915 - val_loss: 0.0606 - val_accuracy: 0.9877\n",
            "Epoch 182/200\n",
            "18/26 [===================>..........] - ETA: 0s - loss: 0.0320 - accuracy: 0.9908\n",
            "Epoch 00182: saving model to /content/model/182-0.0594.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0344 - accuracy: 0.9898 - val_loss: 0.0594 - val_accuracy: 0.9877\n",
            "Epoch 183/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0170 - accuracy: 1.0000\n",
            "Epoch 00183: saving model to /content/model/183-0.0605.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0356 - accuracy: 0.9904 - val_loss: 0.0605 - val_accuracy: 0.9846\n",
            "Epoch 184/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9886\n",
            "Epoch 00184: saving model to /content/model/184-0.0648.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0399 - accuracy: 0.9886 - val_loss: 0.0648 - val_accuracy: 0.9885\n",
            "Epoch 185/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0400 - accuracy: 0.9872\n",
            "Epoch 00185: saving model to /content/model/185-0.0859.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0408 - accuracy: 0.9873 - val_loss: 0.0859 - val_accuracy: 0.9815\n",
            "Epoch 186/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0360 - accuracy: 0.9900\n",
            "Epoch 00186: saving model to /content/model/186-0.0561.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0349 - accuracy: 0.9906 - val_loss: 0.0561 - val_accuracy: 0.9877\n",
            "Epoch 187/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0336 - accuracy: 0.9914\n",
            "Epoch 00187: saving model to /content/model/187-0.0601.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0315 - accuracy: 0.9913 - val_loss: 0.0601 - val_accuracy: 0.9854\n",
            "Epoch 188/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0357 - accuracy: 0.9902\n",
            "Epoch 00188: saving model to /content/model/188-0.0591.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0357 - accuracy: 0.9902 - val_loss: 0.0591 - val_accuracy: 0.9869\n",
            "Epoch 189/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0320 - accuracy: 0.9919\n",
            "Epoch 00189: saving model to /content/model/189-0.0590.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0327 - accuracy: 0.9915 - val_loss: 0.0590 - val_accuracy: 0.9877\n",
            "Epoch 190/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0312 - accuracy: 0.9905\n",
            "Epoch 00190: saving model to /content/model/190-0.0596.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0331 - accuracy: 0.9896 - val_loss: 0.0596 - val_accuracy: 0.9885\n",
            "Epoch 191/200\n",
            "18/26 [===================>..........] - ETA: 0s - loss: 0.0346 - accuracy: 0.9892\n",
            "Epoch 00191: saving model to /content/model/191-0.0598.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0346 - accuracy: 0.9900 - val_loss: 0.0598 - val_accuracy: 0.9854\n",
            "Epoch 192/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9894\n",
            "Epoch 00192: saving model to /content/model/192-0.0550.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0370 - accuracy: 0.9894 - val_loss: 0.0550 - val_accuracy: 0.9869\n",
            "Epoch 193/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0397 - accuracy: 0.9881\n",
            "Epoch 00193: saving model to /content/model/193-0.0635.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0399 - accuracy: 0.9877 - val_loss: 0.0635 - val_accuracy: 0.9892\n",
            "Epoch 194/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0335 - accuracy: 0.9912\n",
            "Epoch 00194: saving model to /content/model/194-0.0573.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0332 - accuracy: 0.9908 - val_loss: 0.0573 - val_accuracy: 0.9869\n",
            "Epoch 195/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0340 - accuracy: 0.9912\n",
            "Epoch 00195: saving model to /content/model/195-0.0591.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0351 - accuracy: 0.9910 - val_loss: 0.0591 - val_accuracy: 0.9846\n",
            "Epoch 196/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0390 - accuracy: 0.9883\n",
            "Epoch 00196: saving model to /content/model/196-0.0574.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0373 - accuracy: 0.9886 - val_loss: 0.0574 - val_accuracy: 0.9885\n",
            "Epoch 197/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0424 - accuracy: 0.9869\n",
            "Epoch 00197: saving model to /content/model/197-0.0686.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0396 - accuracy: 0.9881 - val_loss: 0.0686 - val_accuracy: 0.9869\n",
            "Epoch 198/200\n",
            "19/26 [====================>.........] - ETA: 0s - loss: 0.0319 - accuracy: 0.9908\n",
            "Epoch 00198: saving model to /content/model/198-0.0570.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0337 - accuracy: 0.9902 - val_loss: 0.0570 - val_accuracy: 0.9862\n",
            "Epoch 199/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0383 - accuracy: 0.9890\n",
            "Epoch 00199: saving model to /content/model/199-0.0571.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0375 - accuracy: 0.9890 - val_loss: 0.0571 - val_accuracy: 0.9877\n",
            "Epoch 200/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0185 - accuracy: 0.9900\n",
            "Epoch 00200: saving model to /content/model/200-0.0570.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0380 - accuracy: 0.9892 - val_loss: 0.0570 - val_accuracy: 0.9877\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9e3319f290>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "model.fit(X,Y, validation_split=0.2, epochs=200, batch_size=200, callbacks=[checkpointer]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5_SVvF2xtYQ",
        "outputId": "6442bb85-28b5-4e25-bbee-25a776d767b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0139 - accuracy: 0.9950\n",
            "Epoch 00001: val_loss improved from inf to 0.06344, saving model to /content/model/01-0.0634.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0335 - accuracy: 0.9904 - val_loss: 0.0634 - val_accuracy: 0.9854\n",
            "Epoch 2/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0362 - accuracy: 0.9950\n",
            "Epoch 00002: val_loss improved from 0.06344 to 0.06082, saving model to /content/model/02-0.0608.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.9892 - val_loss: 0.0608 - val_accuracy: 0.9885\n",
            "Epoch 3/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0423 - accuracy: 0.9950\n",
            "Epoch 00003: val_loss improved from 0.06082 to 0.06077, saving model to /content/model/03-0.0608.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0378 - accuracy: 0.9898 - val_loss: 0.0608 - val_accuracy: 0.9869\n",
            "Epoch 4/200\n",
            "20/26 [======================>.......] - ETA: 0s - loss: 0.0300 - accuracy: 0.9910\n",
            "Epoch 00004: val_loss improved from 0.06077 to 0.06044, saving model to /content/model/04-0.0604.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0327 - accuracy: 0.9908 - val_loss: 0.0604 - val_accuracy: 0.9900\n",
            "Epoch 5/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0171 - accuracy: 0.9950\n",
            "Epoch 00005: val_loss did not improve from 0.06044\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0316 - accuracy: 0.9919 - val_loss: 0.0610 - val_accuracy: 0.9885\n",
            "Epoch 6/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0152 - accuracy: 0.9950\n",
            "Epoch 00006: val_loss did not improve from 0.06044\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0341 - accuracy: 0.9911 - val_loss: 0.0678 - val_accuracy: 0.9838\n",
            "Epoch 7/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0371 - accuracy: 0.9900\n",
            "Epoch 00007: val_loss did not improve from 0.06044\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0403 - accuracy: 0.9877 - val_loss: 0.0711 - val_accuracy: 0.9862\n",
            "Epoch 8/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0734 - accuracy: 0.9850\n",
            "Epoch 00008: val_loss improved from 0.06044 to 0.06043, saving model to /content/model/08-0.0604.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0376 - accuracy: 0.9892 - val_loss: 0.0604 - val_accuracy: 0.9862\n",
            "Epoch 9/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0457 - accuracy: 0.9900\n",
            "Epoch 00009: val_loss did not improve from 0.06043\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0322 - accuracy: 0.9902 - val_loss: 0.0655 - val_accuracy: 0.9862\n",
            "Epoch 10/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9911\n",
            "Epoch 00010: val_loss improved from 0.06043 to 0.05974, saving model to /content/model/10-0.0597.hdf5\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0315 - accuracy: 0.9911 - val_loss: 0.0597 - val_accuracy: 0.9877\n",
            "Epoch 11/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0090 - accuracy: 1.0000\n",
            "Epoch 00011: val_loss improved from 0.05974 to 0.05801, saving model to /content/model/11-0.0580.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0332 - accuracy: 0.9921 - val_loss: 0.0580 - val_accuracy: 0.9885\n",
            "Epoch 12/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0313 - accuracy: 0.9904\n",
            "Epoch 00012: val_loss improved from 0.05801 to 0.05731, saving model to /content/model/12-0.0573.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0313 - accuracy: 0.9906 - val_loss: 0.0573 - val_accuracy: 0.9892\n",
            "Epoch 13/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0167 - accuracy: 1.0000\n",
            "Epoch 00013: val_loss improved from 0.05731 to 0.05631, saving model to /content/model/13-0.0563.hdf5\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0314 - accuracy: 0.9911 - val_loss: 0.0563 - val_accuracy: 0.9869\n",
            "Epoch 14/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0554 - accuracy: 0.9950\n",
            "Epoch 00014: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0327 - accuracy: 0.9919 - val_loss: 0.0594 - val_accuracy: 0.9846\n",
            "Epoch 15/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0338 - accuracy: 0.9850\n",
            "Epoch 00015: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0316 - accuracy: 0.9913 - val_loss: 0.0582 - val_accuracy: 0.9869\n",
            "Epoch 16/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0149 - accuracy: 0.9900\n",
            "Epoch 00016: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0320 - accuracy: 0.9925 - val_loss: 0.0619 - val_accuracy: 0.9885\n",
            "Epoch 17/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9910\n",
            "Epoch 00017: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0308 - accuracy: 0.9910 - val_loss: 0.0572 - val_accuracy: 0.9862\n",
            "Epoch 18/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 00018: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0318 - accuracy: 0.9919 - val_loss: 0.0584 - val_accuracy: 0.9869\n",
            "Epoch 19/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0978 - accuracy: 0.9750\n",
            "Epoch 00019: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0326 - accuracy: 0.9902 - val_loss: 0.0616 - val_accuracy: 0.9885\n",
            "Epoch 20/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0322 - accuracy: 0.9900\n",
            "Epoch 00020: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0307 - accuracy: 0.9919 - val_loss: 0.0575 - val_accuracy: 0.9869\n",
            "Epoch 21/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0109 - accuracy: 1.0000\n",
            "Epoch 00021: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0336 - accuracy: 0.9904 - val_loss: 0.0628 - val_accuracy: 0.9877\n",
            "Epoch 22/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0338 - accuracy: 0.9950\n",
            "Epoch 00022: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0426 - accuracy: 0.9877 - val_loss: 0.0593 - val_accuracy: 0.9838\n",
            "Epoch 23/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0734 - accuracy: 0.9800\n",
            "Epoch 00023: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0426 - accuracy: 0.9879 - val_loss: 0.0653 - val_accuracy: 0.9854\n",
            "Epoch 24/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0372 - accuracy: 0.9880\n",
            "Epoch 00024: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0390 - accuracy: 0.9881 - val_loss: 0.0575 - val_accuracy: 0.9862\n",
            "Epoch 25/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0152 - accuracy: 0.9950\n",
            "Epoch 00025: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0307 - accuracy: 0.9925 - val_loss: 0.0588 - val_accuracy: 0.9877\n",
            "Epoch 26/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9908\n",
            "Epoch 00026: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0311 - accuracy: 0.9908 - val_loss: 0.0587 - val_accuracy: 0.9885\n",
            "Epoch 27/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9921\n",
            "Epoch 00027: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0314 - accuracy: 0.9921 - val_loss: 0.0678 - val_accuracy: 0.9854\n",
            "Epoch 28/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0928 - accuracy: 0.9800\n",
            "Epoch 00028: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0324 - accuracy: 0.9910 - val_loss: 0.0606 - val_accuracy: 0.9885\n",
            "Epoch 29/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9913\n",
            "Epoch 00029: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0318 - accuracy: 0.9913 - val_loss: 0.0596 - val_accuracy: 0.9877\n",
            "Epoch 30/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0339 - accuracy: 0.9896\n",
            "Epoch 00030: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0337 - accuracy: 0.9896 - val_loss: 0.0629 - val_accuracy: 0.9885\n",
            "Epoch 31/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0315 - accuracy: 0.9914\n",
            "Epoch 00031: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0325 - accuracy: 0.9911 - val_loss: 0.0690 - val_accuracy: 0.9854\n",
            "Epoch 32/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0257 - accuracy: 0.9900\n",
            "Epoch 00032: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0325 - accuracy: 0.9911 - val_loss: 0.0584 - val_accuracy: 0.9869\n",
            "Epoch 33/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0245 - accuracy: 0.9900\n",
            "Epoch 00033: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0325 - accuracy: 0.9915 - val_loss: 0.0597 - val_accuracy: 0.9862\n",
            "Epoch 34/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0427 - accuracy: 0.9874\n",
            "Epoch 00034: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0429 - accuracy: 0.9881 - val_loss: 0.0706 - val_accuracy: 0.9862\n",
            "Epoch 35/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0353 - accuracy: 0.9894\n",
            "Epoch 00035: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0343 - accuracy: 0.9898 - val_loss: 0.0595 - val_accuracy: 0.9862\n",
            "Epoch 36/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0504 - accuracy: 0.9850\n",
            "Epoch 00036: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0338 - accuracy: 0.9904 - val_loss: 0.0631 - val_accuracy: 0.9869\n",
            "Epoch 37/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0332 - accuracy: 0.9907\n",
            "Epoch 00037: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0332 - accuracy: 0.9913 - val_loss: 0.0601 - val_accuracy: 0.9885\n",
            "Epoch 38/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0120 - accuracy: 1.0000\n",
            "Epoch 00038: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0329 - accuracy: 0.9911 - val_loss: 0.0643 - val_accuracy: 0.9885\n",
            "Epoch 39/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0343 - accuracy: 0.9892\n",
            "Epoch 00039: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0358 - accuracy: 0.9888 - val_loss: 0.0830 - val_accuracy: 0.9800\n",
            "Epoch 40/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.1030 - accuracy: 0.9750\n",
            "Epoch 00040: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0401 - accuracy: 0.9885 - val_loss: 0.0587 - val_accuracy: 0.9877\n",
            "Epoch 41/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0436 - accuracy: 0.9869\n",
            "Epoch 00041: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0421 - accuracy: 0.9871 - val_loss: 0.0645 - val_accuracy: 0.9885\n",
            "Epoch 42/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0284 - accuracy: 0.9850\n",
            "Epoch 00042: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0336 - accuracy: 0.9906 - val_loss: 0.0688 - val_accuracy: 0.9854\n",
            "Epoch 43/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0276 - accuracy: 0.9950\n",
            "Epoch 00043: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0386 - accuracy: 0.9898 - val_loss: 0.0600 - val_accuracy: 0.9869\n",
            "Epoch 44/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0304 - accuracy: 0.9921\n",
            "Epoch 00044: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0321 - accuracy: 0.9913 - val_loss: 0.0606 - val_accuracy: 0.9885\n",
            "Epoch 45/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 00045: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0300 - accuracy: 0.9915 - val_loss: 0.0848 - val_accuracy: 0.9846\n",
            "Epoch 46/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0271 - accuracy: 0.9850\n",
            "Epoch 00046: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0361 - accuracy: 0.9892 - val_loss: 0.0604 - val_accuracy: 0.9892\n",
            "Epoch 47/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0352 - accuracy: 0.9907\n",
            "Epoch 00047: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0343 - accuracy: 0.9908 - val_loss: 0.0613 - val_accuracy: 0.9877\n",
            "Epoch 48/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0322 - accuracy: 0.9914\n",
            "Epoch 00048: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0322 - accuracy: 0.9915 - val_loss: 0.0591 - val_accuracy: 0.9854\n",
            "Epoch 49/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 00049: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0327 - accuracy: 0.9904 - val_loss: 0.0630 - val_accuracy: 0.9885\n",
            "Epoch 50/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9911\n",
            "Epoch 00050: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0324 - accuracy: 0.9911 - val_loss: 0.0620 - val_accuracy: 0.9892\n",
            "Epoch 51/200\n",
            "19/26 [====================>.........] - ETA: 0s - loss: 0.0295 - accuracy: 0.9913\n",
            "Epoch 00051: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0313 - accuracy: 0.9910 - val_loss: 0.0626 - val_accuracy: 0.9877\n",
            "Epoch 52/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0086 - accuracy: 1.0000\n",
            "Epoch 00052: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0298 - accuracy: 0.9921 - val_loss: 0.0609 - val_accuracy: 0.9869\n",
            "Epoch 53/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0408 - accuracy: 0.9800\n",
            "Epoch 00053: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0324 - accuracy: 0.9908 - val_loss: 0.0645 - val_accuracy: 0.9892\n",
            "Epoch 54/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0356 - accuracy: 0.9900\n",
            "Epoch 00054: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0366 - accuracy: 0.9900 - val_loss: 0.0698 - val_accuracy: 0.9831\n",
            "Epoch 55/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0343 - accuracy: 0.9894\n",
            "Epoch 00055: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0340 - accuracy: 0.9896 - val_loss: 0.0724 - val_accuracy: 0.9854\n",
            "Epoch 56/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9908\n",
            "Epoch 00056: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0333 - accuracy: 0.9908 - val_loss: 0.0620 - val_accuracy: 0.9877\n",
            "Epoch 57/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0305 - accuracy: 0.9916\n",
            "Epoch 00057: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0301 - accuracy: 0.9917 - val_loss: 0.0650 - val_accuracy: 0.9862\n",
            "Epoch 58/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0280 - accuracy: 0.9950\n",
            "Epoch 00058: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0309 - accuracy: 0.9913 - val_loss: 0.0604 - val_accuracy: 0.9892\n",
            "Epoch 59/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0330 - accuracy: 0.9914\n",
            "Epoch 00059: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0334 - accuracy: 0.9913 - val_loss: 0.0711 - val_accuracy: 0.9862\n",
            "Epoch 60/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0223 - accuracy: 0.9900\n",
            "Epoch 00060: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0334 - accuracy: 0.9906 - val_loss: 0.0620 - val_accuracy: 0.9885\n",
            "Epoch 61/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0329 - accuracy: 0.9906\n",
            "Epoch 00061: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0312 - accuracy: 0.9911 - val_loss: 0.0595 - val_accuracy: 0.9877\n",
            "Epoch 62/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9911\n",
            "Epoch 00062: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0312 - accuracy: 0.9911 - val_loss: 0.0601 - val_accuracy: 0.9869\n",
            "Epoch 63/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9921\n",
            "Epoch 00063: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0297 - accuracy: 0.9921 - val_loss: 0.0595 - val_accuracy: 0.9869\n",
            "Epoch 64/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0338 - accuracy: 0.9907\n",
            "Epoch 00064: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0364 - accuracy: 0.9902 - val_loss: 0.0648 - val_accuracy: 0.9869\n",
            "Epoch 65/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0338 - accuracy: 0.9907\n",
            "Epoch 00065: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0332 - accuracy: 0.9908 - val_loss: 0.0714 - val_accuracy: 0.9869\n",
            "Epoch 66/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0341 - accuracy: 0.9950\n",
            "Epoch 00066: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0347 - accuracy: 0.9896 - val_loss: 0.0580 - val_accuracy: 0.9862\n",
            "Epoch 67/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0090 - accuracy: 0.9950\n",
            "Epoch 00067: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0325 - accuracy: 0.9906 - val_loss: 0.0589 - val_accuracy: 0.9885\n",
            "Epoch 68/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0323 - accuracy: 0.9850\n",
            "Epoch 00068: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0306 - accuracy: 0.9917 - val_loss: 0.0588 - val_accuracy: 0.9869\n",
            "Epoch 69/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0323 - accuracy: 0.9915\n",
            "Epoch 00069: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0309 - accuracy: 0.9917 - val_loss: 0.0617 - val_accuracy: 0.9838\n",
            "Epoch 70/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0333 - accuracy: 0.9922\n",
            "Epoch 00070: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0326 - accuracy: 0.9923 - val_loss: 0.0588 - val_accuracy: 0.9885\n",
            "Epoch 71/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0438 - accuracy: 0.9850\n",
            "Epoch 00071: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0316 - accuracy: 0.9917 - val_loss: 0.0595 - val_accuracy: 0.9877\n",
            "Epoch 72/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0128 - accuracy: 1.0000\n",
            "Epoch 00072: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0344 - accuracy: 0.9911 - val_loss: 0.0586 - val_accuracy: 0.9892\n",
            "Epoch 73/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0306 - accuracy: 0.9925\n",
            "Epoch 00073: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0298 - accuracy: 0.9925 - val_loss: 0.0604 - val_accuracy: 0.9869\n",
            "Epoch 74/200\n",
            "16/26 [=================>............] - ETA: 0s - loss: 0.0307 - accuracy: 0.9916\n",
            "Epoch 00074: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0321 - accuracy: 0.9913 - val_loss: 0.0778 - val_accuracy: 0.9862\n",
            "Epoch 75/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9904\n",
            "Epoch 00075: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0358 - accuracy: 0.9904 - val_loss: 0.0635 - val_accuracy: 0.9885\n",
            "Epoch 76/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0380 - accuracy: 0.9894\n",
            "Epoch 00076: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0372 - accuracy: 0.9896 - val_loss: 0.0601 - val_accuracy: 0.9877\n",
            "Epoch 77/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0144 - accuracy: 1.0000\n",
            "Epoch 00077: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0390 - accuracy: 0.9886 - val_loss: 0.0628 - val_accuracy: 0.9885\n",
            "Epoch 78/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9917\n",
            "Epoch 00078: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0312 - accuracy: 0.9917 - val_loss: 0.0662 - val_accuracy: 0.9854\n",
            "Epoch 79/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0333 - accuracy: 0.9902\n",
            "Epoch 00079: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0329 - accuracy: 0.9904 - val_loss: 0.0694 - val_accuracy: 0.9862\n",
            "Epoch 80/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0307 - accuracy: 0.9908\n",
            "Epoch 00080: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0324 - accuracy: 0.9904 - val_loss: 0.0603 - val_accuracy: 0.9877\n",
            "Epoch 81/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0313 - accuracy: 0.9920\n",
            "Epoch 00081: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0315 - accuracy: 0.9919 - val_loss: 0.0618 - val_accuracy: 0.9877\n",
            "Epoch 82/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0321 - accuracy: 0.9910\n",
            "Epoch 00082: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0310 - accuracy: 0.9911 - val_loss: 0.0671 - val_accuracy: 0.9877\n",
            "Epoch 83/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9925\n",
            "Epoch 00083: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0310 - accuracy: 0.9925 - val_loss: 0.0602 - val_accuracy: 0.9885\n",
            "Epoch 84/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0091 - accuracy: 0.9950\n",
            "Epoch 00084: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0328 - accuracy: 0.9906 - val_loss: 0.0638 - val_accuracy: 0.9869\n",
            "Epoch 85/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 00085: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0294 - accuracy: 0.9931 - val_loss: 0.0629 - val_accuracy: 0.9869\n",
            "Epoch 86/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0563 - accuracy: 0.9800\n",
            "Epoch 00086: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0292 - accuracy: 0.9929 - val_loss: 0.0618 - val_accuracy: 0.9862\n",
            "Epoch 87/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0303 - accuracy: 0.9918\n",
            "Epoch 00087: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0297 - accuracy: 0.9919 - val_loss: 0.0627 - val_accuracy: 0.9877\n",
            "Epoch 88/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 00088: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0309 - accuracy: 0.9927 - val_loss: 0.0697 - val_accuracy: 0.9892\n",
            "Epoch 89/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9900\n",
            "Epoch 00089: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0335 - accuracy: 0.9900 - val_loss: 0.0614 - val_accuracy: 0.9877\n",
            "Epoch 90/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0301 - accuracy: 0.9926\n",
            "Epoch 00090: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0318 - accuracy: 0.9919 - val_loss: 0.0617 - val_accuracy: 0.9869\n",
            "Epoch 91/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0234 - accuracy: 0.9900\n",
            "Epoch 00091: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0328 - accuracy: 0.9910 - val_loss: 0.0677 - val_accuracy: 0.9877\n",
            "Epoch 92/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0166 - accuracy: 0.9950\n",
            "Epoch 00092: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0327 - accuracy: 0.9906 - val_loss: 0.0604 - val_accuracy: 0.9869\n",
            "Epoch 93/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0282 - accuracy: 0.9928\n",
            "Epoch 00093: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0307 - accuracy: 0.9917 - val_loss: 0.0648 - val_accuracy: 0.9892\n",
            "Epoch 94/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9921\n",
            "Epoch 00094: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0309 - accuracy: 0.9921 - val_loss: 0.0627 - val_accuracy: 0.9862\n",
            "Epoch 95/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.1144 - accuracy: 0.9700\n",
            "Epoch 00095: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0347 - accuracy: 0.9902 - val_loss: 0.0707 - val_accuracy: 0.9892\n",
            "Epoch 96/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0472 - accuracy: 0.9900\n",
            "Epoch 00096: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0382 - accuracy: 0.9879 - val_loss: 0.0653 - val_accuracy: 0.9885\n",
            "Epoch 97/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0403 - accuracy: 0.9876\n",
            "Epoch 00097: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0432 - accuracy: 0.9871 - val_loss: 0.0640 - val_accuracy: 0.9908\n",
            "Epoch 98/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0455 - accuracy: 0.9900\n",
            "Epoch 00098: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0363 - accuracy: 0.9898 - val_loss: 0.0646 - val_accuracy: 0.9892\n",
            "Epoch 99/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0319 - accuracy: 0.9920\n",
            "Epoch 00099: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0312 - accuracy: 0.9921 - val_loss: 0.0618 - val_accuracy: 0.9885\n",
            "Epoch 100/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0315 - accuracy: 0.9924\n",
            "Epoch 00100: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0296 - accuracy: 0.9925 - val_loss: 0.0655 - val_accuracy: 0.9892\n",
            "Epoch 101/200\n",
            "19/26 [====================>.........] - ETA: 0s - loss: 0.0291 - accuracy: 0.9929\n",
            "Epoch 00101: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0309 - accuracy: 0.9913 - val_loss: 0.0666 - val_accuracy: 0.9877\n",
            "Epoch 102/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0448 - accuracy: 0.9850\n",
            "Epoch 00102: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0312 - accuracy: 0.9921 - val_loss: 0.0627 - val_accuracy: 0.9877\n",
            "Epoch 103/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0318 - accuracy: 0.9918\n",
            "Epoch 00103: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0310 - accuracy: 0.9921 - val_loss: 0.0684 - val_accuracy: 0.9900\n",
            "Epoch 104/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0288 - accuracy: 0.9925\n",
            "Epoch 00104: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0291 - accuracy: 0.9927 - val_loss: 0.0654 - val_accuracy: 0.9892\n",
            "Epoch 105/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0264 - accuracy: 0.9936\n",
            "Epoch 00105: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0291 - accuracy: 0.9931 - val_loss: 0.0629 - val_accuracy: 0.9869\n",
            "Epoch 106/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0301 - accuracy: 0.9924\n",
            "Epoch 00106: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0312 - accuracy: 0.9921 - val_loss: 0.0724 - val_accuracy: 0.9892\n",
            "Epoch 107/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0286 - accuracy: 0.9934\n",
            "Epoch 00107: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0301 - accuracy: 0.9929 - val_loss: 0.0674 - val_accuracy: 0.9900\n",
            "Epoch 108/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0289 - accuracy: 0.9924\n",
            "Epoch 00108: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0305 - accuracy: 0.9917 - val_loss: 0.0616 - val_accuracy: 0.9862\n",
            "Epoch 109/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0333 - accuracy: 0.9912\n",
            "Epoch 00109: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0318 - accuracy: 0.9915 - val_loss: 0.0646 - val_accuracy: 0.9892\n",
            "Epoch 110/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0630 - accuracy: 0.9850\n",
            "Epoch 00110: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0330 - accuracy: 0.9910 - val_loss: 0.0678 - val_accuracy: 0.9877\n",
            "Epoch 111/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9910\n",
            "Epoch 00111: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0335 - accuracy: 0.9910 - val_loss: 0.0613 - val_accuracy: 0.9885\n",
            "Epoch 112/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0251 - accuracy: 0.9950\n",
            "Epoch 00112: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0333 - accuracy: 0.9902 - val_loss: 0.0704 - val_accuracy: 0.9892\n",
            "Epoch 113/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0344 - accuracy: 0.9900\n",
            "Epoch 00113: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0340 - accuracy: 0.9904 - val_loss: 0.0693 - val_accuracy: 0.9877\n",
            "Epoch 114/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 00114: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0356 - accuracy: 0.9904 - val_loss: 0.0620 - val_accuracy: 0.9885\n",
            "Epoch 115/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0409 - accuracy: 0.9894\n",
            "Epoch 00115: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0399 - accuracy: 0.9896 - val_loss: 0.0709 - val_accuracy: 0.9823\n",
            "Epoch 116/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9911\n",
            "Epoch 00116: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0332 - accuracy: 0.9911 - val_loss: 0.0635 - val_accuracy: 0.9877\n",
            "Epoch 117/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0343 - accuracy: 0.9906\n",
            "Epoch 00117: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0331 - accuracy: 0.9910 - val_loss: 0.0661 - val_accuracy: 0.9885\n",
            "Epoch 118/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0339 - accuracy: 0.9900\n",
            "Epoch 00118: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0322 - accuracy: 0.9913 - val_loss: 0.0638 - val_accuracy: 0.9892\n",
            "Epoch 119/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0221 - accuracy: 0.9950\n",
            "Epoch 00119: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0354 - accuracy: 0.9890 - val_loss: 0.0618 - val_accuracy: 0.9877\n",
            "Epoch 120/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0417 - accuracy: 0.9900\n",
            "Epoch 00120: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0322 - accuracy: 0.9910 - val_loss: 0.0641 - val_accuracy: 0.9892\n",
            "Epoch 121/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0323 - accuracy: 0.9917\n",
            "Epoch 00121: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0309 - accuracy: 0.9919 - val_loss: 0.0607 - val_accuracy: 0.9854\n",
            "Epoch 122/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0368 - accuracy: 0.9892\n",
            "Epoch 00122: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0381 - accuracy: 0.9888 - val_loss: 0.0623 - val_accuracy: 0.9892\n",
            "Epoch 123/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9913\n",
            "Epoch 00123: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0323 - accuracy: 0.9913 - val_loss: 0.0619 - val_accuracy: 0.9869\n",
            "Epoch 124/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0315 - accuracy: 0.9921\n",
            "Epoch 00124: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0303 - accuracy: 0.9921 - val_loss: 0.0654 - val_accuracy: 0.9846\n",
            "Epoch 125/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0135 - accuracy: 0.9950\n",
            "Epoch 00125: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0368 - accuracy: 0.9904 - val_loss: 0.0900 - val_accuracy: 0.9846\n",
            "Epoch 126/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0637 - accuracy: 0.9850\n",
            "Epoch 00126: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0428 - accuracy: 0.9875 - val_loss: 0.0848 - val_accuracy: 0.9846\n",
            "Epoch 127/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0322 - accuracy: 0.9922\n",
            "Epoch 00127: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0338 - accuracy: 0.9915 - val_loss: 0.0647 - val_accuracy: 0.9877\n",
            "Epoch 128/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0479 - accuracy: 0.9800\n",
            "Epoch 00128: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0353 - accuracy: 0.9906 - val_loss: 0.0616 - val_accuracy: 0.9869\n",
            "Epoch 129/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9910\n",
            "Epoch 00129: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0310 - accuracy: 0.9910 - val_loss: 0.0653 - val_accuracy: 0.9885\n",
            "Epoch 130/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0298 - accuracy: 0.9930\n",
            "Epoch 00130: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0308 - accuracy: 0.9929 - val_loss: 0.0663 - val_accuracy: 0.9885\n",
            "Epoch 131/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9910\n",
            "Epoch 00131: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0341 - accuracy: 0.9910 - val_loss: 0.0625 - val_accuracy: 0.9877\n",
            "Epoch 132/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0192 - accuracy: 0.9850\n",
            "Epoch 00132: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0381 - accuracy: 0.9873 - val_loss: 0.0672 - val_accuracy: 0.9892\n",
            "Epoch 133/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0315 - accuracy: 0.9922\n",
            "Epoch 00133: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0309 - accuracy: 0.9921 - val_loss: 0.0619 - val_accuracy: 0.9900\n",
            "Epoch 134/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9923\n",
            "Epoch 00134: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0305 - accuracy: 0.9923 - val_loss: 0.0648 - val_accuracy: 0.9869\n",
            "Epoch 135/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0315 - accuracy: 0.9914\n",
            "Epoch 00135: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0307 - accuracy: 0.9917 - val_loss: 0.0647 - val_accuracy: 0.9862\n",
            "Epoch 136/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0535 - accuracy: 0.9800\n",
            "Epoch 00136: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0323 - accuracy: 0.9904 - val_loss: 0.0618 - val_accuracy: 0.9877\n",
            "Epoch 137/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9919\n",
            "Epoch 00137: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0308 - accuracy: 0.9919 - val_loss: 0.0627 - val_accuracy: 0.9869\n",
            "Epoch 138/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0271 - accuracy: 0.9900\n",
            "Epoch 00138: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0299 - accuracy: 0.9927 - val_loss: 0.0624 - val_accuracy: 0.9877\n",
            "Epoch 139/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0082 - accuracy: 1.0000\n",
            "Epoch 00139: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0299 - accuracy: 0.9929 - val_loss: 0.0631 - val_accuracy: 0.9869\n",
            "Epoch 140/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0313 - accuracy: 0.9912\n",
            "Epoch 00140: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0315 - accuracy: 0.9911 - val_loss: 0.0620 - val_accuracy: 0.9869\n",
            "Epoch 141/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0229 - accuracy: 0.9936\n",
            "Epoch 00141: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0310 - accuracy: 0.9917 - val_loss: 0.0718 - val_accuracy: 0.9854\n",
            "Epoch 142/200\n",
            "18/26 [===================>..........] - ETA: 0s - loss: 0.0332 - accuracy: 0.9903\n",
            "Epoch 00142: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0332 - accuracy: 0.9910 - val_loss: 0.0615 - val_accuracy: 0.9885\n",
            "Epoch 143/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0348 - accuracy: 0.9907\n",
            "Epoch 00143: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0359 - accuracy: 0.9904 - val_loss: 0.0643 - val_accuracy: 0.9877\n",
            "Epoch 144/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0308 - accuracy: 0.9910\n",
            "Epoch 00144: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0321 - accuracy: 0.9908 - val_loss: 0.0623 - val_accuracy: 0.9877\n",
            "Epoch 145/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0698 - accuracy: 0.9900\n",
            "Epoch 00145: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0304 - accuracy: 0.9921 - val_loss: 0.0633 - val_accuracy: 0.9892\n",
            "Epoch 146/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0355 - accuracy: 0.9898\n",
            "Epoch 00146: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0349 - accuracy: 0.9902 - val_loss: 0.0636 - val_accuracy: 0.9892\n",
            "Epoch 147/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9911\n",
            "Epoch 00147: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0337 - accuracy: 0.9911 - val_loss: 0.0636 - val_accuracy: 0.9885\n",
            "Epoch 148/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0322 - accuracy: 0.9912\n",
            "Epoch 00148: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0314 - accuracy: 0.9915 - val_loss: 0.0607 - val_accuracy: 0.9869\n",
            "Epoch 149/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0567 - accuracy: 0.9800\n",
            "Epoch 00149: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0287 - accuracy: 0.9931 - val_loss: 0.0618 - val_accuracy: 0.9892\n",
            "Epoch 150/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0362 - accuracy: 0.9885\n",
            "Epoch 00150: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.9890 - val_loss: 0.0657 - val_accuracy: 0.9877\n",
            "Epoch 151/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0315 - accuracy: 0.9910\n",
            "Epoch 00151: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0314 - accuracy: 0.9915 - val_loss: 0.0657 - val_accuracy: 0.9892\n",
            "Epoch 152/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0282 - accuracy: 0.9927\n",
            "Epoch 00152: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0294 - accuracy: 0.9925 - val_loss: 0.0731 - val_accuracy: 0.9869\n",
            "Epoch 153/200\n",
            "19/26 [====================>.........] - ETA: 0s - loss: 0.0306 - accuracy: 0.9911\n",
            "Epoch 00153: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0325 - accuracy: 0.9910 - val_loss: 0.0665 - val_accuracy: 0.9885\n",
            "Epoch 154/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0329 - accuracy: 0.9920\n",
            "Epoch 00154: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0326 - accuracy: 0.9921 - val_loss: 0.0727 - val_accuracy: 0.9862\n",
            "Epoch 155/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0338 - accuracy: 0.9904\n",
            "Epoch 00155: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0349 - accuracy: 0.9904 - val_loss: 0.0692 - val_accuracy: 0.9846\n",
            "Epoch 156/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0321 - accuracy: 0.9906\n",
            "Epoch 00156: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0312 - accuracy: 0.9910 - val_loss: 0.0625 - val_accuracy: 0.9877\n",
            "Epoch 157/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 00157: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.9921 - val_loss: 0.0726 - val_accuracy: 0.9885\n",
            "Epoch 158/200\n",
            "20/26 [======================>.......] - ETA: 0s - loss: 0.0285 - accuracy: 0.9930\n",
            "Epoch 00158: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0305 - accuracy: 0.9927 - val_loss: 0.0683 - val_accuracy: 0.9892\n",
            "Epoch 159/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9917\n",
            "Epoch 00159: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0323 - accuracy: 0.9917 - val_loss: 0.0715 - val_accuracy: 0.9869\n",
            "Epoch 160/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0323 - accuracy: 0.9909\n",
            "Epoch 00160: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0339 - accuracy: 0.9906 - val_loss: 0.0624 - val_accuracy: 0.9877\n",
            "Epoch 161/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0301 - accuracy: 0.9920\n",
            "Epoch 00161: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0309 - accuracy: 0.9921 - val_loss: 0.0682 - val_accuracy: 0.9900\n",
            "Epoch 162/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0294 - accuracy: 0.9922\n",
            "Epoch 00162: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0294 - accuracy: 0.9923 - val_loss: 0.0645 - val_accuracy: 0.9892\n",
            "Epoch 163/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0266 - accuracy: 0.9937\n",
            "Epoch 00163: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0293 - accuracy: 0.9933 - val_loss: 0.0641 - val_accuracy: 0.9885\n",
            "Epoch 164/200\n",
            "20/26 [======================>.......] - ETA: 0s - loss: 0.0270 - accuracy: 0.9930\n",
            "Epoch 00164: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0291 - accuracy: 0.9923 - val_loss: 0.0627 - val_accuracy: 0.9862\n",
            "Epoch 165/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0297 - accuracy: 0.9924\n",
            "Epoch 00165: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0293 - accuracy: 0.9919 - val_loss: 0.0617 - val_accuracy: 0.9877\n",
            "Epoch 166/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0303 - accuracy: 0.9923\n",
            "Epoch 00166: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0298 - accuracy: 0.9925 - val_loss: 0.0611 - val_accuracy: 0.9885\n",
            "Epoch 167/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0311 - accuracy: 0.9917\n",
            "Epoch 00167: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0300 - accuracy: 0.9919 - val_loss: 0.0721 - val_accuracy: 0.9869\n",
            "Epoch 168/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0407 - accuracy: 0.9850\n",
            "Epoch 00168: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0296 - accuracy: 0.9927 - val_loss: 0.0624 - val_accuracy: 0.9885\n",
            "Epoch 169/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9913\n",
            "Epoch 00169: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0305 - accuracy: 0.9913 - val_loss: 0.0629 - val_accuracy: 0.9877\n",
            "Epoch 170/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0428 - accuracy: 0.9800\n",
            "Epoch 00170: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0315 - accuracy: 0.9921 - val_loss: 0.0614 - val_accuracy: 0.9892\n",
            "Epoch 171/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0374 - accuracy: 0.9890\n",
            "Epoch 00171: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0357 - accuracy: 0.9898 - val_loss: 0.0874 - val_accuracy: 0.9862\n",
            "Epoch 172/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0668 - accuracy: 0.9794\n",
            "Epoch 00172: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0664 - accuracy: 0.9798 - val_loss: 0.0699 - val_accuracy: 0.9862\n",
            "Epoch 173/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0372 - accuracy: 0.9896\n",
            "Epoch 00173: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0363 - accuracy: 0.9900 - val_loss: 0.0638 - val_accuracy: 0.9877\n",
            "Epoch 174/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0336 - accuracy: 0.9898\n",
            "Epoch 00174: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0327 - accuracy: 0.9902 - val_loss: 0.0612 - val_accuracy: 0.9892\n",
            "Epoch 175/200\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0297 - accuracy: 0.9914\n",
            "Epoch 00175: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0291 - accuracy: 0.9919 - val_loss: 0.0596 - val_accuracy: 0.9885\n",
            "Epoch 176/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9921\n",
            "Epoch 00176: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0288 - accuracy: 0.9921 - val_loss: 0.0652 - val_accuracy: 0.9900\n",
            "Epoch 177/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0321 - accuracy: 0.9922\n",
            "Epoch 00177: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0343 - accuracy: 0.9913 - val_loss: 0.0750 - val_accuracy: 0.9877\n",
            "Epoch 178/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0122 - accuracy: 0.9950\n",
            "Epoch 00178: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0340 - accuracy: 0.9913 - val_loss: 0.0689 - val_accuracy: 0.9877\n",
            "Epoch 179/200\n",
            "19/26 [====================>.........] - ETA: 0s - loss: 0.0295 - accuracy: 0.9924\n",
            "Epoch 00179: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0299 - accuracy: 0.9923 - val_loss: 0.0631 - val_accuracy: 0.9892\n",
            "Epoch 180/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0283 - accuracy: 0.9933\n",
            "Epoch 00180: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.9935 - val_loss: 0.0618 - val_accuracy: 0.9892\n",
            "Epoch 181/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0308 - accuracy: 0.9917\n",
            "Epoch 00181: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0293 - accuracy: 0.9917 - val_loss: 0.0626 - val_accuracy: 0.9885\n",
            "Epoch 182/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0289 - accuracy: 0.9917\n",
            "Epoch 00182: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0302 - accuracy: 0.9915 - val_loss: 0.0654 - val_accuracy: 0.9877\n",
            "Epoch 183/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9913\n",
            "Epoch 00183: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0315 - accuracy: 0.9913 - val_loss: 0.0635 - val_accuracy: 0.9877\n",
            "Epoch 184/200\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0355 - accuracy: 0.9907\n",
            "Epoch 00184: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0400 - accuracy: 0.9894 - val_loss: 0.0634 - val_accuracy: 0.9877\n",
            "Epoch 185/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0223 - accuracy: 0.9900\n",
            "Epoch 00185: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0408 - accuracy: 0.9881 - val_loss: 0.0779 - val_accuracy: 0.9877\n",
            "Epoch 186/200\n",
            " 1/26 [>.............................] - ETA: 0s - loss: 0.0843 - accuracy: 0.9850\n",
            "Epoch 00186: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.0328 - accuracy: 0.9921 - val_loss: 0.0644 - val_accuracy: 0.9869\n",
            "Epoch 187/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0284 - accuracy: 0.9935\n",
            "Epoch 00187: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0282 - accuracy: 0.9929 - val_loss: 0.0653 - val_accuracy: 0.9862\n",
            "Epoch 188/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0339 - accuracy: 0.9922\n",
            "Epoch 00188: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0322 - accuracy: 0.9923 - val_loss: 0.0628 - val_accuracy: 0.9892\n",
            "Epoch 189/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9931\n",
            "Epoch 00189: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0302 - accuracy: 0.9931 - val_loss: 0.0641 - val_accuracy: 0.9885\n",
            "Epoch 190/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0307 - accuracy: 0.9917\n",
            "Epoch 00190: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0326 - accuracy: 0.9910 - val_loss: 0.0680 - val_accuracy: 0.9862\n",
            "Epoch 191/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9908\n",
            "Epoch 00191: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0314 - accuracy: 0.9908 - val_loss: 0.0631 - val_accuracy: 0.9877\n",
            "Epoch 192/200\n",
            "20/26 [======================>.......] - ETA: 0s - loss: 0.0287 - accuracy: 0.9927\n",
            "Epoch 00192: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0288 - accuracy: 0.9931 - val_loss: 0.0616 - val_accuracy: 0.9877\n",
            "Epoch 193/200\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9917\n",
            "Epoch 00193: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0294 - accuracy: 0.9917 - val_loss: 0.0724 - val_accuracy: 0.9885\n",
            "Epoch 194/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0288 - accuracy: 0.9924\n",
            "Epoch 00194: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0303 - accuracy: 0.9919 - val_loss: 0.0644 - val_accuracy: 0.9892\n",
            "Epoch 195/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0314 - accuracy: 0.9913\n",
            "Epoch 00195: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0318 - accuracy: 0.9913 - val_loss: 0.0675 - val_accuracy: 0.9846\n",
            "Epoch 196/200\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0452 - accuracy: 0.9866\n",
            "Epoch 00196: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0440 - accuracy: 0.9869 - val_loss: 0.0622 - val_accuracy: 0.9885\n",
            "Epoch 197/200\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0379 - accuracy: 0.9896\n",
            "Epoch 00197: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0363 - accuracy: 0.9902 - val_loss: 0.0680 - val_accuracy: 0.9892\n",
            "Epoch 198/200\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0299 - accuracy: 0.9921\n",
            "Epoch 00198: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0293 - accuracy: 0.9919 - val_loss: 0.0639 - val_accuracy: 0.9885\n",
            "Epoch 199/200\n",
            "19/26 [====================>.........] - ETA: 0s - loss: 0.0309 - accuracy: 0.9926\n",
            "Epoch 00199: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0304 - accuracy: 0.9923 - val_loss: 0.0644 - val_accuracy: 0.9892\n",
            "Epoch 200/200\n",
            "17/26 [==================>...........] - ETA: 0s - loss: 0.0318 - accuracy: 0.9924\n",
            "Epoch 00200: val_loss did not improve from 0.05631\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.0328 - accuracy: 0.9915 - val_loss: 0.0630 - val_accuracy: 0.9885\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9e331dd990>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래프로 확인하기\n",
        "# 딥러닝 프레임워크가 만들어 낸 모델을 업데이트 하는 과정\n",
        "# 에포크를 얼마나 지정할지를 결정해야함\n",
        "df = df_pre.sample(frac=0.15)\n",
        "dataset=df.values\n",
        "X=dataset[:,0:12]\n",
        "Y=dataset[:,12]"
      ],
      "metadata": {
        "id": "bXf_u67Byilj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X,Y, validation_split=0.33, epochs=3500, batch_size=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2gM2aU_9a8H",
        "outputId": "d0087224-4ab8-4f0b-9fec-398bb4f02a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0248 - accuracy: 0.9908 - val_loss: 0.0552 - val_accuracy: 0.9783\n",
            "Epoch 1002/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0280 - accuracy: 0.9908 - val_loss: 0.0531 - val_accuracy: 0.9783\n",
            "Epoch 1003/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0268 - accuracy: 0.9923 - val_loss: 0.0466 - val_accuracy: 0.9814\n",
            "Epoch 1004/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0248 - accuracy: 0.9893 - val_loss: 0.0459 - val_accuracy: 0.9814\n",
            "Epoch 1005/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0259 - accuracy: 0.9908 - val_loss: 0.0473 - val_accuracy: 0.9814\n",
            "Epoch 1006/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0253 - accuracy: 0.9923 - val_loss: 0.0497 - val_accuracy: 0.9783\n",
            "Epoch 1007/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0250 - accuracy: 0.9923 - val_loss: 0.0476 - val_accuracy: 0.9814\n",
            "Epoch 1008/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0247 - accuracy: 0.9939 - val_loss: 0.0467 - val_accuracy: 0.9814\n",
            "Epoch 1009/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0250 - accuracy: 0.9877 - val_loss: 0.0464 - val_accuracy: 0.9814\n",
            "Epoch 1010/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0252 - accuracy: 0.9877 - val_loss: 0.0474 - val_accuracy: 0.9814\n",
            "Epoch 1011/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0251 - accuracy: 0.9923 - val_loss: 0.0501 - val_accuracy: 0.9783\n",
            "Epoch 1012/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0251 - accuracy: 0.9908 - val_loss: 0.0481 - val_accuracy: 0.9783\n",
            "Epoch 1013/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0247 - accuracy: 0.9923 - val_loss: 0.0466 - val_accuracy: 0.9814\n",
            "Epoch 1014/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0250 - accuracy: 0.9908 - val_loss: 0.0490 - val_accuracy: 0.9783\n",
            "Epoch 1015/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0245 - accuracy: 0.9908 - val_loss: 0.0544 - val_accuracy: 0.9783\n",
            "Epoch 1016/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0269 - accuracy: 0.9923 - val_loss: 0.0494 - val_accuracy: 0.9783\n",
            "Epoch 1017/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0252 - accuracy: 0.9923 - val_loss: 0.0465 - val_accuracy: 0.9814\n",
            "Epoch 1018/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0267 - accuracy: 0.9908 - val_loss: 0.0473 - val_accuracy: 0.9814\n",
            "Epoch 1019/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0254 - accuracy: 0.9877 - val_loss: 0.0500 - val_accuracy: 0.9783\n",
            "Epoch 1020/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0249 - accuracy: 0.9923 - val_loss: 0.0483 - val_accuracy: 0.9814\n",
            "Epoch 1021/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0243 - accuracy: 0.9939 - val_loss: 0.0493 - val_accuracy: 0.9783\n",
            "Epoch 1022/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0258 - accuracy: 0.9923 - val_loss: 0.0475 - val_accuracy: 0.9814\n",
            "Epoch 1023/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0249 - accuracy: 0.9908 - val_loss: 0.0460 - val_accuracy: 0.9814\n",
            "Epoch 1024/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0261 - accuracy: 0.9893 - val_loss: 0.0478 - val_accuracy: 0.9814\n",
            "Epoch 1025/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0237 - accuracy: 0.9939 - val_loss: 0.0531 - val_accuracy: 0.9783\n",
            "Epoch 1026/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0272 - accuracy: 0.9923 - val_loss: 0.0506 - val_accuracy: 0.9783\n",
            "Epoch 1027/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0238 - accuracy: 0.9923 - val_loss: 0.0455 - val_accuracy: 0.9814\n",
            "Epoch 1028/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0290 - accuracy: 0.9893 - val_loss: 0.0452 - val_accuracy: 0.9814\n",
            "Epoch 1029/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0255 - accuracy: 0.9908 - val_loss: 0.0531 - val_accuracy: 0.9783\n",
            "Epoch 1030/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0271 - accuracy: 0.9908 - val_loss: 0.0614 - val_accuracy: 0.9814\n",
            "Epoch 1031/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0311 - accuracy: 0.9893 - val_loss: 0.0482 - val_accuracy: 0.9783\n",
            "Epoch 1032/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0233 - accuracy: 0.9954 - val_loss: 0.0472 - val_accuracy: 0.9876\n",
            "Epoch 1033/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0326 - accuracy: 0.9893 - val_loss: 0.0458 - val_accuracy: 0.9814\n",
            "Epoch 1034/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0277 - accuracy: 0.9893 - val_loss: 0.0561 - val_accuracy: 0.9783\n",
            "Epoch 1035/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0277 - accuracy: 0.9923 - val_loss: 0.0505 - val_accuracy: 0.9783\n",
            "Epoch 1036/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0251 - accuracy: 0.9893 - val_loss: 0.0464 - val_accuracy: 0.9814\n",
            "Epoch 1037/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0241 - accuracy: 0.9908 - val_loss: 0.0466 - val_accuracy: 0.9814\n",
            "Epoch 1038/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0241 - accuracy: 0.9908 - val_loss: 0.0478 - val_accuracy: 0.9783\n",
            "Epoch 1039/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0237 - accuracy: 0.9923 - val_loss: 0.0471 - val_accuracy: 0.9814\n",
            "Epoch 1040/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0238 - accuracy: 0.9908 - val_loss: 0.0467 - val_accuracy: 0.9814\n",
            "Epoch 1041/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0235 - accuracy: 0.9893 - val_loss: 0.0483 - val_accuracy: 0.9783\n",
            "Epoch 1042/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0237 - accuracy: 0.9923 - val_loss: 0.0496 - val_accuracy: 0.9783\n",
            "Epoch 1043/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0242 - accuracy: 0.9923 - val_loss: 0.0485 - val_accuracy: 0.9783\n",
            "Epoch 1044/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0242 - accuracy: 0.9923 - val_loss: 0.0472 - val_accuracy: 0.9783\n",
            "Epoch 1045/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0237 - accuracy: 0.9923 - val_loss: 0.0477 - val_accuracy: 0.9783\n",
            "Epoch 1046/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0240 - accuracy: 0.9923 - val_loss: 0.0468 - val_accuracy: 0.9814\n",
            "Epoch 1047/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0232 - accuracy: 0.9923 - val_loss: 0.0456 - val_accuracy: 0.9814\n",
            "Epoch 1048/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0248 - accuracy: 0.9923 - val_loss: 0.0467 - val_accuracy: 0.9814\n",
            "Epoch 1049/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0234 - accuracy: 0.9908 - val_loss: 0.0516 - val_accuracy: 0.9783\n",
            "Epoch 1050/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0255 - accuracy: 0.9923 - val_loss: 0.0498 - val_accuracy: 0.9783\n",
            "Epoch 1051/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0230 - accuracy: 0.9908 - val_loss: 0.0460 - val_accuracy: 0.9814\n",
            "Epoch 1052/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0268 - accuracy: 0.9893 - val_loss: 0.0457 - val_accuracy: 0.9814\n",
            "Epoch 1053/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0255 - accuracy: 0.9923 - val_loss: 0.0493 - val_accuracy: 0.9783\n",
            "Epoch 1054/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0237 - accuracy: 0.9923 - val_loss: 0.0507 - val_accuracy: 0.9783\n",
            "Epoch 1055/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0241 - accuracy: 0.9923 - val_loss: 0.0473 - val_accuracy: 0.9814\n",
            "Epoch 1056/3500\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.0233 - accuracy: 0.9939 - val_loss: 0.0462 - val_accuracy: 0.9814\n",
            "Epoch 1057/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0234 - accuracy: 0.9908 - val_loss: 0.0471 - val_accuracy: 0.9814\n",
            "Epoch 1058/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0231 - accuracy: 0.9939 - val_loss: 0.0472 - val_accuracy: 0.9814\n",
            "Epoch 1059/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0232 - accuracy: 0.9923 - val_loss: 0.0466 - val_accuracy: 0.9814\n",
            "Epoch 1060/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0232 - accuracy: 0.9923 - val_loss: 0.0464 - val_accuracy: 0.9814\n",
            "Epoch 1061/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0231 - accuracy: 0.9923 - val_loss: 0.0477 - val_accuracy: 0.9783\n",
            "Epoch 1062/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0230 - accuracy: 0.9923 - val_loss: 0.0496 - val_accuracy: 0.9783\n",
            "Epoch 1063/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0236 - accuracy: 0.9923 - val_loss: 0.0493 - val_accuracy: 0.9783\n",
            "Epoch 1064/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0232 - accuracy: 0.9923 - val_loss: 0.0465 - val_accuracy: 0.9814\n",
            "Epoch 1065/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0229 - accuracy: 0.9923 - val_loss: 0.0454 - val_accuracy: 0.9814\n",
            "Epoch 1066/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0234 - accuracy: 0.9908 - val_loss: 0.0468 - val_accuracy: 0.9814\n",
            "Epoch 1067/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0228 - accuracy: 0.9923 - val_loss: 0.0506 - val_accuracy: 0.9783\n",
            "Epoch 1068/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0243 - accuracy: 0.9923 - val_loss: 0.0469 - val_accuracy: 0.9783\n",
            "Epoch 1069/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0226 - accuracy: 0.9923 - val_loss: 0.0441 - val_accuracy: 0.9814\n",
            "Epoch 1070/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0257 - accuracy: 0.9893 - val_loss: 0.0442 - val_accuracy: 0.9814\n",
            "Epoch 1071/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0235 - accuracy: 0.9923 - val_loss: 0.0493 - val_accuracy: 0.9783\n",
            "Epoch 1072/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0234 - accuracy: 0.9923 - val_loss: 0.0540 - val_accuracy: 0.9783\n",
            "Epoch 1073/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0262 - accuracy: 0.9923 - val_loss: 0.0508 - val_accuracy: 0.9783\n",
            "Epoch 1074/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0241 - accuracy: 0.9923 - val_loss: 0.0449 - val_accuracy: 0.9814\n",
            "Epoch 1075/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0225 - accuracy: 0.9923 - val_loss: 0.0439 - val_accuracy: 0.9814\n",
            "Epoch 1076/3500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0259 - accuracy: 0.9908 - val_loss: 0.0452 - val_accuracy: 0.9814\n",
            "Epoch 1077/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0233 - accuracy: 0.9908 - val_loss: 0.0533 - val_accuracy: 0.9783\n",
            "Epoch 1078/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0255 - accuracy: 0.9923 - val_loss: 0.0487 - val_accuracy: 0.9783\n",
            "Epoch 1079/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0225 - accuracy: 0.9939 - val_loss: 0.0442 - val_accuracy: 0.9814\n",
            "Epoch 1080/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0263 - accuracy: 0.9923 - val_loss: 0.0444 - val_accuracy: 0.9814\n",
            "Epoch 1081/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0231 - accuracy: 0.9923 - val_loss: 0.0535 - val_accuracy: 0.9783\n",
            "Epoch 1082/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0257 - accuracy: 0.9923 - val_loss: 0.0539 - val_accuracy: 0.9783\n",
            "Epoch 1083/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0264 - accuracy: 0.9923 - val_loss: 0.0473 - val_accuracy: 0.9783\n",
            "Epoch 1084/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0225 - accuracy: 0.9923 - val_loss: 0.0442 - val_accuracy: 0.9814\n",
            "Epoch 1085/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0235 - accuracy: 0.9893 - val_loss: 0.0442 - val_accuracy: 0.9814\n",
            "Epoch 1086/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0233 - accuracy: 0.9908 - val_loss: 0.0488 - val_accuracy: 0.9783\n",
            "Epoch 1087/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0246 - accuracy: 0.9939 - val_loss: 0.0541 - val_accuracy: 0.9783\n",
            "Epoch 1088/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0251 - accuracy: 0.9923 - val_loss: 0.0465 - val_accuracy: 0.9814\n",
            "Epoch 1089/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0230 - accuracy: 0.9923 - val_loss: 0.0447 - val_accuracy: 0.9814\n",
            "Epoch 1090/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0234 - accuracy: 0.9923 - val_loss: 0.0466 - val_accuracy: 0.9814\n",
            "Epoch 1091/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0220 - accuracy: 0.9939 - val_loss: 0.0516 - val_accuracy: 0.9783\n",
            "Epoch 1092/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0244 - accuracy: 0.9923 - val_loss: 0.0501 - val_accuracy: 0.9783\n",
            "Epoch 1093/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0241 - accuracy: 0.9923 - val_loss: 0.0450 - val_accuracy: 0.9814\n",
            "Epoch 1094/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0224 - accuracy: 0.9908 - val_loss: 0.0438 - val_accuracy: 0.9814\n",
            "Epoch 1095/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0235 - accuracy: 0.9923 - val_loss: 0.0438 - val_accuracy: 0.9814\n",
            "Epoch 1096/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0234 - accuracy: 0.9923 - val_loss: 0.0470 - val_accuracy: 0.9783\n",
            "Epoch 1097/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0221 - accuracy: 0.9923 - val_loss: 0.0508 - val_accuracy: 0.9783\n",
            "Epoch 1098/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0238 - accuracy: 0.9923 - val_loss: 0.0504 - val_accuracy: 0.9783\n",
            "Epoch 1099/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0233 - accuracy: 0.9923 - val_loss: 0.0461 - val_accuracy: 0.9814\n",
            "Epoch 1100/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0218 - accuracy: 0.9923 - val_loss: 0.0445 - val_accuracy: 0.9814\n",
            "Epoch 1101/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0256 - accuracy: 0.9939 - val_loss: 0.0449 - val_accuracy: 0.9814\n",
            "Epoch 1102/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0220 - accuracy: 0.9908 - val_loss: 0.0520 - val_accuracy: 0.9783\n",
            "Epoch 1103/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0249 - accuracy: 0.9923 - val_loss: 0.0505 - val_accuracy: 0.9783\n",
            "Epoch 1104/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0235 - accuracy: 0.9923 - val_loss: 0.0437 - val_accuracy: 0.9814\n",
            "Epoch 1105/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0232 - accuracy: 0.9923 - val_loss: 0.0435 - val_accuracy: 0.9814\n",
            "Epoch 1106/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0236 - accuracy: 0.9923 - val_loss: 0.0465 - val_accuracy: 0.9783\n",
            "Epoch 1107/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0226 - accuracy: 0.9923 - val_loss: 0.0519 - val_accuracy: 0.9783\n",
            "Epoch 1108/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0243 - accuracy: 0.9923 - val_loss: 0.0475 - val_accuracy: 0.9783\n",
            "Epoch 1109/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0217 - accuracy: 0.9939 - val_loss: 0.0437 - val_accuracy: 0.9814\n",
            "Epoch 1110/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0232 - accuracy: 0.9939 - val_loss: 0.0435 - val_accuracy: 0.9814\n",
            "Epoch 1111/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0240 - accuracy: 0.9908 - val_loss: 0.0463 - val_accuracy: 0.9783\n",
            "Epoch 1112/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0220 - accuracy: 0.9923 - val_loss: 0.0494 - val_accuracy: 0.9783\n",
            "Epoch 1113/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0227 - accuracy: 0.9923 - val_loss: 0.0459 - val_accuracy: 0.9814\n",
            "Epoch 1114/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0212 - accuracy: 0.9954 - val_loss: 0.0431 - val_accuracy: 0.9814\n",
            "Epoch 1115/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0243 - accuracy: 0.9923 - val_loss: 0.0429 - val_accuracy: 0.9814\n",
            "Epoch 1116/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0221 - accuracy: 0.9923 - val_loss: 0.0501 - val_accuracy: 0.9783\n",
            "Epoch 1117/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0254 - accuracy: 0.9923 - val_loss: 0.0497 - val_accuracy: 0.9783\n",
            "Epoch 1118/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0222 - accuracy: 0.9939 - val_loss: 0.0434 - val_accuracy: 0.9814\n",
            "Epoch 1119/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0253 - accuracy: 0.9908 - val_loss: 0.0438 - val_accuracy: 0.9814\n",
            "Epoch 1120/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0256 - accuracy: 0.9908 - val_loss: 0.0473 - val_accuracy: 0.9783\n",
            "Epoch 1121/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0216 - accuracy: 0.9939 - val_loss: 0.0510 - val_accuracy: 0.9783\n",
            "Epoch 1122/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0233 - accuracy: 0.9923 - val_loss: 0.0503 - val_accuracy: 0.9783\n",
            "Epoch 1123/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0228 - accuracy: 0.9923 - val_loss: 0.0455 - val_accuracy: 0.9814\n",
            "Epoch 1124/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0223 - accuracy: 0.9939 - val_loss: 0.0447 - val_accuracy: 0.9814\n",
            "Epoch 1125/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0215 - accuracy: 0.9939 - val_loss: 0.0480 - val_accuracy: 0.9783\n",
            "Epoch 1126/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0222 - accuracy: 0.9939 - val_loss: 0.0477 - val_accuracy: 0.9783\n",
            "Epoch 1127/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0218 - accuracy: 0.9939 - val_loss: 0.0446 - val_accuracy: 0.9814\n",
            "Epoch 1128/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0216 - accuracy: 0.9908 - val_loss: 0.0442 - val_accuracy: 0.9814\n",
            "Epoch 1129/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0217 - accuracy: 0.9908 - val_loss: 0.0460 - val_accuracy: 0.9814\n",
            "Epoch 1130/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0214 - accuracy: 0.9954 - val_loss: 0.0474 - val_accuracy: 0.9783\n",
            "Epoch 1131/3500\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.0218 - accuracy: 0.9939 - val_loss: 0.0463 - val_accuracy: 0.9814\n",
            "Epoch 1132/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0220 - accuracy: 0.9939 - val_loss: 0.0442 - val_accuracy: 0.9814\n",
            "Epoch 1133/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0224 - accuracy: 0.9908 - val_loss: 0.0443 - val_accuracy: 0.9814\n",
            "Epoch 1134/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0244 - accuracy: 0.9939 - val_loss: 0.0462 - val_accuracy: 0.9814\n",
            "Epoch 1135/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0221 - accuracy: 0.9939 - val_loss: 0.0474 - val_accuracy: 0.9814\n",
            "Epoch 1136/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0212 - accuracy: 0.9939 - val_loss: 0.0452 - val_accuracy: 0.9814\n",
            "Epoch 1137/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0219 - accuracy: 0.9939 - val_loss: 0.0455 - val_accuracy: 0.9814\n",
            "Epoch 1138/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0214 - accuracy: 0.9923 - val_loss: 0.0491 - val_accuracy: 0.9814\n",
            "Epoch 1139/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0215 - accuracy: 0.9923 - val_loss: 0.0543 - val_accuracy: 0.9783\n",
            "Epoch 1140/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0244 - accuracy: 0.9923 - val_loss: 0.0516 - val_accuracy: 0.9783\n",
            "Epoch 1141/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0230 - accuracy: 0.9923 - val_loss: 0.0459 - val_accuracy: 0.9814\n",
            "Epoch 1142/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0212 - accuracy: 0.9954 - val_loss: 0.0446 - val_accuracy: 0.9814\n",
            "Epoch 1143/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0214 - accuracy: 0.9923 - val_loss: 0.0472 - val_accuracy: 0.9783\n",
            "Epoch 1144/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0212 - accuracy: 0.9923 - val_loss: 0.0528 - val_accuracy: 0.9783\n",
            "Epoch 1145/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0243 - accuracy: 0.9923 - val_loss: 0.0471 - val_accuracy: 0.9783\n",
            "Epoch 1146/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0215 - accuracy: 0.9923 - val_loss: 0.0432 - val_accuracy: 0.9814\n",
            "Epoch 1147/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0249 - accuracy: 0.9923 - val_loss: 0.0437 - val_accuracy: 0.9814\n",
            "Epoch 1148/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0216 - accuracy: 0.9893 - val_loss: 0.0535 - val_accuracy: 0.9783\n",
            "Epoch 1149/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0252 - accuracy: 0.9923 - val_loss: 0.0512 - val_accuracy: 0.9783\n",
            "Epoch 1150/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0234 - accuracy: 0.9939 - val_loss: 0.0439 - val_accuracy: 0.9814\n",
            "Epoch 1151/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0229 - accuracy: 0.9939 - val_loss: 0.0443 - val_accuracy: 0.9814\n",
            "Epoch 1152/3500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0210 - accuracy: 0.9908 - val_loss: 0.0509 - val_accuracy: 0.9783\n",
            "Epoch 1153/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0225 - accuracy: 0.9923 - val_loss: 0.0529 - val_accuracy: 0.9783\n",
            "Epoch 1154/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0233 - accuracy: 0.9923 - val_loss: 0.0462 - val_accuracy: 0.9814\n",
            "Epoch 1155/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0209 - accuracy: 0.9939 - val_loss: 0.0436 - val_accuracy: 0.9845\n",
            "Epoch 1156/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0251 - accuracy: 0.9939 - val_loss: 0.0430 - val_accuracy: 0.9814\n",
            "Epoch 1157/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0216 - accuracy: 0.9939 - val_loss: 0.0469 - val_accuracy: 0.9783\n",
            "Epoch 1158/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0218 - accuracy: 0.9923 - val_loss: 0.0477 - val_accuracy: 0.9783\n",
            "Epoch 1159/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0215 - accuracy: 0.9923 - val_loss: 0.0431 - val_accuracy: 0.9814\n",
            "Epoch 1160/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0210 - accuracy: 0.9908 - val_loss: 0.0428 - val_accuracy: 0.9814\n",
            "Epoch 1161/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0208 - accuracy: 0.9908 - val_loss: 0.0455 - val_accuracy: 0.9783\n",
            "Epoch 1162/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0207 - accuracy: 0.9939 - val_loss: 0.0505 - val_accuracy: 0.9783\n",
            "Epoch 1163/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0234 - accuracy: 0.9939 - val_loss: 0.0498 - val_accuracy: 0.9783\n",
            "Epoch 1164/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0224 - accuracy: 0.9939 - val_loss: 0.0431 - val_accuracy: 0.9814\n",
            "Epoch 1165/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0210 - accuracy: 0.9923 - val_loss: 0.0426 - val_accuracy: 0.9814\n",
            "Epoch 1166/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0234 - accuracy: 0.9908 - val_loss: 0.0433 - val_accuracy: 0.9814\n",
            "Epoch 1167/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0211 - accuracy: 0.9923 - val_loss: 0.0451 - val_accuracy: 0.9814\n",
            "Epoch 1168/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0205 - accuracy: 0.9954 - val_loss: 0.0462 - val_accuracy: 0.9814\n",
            "Epoch 1169/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0207 - accuracy: 0.9939 - val_loss: 0.0449 - val_accuracy: 0.9814\n",
            "Epoch 1170/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0211 - accuracy: 0.9939 - val_loss: 0.0440 - val_accuracy: 0.9814\n",
            "Epoch 1171/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0206 - accuracy: 0.9923 - val_loss: 0.0474 - val_accuracy: 0.9783\n",
            "Epoch 1172/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0213 - accuracy: 0.9923 - val_loss: 0.0491 - val_accuracy: 0.9783\n",
            "Epoch 1173/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0211 - accuracy: 0.9923 - val_loss: 0.0447 - val_accuracy: 0.9814\n",
            "Epoch 1174/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0211 - accuracy: 0.9923 - val_loss: 0.0440 - val_accuracy: 0.9814\n",
            "Epoch 1175/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0209 - accuracy: 0.9939 - val_loss: 0.0464 - val_accuracy: 0.9814\n",
            "Epoch 1176/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0204 - accuracy: 0.9939 - val_loss: 0.0469 - val_accuracy: 0.9783\n",
            "Epoch 1177/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0206 - accuracy: 0.9939 - val_loss: 0.0466 - val_accuracy: 0.9783\n",
            "Epoch 1178/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0207 - accuracy: 0.9939 - val_loss: 0.0461 - val_accuracy: 0.9783\n",
            "Epoch 1179/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0205 - accuracy: 0.9939 - val_loss: 0.0462 - val_accuracy: 0.9783\n",
            "Epoch 1180/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0205 - accuracy: 0.9939 - val_loss: 0.0451 - val_accuracy: 0.9814\n",
            "Epoch 1181/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0204 - accuracy: 0.9939 - val_loss: 0.0441 - val_accuracy: 0.9814\n",
            "Epoch 1182/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0203 - accuracy: 0.9954 - val_loss: 0.0466 - val_accuracy: 0.9814\n",
            "Epoch 1183/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0205 - accuracy: 0.9939 - val_loss: 0.0486 - val_accuracy: 0.9783\n",
            "Epoch 1184/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0207 - accuracy: 0.9923 - val_loss: 0.0457 - val_accuracy: 0.9814\n",
            "Epoch 1185/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0199 - accuracy: 0.9954 - val_loss: 0.0442 - val_accuracy: 0.9814\n",
            "Epoch 1186/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0208 - accuracy: 0.9939 - val_loss: 0.0444 - val_accuracy: 0.9814\n",
            "Epoch 1187/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0202 - accuracy: 0.9939 - val_loss: 0.0480 - val_accuracy: 0.9783\n",
            "Epoch 1188/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0214 - accuracy: 0.9923 - val_loss: 0.0495 - val_accuracy: 0.9783\n",
            "Epoch 1189/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0208 - accuracy: 0.9923 - val_loss: 0.0435 - val_accuracy: 0.9814\n",
            "Epoch 1190/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0206 - accuracy: 0.9908 - val_loss: 0.0428 - val_accuracy: 0.9814\n",
            "Epoch 1191/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0222 - accuracy: 0.9954 - val_loss: 0.0449 - val_accuracy: 0.9814\n",
            "Epoch 1192/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0209 - accuracy: 0.9939 - val_loss: 0.0469 - val_accuracy: 0.9783\n",
            "Epoch 1193/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0200 - accuracy: 0.9939 - val_loss: 0.0428 - val_accuracy: 0.9814\n",
            "Epoch 1194/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0214 - accuracy: 0.9954 - val_loss: 0.0427 - val_accuracy: 0.9845\n",
            "Epoch 1195/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0222 - accuracy: 0.9939 - val_loss: 0.0472 - val_accuracy: 0.9783\n",
            "Epoch 1196/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0204 - accuracy: 0.9939 - val_loss: 0.0581 - val_accuracy: 0.9814\n",
            "Epoch 1197/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0287 - accuracy: 0.9893 - val_loss: 0.0469 - val_accuracy: 0.9783\n",
            "Epoch 1198/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0220 - accuracy: 0.9939 - val_loss: 0.0491 - val_accuracy: 0.9814\n",
            "Epoch 1199/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0345 - accuracy: 0.9893 - val_loss: 0.0437 - val_accuracy: 0.9814\n",
            "Epoch 1200/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0206 - accuracy: 0.9923 - val_loss: 0.0541 - val_accuracy: 0.9783\n",
            "Epoch 1201/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0243 - accuracy: 0.9923 - val_loss: 0.0499 - val_accuracy: 0.9783\n",
            "Epoch 1202/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0205 - accuracy: 0.9923 - val_loss: 0.0434 - val_accuracy: 0.9814\n",
            "Epoch 1203/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0244 - accuracy: 0.9939 - val_loss: 0.0432 - val_accuracy: 0.9814\n",
            "Epoch 1204/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0207 - accuracy: 0.9954 - val_loss: 0.0512 - val_accuracy: 0.9783\n",
            "Epoch 1205/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0228 - accuracy: 0.9923 - val_loss: 0.0502 - val_accuracy: 0.9783\n",
            "Epoch 1206/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0202 - accuracy: 0.9923 - val_loss: 0.0426 - val_accuracy: 0.9814\n",
            "Epoch 1207/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0235 - accuracy: 0.9939 - val_loss: 0.0423 - val_accuracy: 0.9814\n",
            "Epoch 1208/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0219 - accuracy: 0.9923 - val_loss: 0.0490 - val_accuracy: 0.9783\n",
            "Epoch 1209/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0218 - accuracy: 0.9923 - val_loss: 0.0530 - val_accuracy: 0.9783\n",
            "Epoch 1210/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0239 - accuracy: 0.9923 - val_loss: 0.0466 - val_accuracy: 0.9783\n",
            "Epoch 1211/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0203 - accuracy: 0.9939 - val_loss: 0.0429 - val_accuracy: 0.9814\n",
            "Epoch 1212/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0202 - accuracy: 0.9923 - val_loss: 0.0433 - val_accuracy: 0.9814\n",
            "Epoch 1213/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0196 - accuracy: 0.9939 - val_loss: 0.0470 - val_accuracy: 0.9783\n",
            "Epoch 1214/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0206 - accuracy: 0.9939 - val_loss: 0.0476 - val_accuracy: 0.9783\n",
            "Epoch 1215/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0208 - accuracy: 0.9939 - val_loss: 0.0449 - val_accuracy: 0.9783\n",
            "Epoch 1216/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0199 - accuracy: 0.9954 - val_loss: 0.0420 - val_accuracy: 0.9814\n",
            "Epoch 1217/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0201 - accuracy: 0.9923 - val_loss: 0.0416 - val_accuracy: 0.9814\n",
            "Epoch 1218/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0207 - accuracy: 0.9923 - val_loss: 0.0426 - val_accuracy: 0.9814\n",
            "Epoch 1219/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0197 - accuracy: 0.9923 - val_loss: 0.0457 - val_accuracy: 0.9814\n",
            "Epoch 1220/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.0477 - val_accuracy: 0.9783\n",
            "Epoch 1221/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0204 - accuracy: 0.9939 - val_loss: 0.0467 - val_accuracy: 0.9814\n",
            "Epoch 1222/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.0456 - val_accuracy: 0.9814\n",
            "Epoch 1223/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0196 - accuracy: 0.9954 - val_loss: 0.0429 - val_accuracy: 0.9814\n",
            "Epoch 1224/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0209 - accuracy: 0.9939 - val_loss: 0.0427 - val_accuracy: 0.9814\n",
            "Epoch 1225/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0219 - accuracy: 0.9954 - val_loss: 0.0463 - val_accuracy: 0.9814\n",
            "Epoch 1226/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0206 - accuracy: 0.9939 - val_loss: 0.0485 - val_accuracy: 0.9783\n",
            "Epoch 1227/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0199 - accuracy: 0.9923 - val_loss: 0.0429 - val_accuracy: 0.9814\n",
            "Epoch 1228/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0205 - accuracy: 0.9923 - val_loss: 0.0424 - val_accuracy: 0.9814\n",
            "Epoch 1229/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0202 - accuracy: 0.9954 - val_loss: 0.0471 - val_accuracy: 0.9783\n",
            "Epoch 1230/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0198 - accuracy: 0.9939 - val_loss: 0.0518 - val_accuracy: 0.9783\n",
            "Epoch 1231/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0221 - accuracy: 0.9923 - val_loss: 0.0475 - val_accuracy: 0.9783\n",
            "Epoch 1232/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0212 - accuracy: 0.9923 - val_loss: 0.0437 - val_accuracy: 0.9814\n",
            "Epoch 1233/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0193 - accuracy: 0.9969 - val_loss: 0.0444 - val_accuracy: 0.9814\n",
            "Epoch 1234/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0192 - accuracy: 0.9969 - val_loss: 0.0452 - val_accuracy: 0.9814\n",
            "Epoch 1235/3500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.0190 - accuracy: 0.9969 - val_loss: 0.0469 - val_accuracy: 0.9814\n",
            "Epoch 1236/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0196 - accuracy: 0.9939 - val_loss: 0.0462 - val_accuracy: 0.9814\n",
            "Epoch 1237/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.0443 - val_accuracy: 0.9814\n",
            "Epoch 1238/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0197 - accuracy: 0.9954 - val_loss: 0.0426 - val_accuracy: 0.9814\n",
            "Epoch 1239/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0205 - accuracy: 0.9954 - val_loss: 0.0424 - val_accuracy: 0.9814\n",
            "Epoch 1240/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0204 - accuracy: 0.9939 - val_loss: 0.0467 - val_accuracy: 0.9814\n",
            "Epoch 1241/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0207 - accuracy: 0.9923 - val_loss: 0.0495 - val_accuracy: 0.9783\n",
            "Epoch 1242/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0201 - accuracy: 0.9923 - val_loss: 0.0427 - val_accuracy: 0.9814\n",
            "Epoch 1243/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0201 - accuracy: 0.9939 - val_loss: 0.0418 - val_accuracy: 0.9814\n",
            "Epoch 1244/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0203 - accuracy: 0.9939 - val_loss: 0.0452 - val_accuracy: 0.9814\n",
            "Epoch 1245/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.0475 - val_accuracy: 0.9783\n",
            "Epoch 1246/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0198 - accuracy: 0.9939 - val_loss: 0.0463 - val_accuracy: 0.9783\n",
            "Epoch 1247/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0192 - accuracy: 0.9954 - val_loss: 0.0428 - val_accuracy: 0.9814\n",
            "Epoch 1248/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.0414 - val_accuracy: 0.9814\n",
            "Epoch 1249/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0200 - accuracy: 0.9954 - val_loss: 0.0423 - val_accuracy: 0.9814\n",
            "Epoch 1250/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0190 - accuracy: 0.9939 - val_loss: 0.0463 - val_accuracy: 0.9783\n",
            "Epoch 1251/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.0451 - val_accuracy: 0.9814\n",
            "Epoch 1252/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0186 - accuracy: 0.9954 - val_loss: 0.0408 - val_accuracy: 0.9814\n",
            "Epoch 1253/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0222 - accuracy: 0.9954 - val_loss: 0.0409 - val_accuracy: 0.9845\n",
            "Epoch 1254/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0208 - accuracy: 0.9954 - val_loss: 0.0482 - val_accuracy: 0.9783\n",
            "Epoch 1255/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0216 - accuracy: 0.9923 - val_loss: 0.0575 - val_accuracy: 0.9814\n",
            "Epoch 1256/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0255 - accuracy: 0.9908 - val_loss: 0.0447 - val_accuracy: 0.9783\n",
            "Epoch 1257/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0191 - accuracy: 0.9939 - val_loss: 0.0417 - val_accuracy: 0.9876\n",
            "Epoch 1258/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0256 - accuracy: 0.9939 - val_loss: 0.0416 - val_accuracy: 0.9814\n",
            "Epoch 1259/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0192 - accuracy: 0.9939 - val_loss: 0.0561 - val_accuracy: 0.9814\n",
            "Epoch 1260/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0256 - accuracy: 0.9908 - val_loss: 0.0520 - val_accuracy: 0.9783\n",
            "Epoch 1261/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0206 - accuracy: 0.9954 - val_loss: 0.0407 - val_accuracy: 0.9814\n",
            "Epoch 1262/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0219 - accuracy: 0.9923 - val_loss: 0.0433 - val_accuracy: 0.9876\n",
            "Epoch 1263/3500\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.0283 - accuracy: 0.9923 - val_loss: 0.0436 - val_accuracy: 0.9814\n",
            "Epoch 1264/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0185 - accuracy: 0.9969 - val_loss: 0.0498 - val_accuracy: 0.9783\n",
            "Epoch 1265/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0208 - accuracy: 0.9939 - val_loss: 0.0500 - val_accuracy: 0.9783\n",
            "Epoch 1266/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0201 - accuracy: 0.9939 - val_loss: 0.0433 - val_accuracy: 0.9814\n",
            "Epoch 1267/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0204 - accuracy: 0.9939 - val_loss: 0.0427 - val_accuracy: 0.9814\n",
            "Epoch 1268/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0198 - accuracy: 0.9969 - val_loss: 0.0476 - val_accuracy: 0.9814\n",
            "Epoch 1269/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0194 - accuracy: 0.9923 - val_loss: 0.0486 - val_accuracy: 0.9814\n",
            "Epoch 1270/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0196 - accuracy: 0.9923 - val_loss: 0.0458 - val_accuracy: 0.9814\n",
            "Epoch 1271/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0191 - accuracy: 0.9939 - val_loss: 0.0434 - val_accuracy: 0.9814\n",
            "Epoch 1272/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0186 - accuracy: 0.9939 - val_loss: 0.0450 - val_accuracy: 0.9814\n",
            "Epoch 1273/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0199 - accuracy: 0.9954 - val_loss: 0.0449 - val_accuracy: 0.9814\n",
            "Epoch 1274/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0195 - accuracy: 0.9954 - val_loss: 0.0426 - val_accuracy: 0.9814\n",
            "Epoch 1275/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0193 - accuracy: 0.9923 - val_loss: 0.0431 - val_accuracy: 0.9814\n",
            "Epoch 1276/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0187 - accuracy: 0.9939 - val_loss: 0.0436 - val_accuracy: 0.9814\n",
            "Epoch 1277/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0185 - accuracy: 0.9969 - val_loss: 0.0447 - val_accuracy: 0.9814\n",
            "Epoch 1278/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0184 - accuracy: 0.9969 - val_loss: 0.0453 - val_accuracy: 0.9814\n",
            "Epoch 1279/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0184 - accuracy: 0.9939 - val_loss: 0.0464 - val_accuracy: 0.9814\n",
            "Epoch 1280/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0193 - accuracy: 0.9923 - val_loss: 0.0439 - val_accuracy: 0.9814\n",
            "Epoch 1281/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.0417 - val_accuracy: 0.9814\n",
            "Epoch 1282/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0218 - accuracy: 0.9954 - val_loss: 0.0417 - val_accuracy: 0.9814\n",
            "Epoch 1283/3500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0183 - accuracy: 0.9954 - val_loss: 0.0500 - val_accuracy: 0.9783\n",
            "Epoch 1284/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0215 - accuracy: 0.9923 - val_loss: 0.0541 - val_accuracy: 0.9783\n",
            "Epoch 1285/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0226 - accuracy: 0.9923 - val_loss: 0.0435 - val_accuracy: 0.9814\n",
            "Epoch 1286/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0177 - accuracy: 0.9969 - val_loss: 0.0420 - val_accuracy: 0.9845\n",
            "Epoch 1287/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0232 - accuracy: 0.9954 - val_loss: 0.0418 - val_accuracy: 0.9814\n",
            "Epoch 1288/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0185 - accuracy: 0.9923 - val_loss: 0.0522 - val_accuracy: 0.9783\n",
            "Epoch 1289/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0219 - accuracy: 0.9923 - val_loss: 0.0526 - val_accuracy: 0.9783\n",
            "Epoch 1290/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0213 - accuracy: 0.9939 - val_loss: 0.0422 - val_accuracy: 0.9814\n",
            "Epoch 1291/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0203 - accuracy: 0.9939 - val_loss: 0.0432 - val_accuracy: 0.9845\n",
            "Epoch 1292/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0243 - accuracy: 0.9923 - val_loss: 0.0453 - val_accuracy: 0.9814\n",
            "Epoch 1293/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0183 - accuracy: 0.9954 - val_loss: 0.0557 - val_accuracy: 0.9783\n",
            "Epoch 1294/3500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0245 - accuracy: 0.9939 - val_loss: 0.0496 - val_accuracy: 0.9783\n",
            "Epoch 1295/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0219 - accuracy: 0.9923 - val_loss: 0.0423 - val_accuracy: 0.9814\n",
            "Epoch 1296/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0196 - accuracy: 0.9939 - val_loss: 0.0425 - val_accuracy: 0.9814\n",
            "Epoch 1297/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.0454 - val_accuracy: 0.9814\n",
            "Epoch 1298/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0186 - accuracy: 0.9954 - val_loss: 0.0492 - val_accuracy: 0.9783\n",
            "Epoch 1299/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.0443 - val_accuracy: 0.9814\n",
            "Epoch 1300/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0184 - accuracy: 0.9969 - val_loss: 0.0428 - val_accuracy: 0.9814\n",
            "Epoch 1301/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0217 - accuracy: 0.9954 - val_loss: 0.0431 - val_accuracy: 0.9814\n",
            "Epoch 1302/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0178 - accuracy: 0.9939 - val_loss: 0.0482 - val_accuracy: 0.9783\n",
            "Epoch 1303/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0192 - accuracy: 0.9939 - val_loss: 0.0507 - val_accuracy: 0.9783\n",
            "Epoch 1304/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0204 - accuracy: 0.9939 - val_loss: 0.0458 - val_accuracy: 0.9814\n",
            "Epoch 1305/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0178 - accuracy: 0.9969 - val_loss: 0.0421 - val_accuracy: 0.9814\n",
            "Epoch 1306/3500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0201 - accuracy: 0.9954 - val_loss: 0.0425 - val_accuracy: 0.9814\n",
            "Epoch 1307/3500\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0181 - accuracy: 0.9939 - val_loss: 0.0505 - val_accuracy: 0.9783\n",
            "Epoch 1308/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0216 - accuracy: 0.9939 - val_loss: 0.0479 - val_accuracy: 0.9783\n",
            "Epoch 1309/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0200 - accuracy: 0.9954 - val_loss: 0.0412 - val_accuracy: 0.9814\n",
            "Epoch 1310/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0194 - accuracy: 0.9939 - val_loss: 0.0416 - val_accuracy: 0.9814\n",
            "Epoch 1311/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0182 - accuracy: 0.9954 - val_loss: 0.0461 - val_accuracy: 0.9814\n",
            "Epoch 1312/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0184 - accuracy: 0.9939 - val_loss: 0.0467 - val_accuracy: 0.9814\n",
            "Epoch 1313/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.0433 - val_accuracy: 0.9814\n",
            "Epoch 1314/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0178 - accuracy: 0.9954 - val_loss: 0.0414 - val_accuracy: 0.9814\n",
            "Epoch 1315/3500\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0184 - accuracy: 0.9954 - val_loss: 0.0415 - val_accuracy: 0.9814\n",
            "Epoch 1316/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0181 - accuracy: 0.9939 - val_loss: 0.0423 - val_accuracy: 0.9814\n",
            "Epoch 1317/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0176 - accuracy: 0.9969 - val_loss: 0.0442 - val_accuracy: 0.9814\n",
            "Epoch 1318/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0182 - accuracy: 0.9954 - val_loss: 0.0438 - val_accuracy: 0.9814\n",
            "Epoch 1319/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0175 - accuracy: 0.9969 - val_loss: 0.0412 - val_accuracy: 0.9814\n",
            "Epoch 1320/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0184 - accuracy: 0.9954 - val_loss: 0.0413 - val_accuracy: 0.9814\n",
            "Epoch 1321/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0193 - accuracy: 0.9939 - val_loss: 0.0421 - val_accuracy: 0.9814\n",
            "Epoch 1322/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0179 - accuracy: 0.9939 - val_loss: 0.0429 - val_accuracy: 0.9814\n",
            "Epoch 1323/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0174 - accuracy: 0.9969 - val_loss: 0.0463 - val_accuracy: 0.9814\n",
            "Epoch 1324/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0184 - accuracy: 0.9939 - val_loss: 0.0461 - val_accuracy: 0.9814\n",
            "Epoch 1325/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0183 - accuracy: 0.9939 - val_loss: 0.0427 - val_accuracy: 0.9814\n",
            "Epoch 1326/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0177 - accuracy: 0.9954 - val_loss: 0.0426 - val_accuracy: 0.9814\n",
            "Epoch 1327/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0181 - accuracy: 0.9969 - val_loss: 0.0429 - val_accuracy: 0.9814\n",
            "Epoch 1328/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.0426 - val_accuracy: 0.9814\n",
            "Epoch 1329/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0176 - accuracy: 0.9969 - val_loss: 0.0451 - val_accuracy: 0.9814\n",
            "Epoch 1330/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0186 - accuracy: 0.9954 - val_loss: 0.0447 - val_accuracy: 0.9814\n",
            "Epoch 1331/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0169 - accuracy: 0.9969 - val_loss: 0.0423 - val_accuracy: 0.9814\n",
            "Epoch 1332/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0205 - accuracy: 0.9954 - val_loss: 0.0423 - val_accuracy: 0.9814\n",
            "Epoch 1333/3500\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.0194 - accuracy: 0.9954 - val_loss: 0.0471 - val_accuracy: 0.9814\n",
            "Epoch 1334/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.0537 - val_accuracy: 0.9783\n",
            "Epoch 1335/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0222 - accuracy: 0.9908 - val_loss: 0.0438 - val_accuracy: 0.9814\n",
            "Epoch 1336/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0198 - accuracy: 0.9954 - val_loss: 0.0467 - val_accuracy: 0.9876\n",
            "Epoch 1337/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0272 - accuracy: 0.9939 - val_loss: 0.0457 - val_accuracy: 0.9814\n",
            "Epoch 1338/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0177 - accuracy: 0.9954 - val_loss: 0.0598 - val_accuracy: 0.9814\n",
            "Epoch 1339/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0262 - accuracy: 0.9908 - val_loss: 0.0535 - val_accuracy: 0.9783\n",
            "Epoch 1340/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0214 - accuracy: 0.9939 - val_loss: 0.0415 - val_accuracy: 0.9814\n",
            "Epoch 1341/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0187 - accuracy: 0.9923 - val_loss: 0.0415 - val_accuracy: 0.9845\n",
            "Epoch 1342/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0213 - accuracy: 0.9923 - val_loss: 0.0432 - val_accuracy: 0.9814\n",
            "Epoch 1343/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0168 - accuracy: 0.9969 - val_loss: 0.0496 - val_accuracy: 0.9783\n",
            "Epoch 1344/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.0515 - val_accuracy: 0.9783\n",
            "Epoch 1345/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0204 - accuracy: 0.9923 - val_loss: 0.0452 - val_accuracy: 0.9814\n",
            "Epoch 1346/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0182 - accuracy: 0.9939 - val_loss: 0.0413 - val_accuracy: 0.9814\n",
            "Epoch 1347/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0182 - accuracy: 0.9954 - val_loss: 0.0422 - val_accuracy: 0.9814\n",
            "Epoch 1348/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0172 - accuracy: 0.9939 - val_loss: 0.0471 - val_accuracy: 0.9814\n",
            "Epoch 1349/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0186 - accuracy: 0.9923 - val_loss: 0.0495 - val_accuracy: 0.9783\n",
            "Epoch 1350/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0193 - accuracy: 0.9923 - val_loss: 0.0438 - val_accuracy: 0.9814\n",
            "Epoch 1351/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0167 - accuracy: 0.9969 - val_loss: 0.0410 - val_accuracy: 0.9845\n",
            "Epoch 1352/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0198 - accuracy: 0.9954 - val_loss: 0.0411 - val_accuracy: 0.9814\n",
            "Epoch 1353/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0176 - accuracy: 0.9985 - val_loss: 0.0461 - val_accuracy: 0.9783\n",
            "Epoch 1354/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0183 - accuracy: 0.9954 - val_loss: 0.0506 - val_accuracy: 0.9783\n",
            "Epoch 1355/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0209 - accuracy: 0.9954 - val_loss: 0.0473 - val_accuracy: 0.9783\n",
            "Epoch 1356/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0193 - accuracy: 0.9954 - val_loss: 0.0431 - val_accuracy: 0.9814\n",
            "Epoch 1357/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0175 - accuracy: 0.9969 - val_loss: 0.0422 - val_accuracy: 0.9814\n",
            "Epoch 1358/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0175 - accuracy: 0.9969 - val_loss: 0.0424 - val_accuracy: 0.9814\n",
            "Epoch 1359/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0174 - accuracy: 0.9939 - val_loss: 0.0424 - val_accuracy: 0.9814\n",
            "Epoch 1360/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.0428 - val_accuracy: 0.9814\n",
            "Epoch 1361/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0172 - accuracy: 0.9939 - val_loss: 0.0436 - val_accuracy: 0.9814\n",
            "Epoch 1362/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0171 - accuracy: 0.9969 - val_loss: 0.0450 - val_accuracy: 0.9814\n",
            "Epoch 1363/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0172 - accuracy: 0.9969 - val_loss: 0.0456 - val_accuracy: 0.9814\n",
            "Epoch 1364/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0176 - accuracy: 0.9954 - val_loss: 0.0443 - val_accuracy: 0.9814\n",
            "Epoch 1365/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0174 - accuracy: 0.9954 - val_loss: 0.0425 - val_accuracy: 0.9814\n",
            "Epoch 1366/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0174 - accuracy: 0.9954 - val_loss: 0.0443 - val_accuracy: 0.9814\n",
            "Epoch 1367/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0173 - accuracy: 0.9969 - val_loss: 0.0461 - val_accuracy: 0.9814\n",
            "Epoch 1368/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0174 - accuracy: 0.9954 - val_loss: 0.0438 - val_accuracy: 0.9814\n",
            "Epoch 1369/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0171 - accuracy: 0.9969 - val_loss: 0.0433 - val_accuracy: 0.9814\n",
            "Epoch 1370/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0169 - accuracy: 0.9969 - val_loss: 0.0445 - val_accuracy: 0.9814\n",
            "Epoch 1371/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0170 - accuracy: 0.9969 - val_loss: 0.0465 - val_accuracy: 0.9814\n",
            "Epoch 1372/3500\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.0482 - val_accuracy: 0.9783\n",
            "Epoch 1373/3500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0185 - accuracy: 0.9954 - val_loss: 0.0458 - val_accuracy: 0.9814\n",
            "Epoch 1374/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0172 - accuracy: 0.9969 - val_loss: 0.0429 - val_accuracy: 0.9814\n",
            "Epoch 1375/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0169 - accuracy: 0.9969 - val_loss: 0.0419 - val_accuracy: 0.9814\n",
            "Epoch 1376/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0171 - accuracy: 0.9954 - val_loss: 0.0433 - val_accuracy: 0.9814\n",
            "Epoch 1377/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0173 - accuracy: 0.9969 - val_loss: 0.0447 - val_accuracy: 0.9814\n",
            "Epoch 1378/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0171 - accuracy: 0.9954 - val_loss: 0.0423 - val_accuracy: 0.9814\n",
            "Epoch 1379/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0169 - accuracy: 0.9969 - val_loss: 0.0419 - val_accuracy: 0.9814\n",
            "Epoch 1380/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0169 - accuracy: 0.9969 - val_loss: 0.0448 - val_accuracy: 0.9814\n",
            "Epoch 1381/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0171 - accuracy: 0.9939 - val_loss: 0.0488 - val_accuracy: 0.9783\n",
            "Epoch 1382/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.0444 - val_accuracy: 0.9814\n",
            "Epoch 1383/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0176 - accuracy: 0.9954 - val_loss: 0.0415 - val_accuracy: 0.9814\n",
            "Epoch 1384/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0179 - accuracy: 0.9954 - val_loss: 0.0427 - val_accuracy: 0.9814\n",
            "Epoch 1385/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0174 - accuracy: 0.9969 - val_loss: 0.0455 - val_accuracy: 0.9814\n",
            "Epoch 1386/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0171 - accuracy: 0.9954 - val_loss: 0.0447 - val_accuracy: 0.9814\n",
            "Epoch 1387/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0167 - accuracy: 0.9969 - val_loss: 0.0432 - val_accuracy: 0.9814\n",
            "Epoch 1388/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0173 - accuracy: 0.9939 - val_loss: 0.0438 - val_accuracy: 0.9814\n",
            "Epoch 1389/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.0452 - val_accuracy: 0.9814\n",
            "Epoch 1390/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0168 - accuracy: 0.9969 - val_loss: 0.0425 - val_accuracy: 0.9814\n",
            "Epoch 1391/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0172 - accuracy: 0.9939 - val_loss: 0.0419 - val_accuracy: 0.9814\n",
            "Epoch 1392/3500\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.0424 - val_accuracy: 0.9814\n",
            "Epoch 1393/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0169 - accuracy: 0.9969 - val_loss: 0.0448 - val_accuracy: 0.9814\n",
            "Epoch 1394/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0167 - accuracy: 0.9969 - val_loss: 0.0489 - val_accuracy: 0.9783\n",
            "Epoch 1395/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.0451 - val_accuracy: 0.9814\n",
            "Epoch 1396/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.0421 - val_accuracy: 0.9845\n",
            "Epoch 1397/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0211 - accuracy: 0.9939 - val_loss: 0.0414 - val_accuracy: 0.9814\n",
            "Epoch 1398/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0176 - accuracy: 0.9939 - val_loss: 0.0494 - val_accuracy: 0.9783\n",
            "Epoch 1399/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0194 - accuracy: 0.9954 - val_loss: 0.0517 - val_accuracy: 0.9783\n",
            "Epoch 1400/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0208 - accuracy: 0.9954 - val_loss: 0.0449 - val_accuracy: 0.9814\n",
            "Epoch 1401/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0176 - accuracy: 0.9954 - val_loss: 0.0421 - val_accuracy: 0.9814\n",
            "Epoch 1402/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0168 - accuracy: 0.9969 - val_loss: 0.0421 - val_accuracy: 0.9814\n",
            "Epoch 1403/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0169 - accuracy: 0.9939 - val_loss: 0.0426 - val_accuracy: 0.9814\n",
            "Epoch 1404/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0163 - accuracy: 0.9969 - val_loss: 0.0453 - val_accuracy: 0.9814\n",
            "Epoch 1405/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.0488 - val_accuracy: 0.9783\n",
            "Epoch 1406/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0187 - accuracy: 0.9939 - val_loss: 0.0466 - val_accuracy: 0.9814\n",
            "Epoch 1407/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0171 - accuracy: 0.9954 - val_loss: 0.0412 - val_accuracy: 0.9814\n",
            "Epoch 1408/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.0411 - val_accuracy: 0.9814\n",
            "Epoch 1409/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.0451 - val_accuracy: 0.9814\n",
            "Epoch 1410/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.0472 - val_accuracy: 0.9783\n",
            "Epoch 1411/3500\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.0182 - accuracy: 0.9954 - val_loss: 0.0451 - val_accuracy: 0.9814\n",
            "Epoch 1412/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0170 - accuracy: 0.9954 - val_loss: 0.0411 - val_accuracy: 0.9814\n",
            "Epoch 1413/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0165 - accuracy: 0.9969 - val_loss: 0.0402 - val_accuracy: 0.9845\n",
            "Epoch 1414/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0181 - accuracy: 0.9954 - val_loss: 0.0423 - val_accuracy: 0.9814\n",
            "Epoch 1415/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0165 - accuracy: 0.9969 - val_loss: 0.0477 - val_accuracy: 0.9783\n",
            "Epoch 1416/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0181 - accuracy: 0.9954 - val_loss: 0.0435 - val_accuracy: 0.9814\n",
            "Epoch 1417/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0165 - accuracy: 0.9969 - val_loss: 0.0404 - val_accuracy: 0.9814\n",
            "Epoch 1418/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0196 - accuracy: 0.9954 - val_loss: 0.0435 - val_accuracy: 0.9814\n",
            "Epoch 1419/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0183 - accuracy: 0.9954 - val_loss: 0.0542 - val_accuracy: 0.9783\n",
            "Epoch 1420/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0212 - accuracy: 0.9939 - val_loss: 0.0453 - val_accuracy: 0.9814\n",
            "Epoch 1421/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0166 - accuracy: 0.9939 - val_loss: 0.0411 - val_accuracy: 0.9845\n",
            "Epoch 1422/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0204 - accuracy: 0.9939 - val_loss: 0.0407 - val_accuracy: 0.9814\n",
            "Epoch 1423/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.0469 - val_accuracy: 0.9783\n",
            "Epoch 1424/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0178 - accuracy: 0.9954 - val_loss: 0.0554 - val_accuracy: 0.9783\n",
            "Epoch 1425/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0230 - accuracy: 0.9954 - val_loss: 0.0497 - val_accuracy: 0.9783\n",
            "Epoch 1426/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0185 - accuracy: 0.9954 - val_loss: 0.0416 - val_accuracy: 0.9814\n",
            "Epoch 1427/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0167 - accuracy: 0.9969 - val_loss: 0.0407 - val_accuracy: 0.9845\n",
            "Epoch 1428/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0193 - accuracy: 0.9954 - val_loss: 0.0415 - val_accuracy: 0.9814\n",
            "Epoch 1429/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0163 - accuracy: 0.9969 - val_loss: 0.0458 - val_accuracy: 0.9814\n",
            "Epoch 1430/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0170 - accuracy: 0.9954 - val_loss: 0.0438 - val_accuracy: 0.9814\n",
            "Epoch 1431/3500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0169 - accuracy: 0.9969 - val_loss: 0.0411 - val_accuracy: 0.9814\n",
            "Epoch 1432/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.0431 - val_accuracy: 0.9814\n",
            "Epoch 1433/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0164 - accuracy: 0.9969 - val_loss: 0.0449 - val_accuracy: 0.9814\n",
            "Epoch 1434/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0168 - accuracy: 0.9939 - val_loss: 0.0423 - val_accuracy: 0.9814\n",
            "Epoch 1435/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0160 - accuracy: 0.9969 - val_loss: 0.0406 - val_accuracy: 0.9814\n",
            "Epoch 1436/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0164 - accuracy: 0.9939 - val_loss: 0.0410 - val_accuracy: 0.9814\n",
            "Epoch 1437/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0160 - accuracy: 0.9969 - val_loss: 0.0444 - val_accuracy: 0.9814\n",
            "Epoch 1438/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.0434 - val_accuracy: 0.9814\n",
            "Epoch 1439/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0160 - accuracy: 0.9969 - val_loss: 0.0403 - val_accuracy: 0.9814\n",
            "Epoch 1440/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0177 - accuracy: 0.9954 - val_loss: 0.0413 - val_accuracy: 0.9814\n",
            "Epoch 1441/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0179 - accuracy: 0.9954 - val_loss: 0.0459 - val_accuracy: 0.9814\n",
            "Epoch 1442/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0173 - accuracy: 0.9954 - val_loss: 0.0426 - val_accuracy: 0.9814\n",
            "Epoch 1443/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0159 - accuracy: 0.9969 - val_loss: 0.0413 - val_accuracy: 0.9814\n",
            "Epoch 1444/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0161 - accuracy: 0.9969 - val_loss: 0.0411 - val_accuracy: 0.9814\n",
            "Epoch 1445/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.0421 - val_accuracy: 0.9814\n",
            "Epoch 1446/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0157 - accuracy: 0.9969 - val_loss: 0.0471 - val_accuracy: 0.9783\n",
            "Epoch 1447/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.0457 - val_accuracy: 0.9814\n",
            "Epoch 1448/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0164 - accuracy: 0.9954 - val_loss: 0.0407 - val_accuracy: 0.9814\n",
            "Epoch 1449/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0172 - accuracy: 0.9954 - val_loss: 0.0410 - val_accuracy: 0.9845\n",
            "Epoch 1450/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.0443 - val_accuracy: 0.9814\n",
            "Epoch 1451/3500\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.0500 - val_accuracy: 0.9783\n",
            "Epoch 1452/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0191 - accuracy: 0.9954 - val_loss: 0.0453 - val_accuracy: 0.9814\n",
            "Epoch 1453/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.0415 - val_accuracy: 0.9814\n",
            "Epoch 1454/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0164 - accuracy: 0.9954 - val_loss: 0.0408 - val_accuracy: 0.9814\n",
            "Epoch 1455/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.0436 - val_accuracy: 0.9814\n",
            "Epoch 1456/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0160 - accuracy: 0.9969 - val_loss: 0.0485 - val_accuracy: 0.9783\n",
            "Epoch 1457/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0181 - accuracy: 0.9954 - val_loss: 0.0456 - val_accuracy: 0.9783\n",
            "Epoch 1458/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.0406 - val_accuracy: 0.9814\n",
            "Epoch 1459/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.0394 - val_accuracy: 0.9845\n",
            "Epoch 1460/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.0417 - val_accuracy: 0.9814\n",
            "Epoch 1461/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 0.0470 - val_accuracy: 0.9783\n",
            "Epoch 1462/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0176 - accuracy: 0.9954 - val_loss: 0.0438 - val_accuracy: 0.9814\n",
            "Epoch 1463/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0156 - accuracy: 0.9969 - val_loss: 0.0400 - val_accuracy: 0.9845\n",
            "Epoch 1464/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0192 - accuracy: 0.9954 - val_loss: 0.0407 - val_accuracy: 0.9814\n",
            "Epoch 1465/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.0540 - val_accuracy: 0.9783\n",
            "Epoch 1466/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0220 - accuracy: 0.9939 - val_loss: 0.0544 - val_accuracy: 0.9783\n",
            "Epoch 1467/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0204 - accuracy: 0.9954 - val_loss: 0.0407 - val_accuracy: 0.9814\n",
            "Epoch 1468/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0173 - accuracy: 0.9954 - val_loss: 0.0433 - val_accuracy: 0.9845\n",
            "Epoch 1469/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0232 - accuracy: 0.9939 - val_loss: 0.0422 - val_accuracy: 0.9814\n",
            "Epoch 1470/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0170 - accuracy: 0.9969 - val_loss: 0.0498 - val_accuracy: 0.9783\n",
            "Epoch 1471/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0185 - accuracy: 0.9954 - val_loss: 0.0438 - val_accuracy: 0.9814\n",
            "Epoch 1472/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.0406 - val_accuracy: 0.9845\n",
            "Epoch 1473/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0170 - accuracy: 0.9954 - val_loss: 0.0408 - val_accuracy: 0.9814\n",
            "Epoch 1474/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.0453 - val_accuracy: 0.9814\n",
            "Epoch 1475/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0187 - accuracy: 0.9954 - val_loss: 0.0493 - val_accuracy: 0.9783\n",
            "Epoch 1476/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0183 - accuracy: 0.9939 - val_loss: 0.0418 - val_accuracy: 0.9814\n",
            "Epoch 1477/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0161 - accuracy: 0.9939 - val_loss: 0.0411 - val_accuracy: 0.9814\n",
            "Epoch 1478/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0158 - accuracy: 0.9969 - val_loss: 0.0437 - val_accuracy: 0.9814\n",
            "Epoch 1479/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0157 - accuracy: 0.9969 - val_loss: 0.0451 - val_accuracy: 0.9814\n",
            "Epoch 1480/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.0429 - val_accuracy: 0.9814\n",
            "Epoch 1481/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0157 - accuracy: 0.9969 - val_loss: 0.0412 - val_accuracy: 0.9814\n",
            "Epoch 1482/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0156 - accuracy: 0.9969 - val_loss: 0.0406 - val_accuracy: 0.9814\n",
            "Epoch 1483/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.0405 - val_accuracy: 0.9814\n",
            "Epoch 1484/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.0432 - val_accuracy: 0.9814\n",
            "Epoch 1485/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0164 - accuracy: 0.9969 - val_loss: 0.0475 - val_accuracy: 0.9814\n",
            "Epoch 1486/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.0421 - val_accuracy: 0.9814\n",
            "Epoch 1487/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0160 - accuracy: 0.9969 - val_loss: 0.0411 - val_accuracy: 0.9845\n",
            "Epoch 1488/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0172 - accuracy: 0.9954 - val_loss: 0.0428 - val_accuracy: 0.9814\n",
            "Epoch 1489/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0165 - accuracy: 0.9954 - val_loss: 0.0459 - val_accuracy: 0.9814\n",
            "Epoch 1490/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.0425 - val_accuracy: 0.9814\n",
            "Epoch 1491/3500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0152 - accuracy: 0.9969 - val_loss: 0.0413 - val_accuracy: 0.9814\n",
            "Epoch 1492/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0159 - accuracy: 0.9939 - val_loss: 0.0417 - val_accuracy: 0.9814\n",
            "Epoch 1493/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0164 - accuracy: 0.9969 - val_loss: 0.0431 - val_accuracy: 0.9814\n",
            "Epoch 1494/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0155 - accuracy: 0.9969 - val_loss: 0.0416 - val_accuracy: 0.9814\n",
            "Epoch 1495/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0154 - accuracy: 0.9969 - val_loss: 0.0412 - val_accuracy: 0.9814\n",
            "Epoch 1496/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0156 - accuracy: 0.9939 - val_loss: 0.0411 - val_accuracy: 0.9814\n",
            "Epoch 1497/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.0423 - val_accuracy: 0.9814\n",
            "Epoch 1498/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0151 - accuracy: 0.9954 - val_loss: 0.0472 - val_accuracy: 0.9814\n",
            "Epoch 1499/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0168 - accuracy: 0.9954 - val_loss: 0.0459 - val_accuracy: 0.9814\n",
            "Epoch 1500/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.0423 - val_accuracy: 0.9814\n",
            "Epoch 1501/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0153 - accuracy: 0.9969 - val_loss: 0.0431 - val_accuracy: 0.9814\n",
            "Epoch 1502/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0151 - accuracy: 0.9969 - val_loss: 0.0455 - val_accuracy: 0.9814\n",
            "Epoch 1503/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0159 - accuracy: 0.9954 - val_loss: 0.0436 - val_accuracy: 0.9814\n",
            "Epoch 1504/3500\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.0149 - accuracy: 0.9969 - val_loss: 0.0407 - val_accuracy: 0.9845\n",
            "Epoch 1505/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.0407 - val_accuracy: 0.9845\n",
            "Epoch 1506/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0168 - accuracy: 0.9939 - val_loss: 0.0434 - val_accuracy: 0.9814\n",
            "Epoch 1507/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0158 - accuracy: 0.9969 - val_loss: 0.0453 - val_accuracy: 0.9814\n",
            "Epoch 1508/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.0417 - val_accuracy: 0.9814\n",
            "Epoch 1509/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0150 - accuracy: 0.9969 - val_loss: 0.0406 - val_accuracy: 0.9814\n",
            "Epoch 1510/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 0.0407 - val_accuracy: 0.9814\n",
            "Epoch 1511/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.0437 - val_accuracy: 0.9814\n",
            "Epoch 1512/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.0455 - val_accuracy: 0.9814\n",
            "Epoch 1513/3500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.0413 - val_accuracy: 0.9814\n",
            "Epoch 1514/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 0.0407 - val_accuracy: 0.9814\n",
            "Epoch 1515/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0151 - accuracy: 0.9954 - val_loss: 0.0451 - val_accuracy: 0.9814\n",
            "Epoch 1516/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.0468 - val_accuracy: 0.9814\n",
            "Epoch 1517/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.0409 - val_accuracy: 0.9814\n",
            "Epoch 1518/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.0396 - val_accuracy: 0.9845\n",
            "Epoch 1519/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.0399 - val_accuracy: 0.9814\n",
            "Epoch 1520/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.0400 - val_accuracy: 0.9814\n",
            "Epoch 1521/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0149 - accuracy: 0.9969 - val_loss: 0.0413 - val_accuracy: 0.9814\n",
            "Epoch 1522/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0152 - accuracy: 0.9969 - val_loss: 0.0422 - val_accuracy: 0.9814\n",
            "Epoch 1523/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.0409 - val_accuracy: 0.9814\n",
            "Epoch 1524/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.0414 - val_accuracy: 0.9814\n",
            "Epoch 1525/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0152 - accuracy: 0.9969 - val_loss: 0.0433 - val_accuracy: 0.9814\n",
            "Epoch 1526/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0154 - accuracy: 0.9969 - val_loss: 0.0424 - val_accuracy: 0.9814\n",
            "Epoch 1527/3500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0149 - accuracy: 0.9969 - val_loss: 0.0425 - val_accuracy: 0.9814\n",
            "Epoch 1528/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0149 - accuracy: 0.9969 - val_loss: 0.0421 - val_accuracy: 0.9814\n",
            "Epoch 1529/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0151 - accuracy: 0.9969 - val_loss: 0.0439 - val_accuracy: 0.9814\n",
            "Epoch 1530/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0151 - accuracy: 0.9969 - val_loss: 0.0466 - val_accuracy: 0.9814\n",
            "Epoch 1531/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.0436 - val_accuracy: 0.9814\n",
            "Epoch 1532/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0146 - accuracy: 0.9969 - val_loss: 0.0399 - val_accuracy: 0.9845\n",
            "Epoch 1533/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0165 - accuracy: 0.9954 - val_loss: 0.0396 - val_accuracy: 0.9845\n",
            "Epoch 1534/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.0428 - val_accuracy: 0.9814\n",
            "Epoch 1535/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.0475 - val_accuracy: 0.9783\n",
            "Epoch 1536/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0176 - accuracy: 0.9954 - val_loss: 0.0431 - val_accuracy: 0.9814\n",
            "Epoch 1537/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.0394 - val_accuracy: 0.9845\n",
            "Epoch 1538/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.0402 - val_accuracy: 0.9845\n",
            "Epoch 1539/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0170 - accuracy: 0.9954 - val_loss: 0.0419 - val_accuracy: 0.9814\n",
            "Epoch 1540/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.0457 - val_accuracy: 0.9814\n",
            "Epoch 1541/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.0422 - val_accuracy: 0.9814\n",
            "Epoch 1542/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0149 - accuracy: 0.9969 - val_loss: 0.0421 - val_accuracy: 0.9814\n",
            "Epoch 1543/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0145 - accuracy: 0.9969 - val_loss: 0.0448 - val_accuracy: 0.9814\n",
            "Epoch 1544/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0151 - accuracy: 0.9954 - val_loss: 0.0474 - val_accuracy: 0.9814\n",
            "Epoch 1545/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.0465 - val_accuracy: 0.9814\n",
            "Epoch 1546/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.0423 - val_accuracy: 0.9814\n",
            "Epoch 1547/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0141 - accuracy: 0.9969 - val_loss: 0.0411 - val_accuracy: 0.9845\n",
            "Epoch 1548/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0178 - accuracy: 0.9954 - val_loss: 0.0407 - val_accuracy: 0.9814\n",
            "Epoch 1549/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0159 - accuracy: 0.9969 - val_loss: 0.0477 - val_accuracy: 0.9814\n",
            "Epoch 1550/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0164 - accuracy: 0.9954 - val_loss: 0.0469 - val_accuracy: 0.9814\n",
            "Epoch 1551/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.0433 - val_accuracy: 0.9814\n",
            "Epoch 1552/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0145 - accuracy: 0.9969 - val_loss: 0.0428 - val_accuracy: 0.9814\n",
            "Epoch 1553/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0147 - accuracy: 0.9969 - val_loss: 0.0437 - val_accuracy: 0.9814\n",
            "Epoch 1554/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0145 - accuracy: 0.9969 - val_loss: 0.0451 - val_accuracy: 0.9814\n",
            "Epoch 1555/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.0436 - val_accuracy: 0.9814\n",
            "Epoch 1556/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0151 - accuracy: 0.9969 - val_loss: 0.0428 - val_accuracy: 0.9814\n",
            "Epoch 1557/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0158 - accuracy: 0.9969 - val_loss: 0.0443 - val_accuracy: 0.9814\n",
            "Epoch 1558/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0149 - accuracy: 0.9969 - val_loss: 0.0445 - val_accuracy: 0.9814\n",
            "Epoch 1559/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0144 - accuracy: 0.9969 - val_loss: 0.0421 - val_accuracy: 0.9814\n",
            "Epoch 1560/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0160 - accuracy: 0.9969 - val_loss: 0.0420 - val_accuracy: 0.9814\n",
            "Epoch 1561/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0147 - accuracy: 0.9969 - val_loss: 0.0465 - val_accuracy: 0.9814\n",
            "Epoch 1562/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0159 - accuracy: 0.9954 - val_loss: 0.0459 - val_accuracy: 0.9814\n",
            "Epoch 1563/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.0409 - val_accuracy: 0.9814\n",
            "Epoch 1564/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0151 - accuracy: 0.9969 - val_loss: 0.0399 - val_accuracy: 0.9845\n",
            "Epoch 1565/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0159 - accuracy: 0.9954 - val_loss: 0.0414 - val_accuracy: 0.9814\n",
            "Epoch 1566/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0141 - accuracy: 0.9969 - val_loss: 0.0455 - val_accuracy: 0.9814\n",
            "Epoch 1567/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.0444 - val_accuracy: 0.9814\n",
            "Epoch 1568/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.0401 - val_accuracy: 0.9814\n",
            "Epoch 1569/3500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.0399 - val_accuracy: 0.9814\n",
            "Epoch 1570/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0147 - accuracy: 0.9969 - val_loss: 0.0417 - val_accuracy: 0.9814\n",
            "Epoch 1571/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0144 - accuracy: 0.9969 - val_loss: 0.0424 - val_accuracy: 0.9814\n",
            "Epoch 1572/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0144 - accuracy: 0.9969 - val_loss: 0.0412 - val_accuracy: 0.9814\n",
            "Epoch 1573/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0143 - accuracy: 0.9969 - val_loss: 0.0408 - val_accuracy: 0.9814\n",
            "Epoch 1574/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0142 - accuracy: 0.9969 - val_loss: 0.0412 - val_accuracy: 0.9814\n",
            "Epoch 1575/3500\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.0144 - accuracy: 0.9969 - val_loss: 0.0404 - val_accuracy: 0.9814\n",
            "Epoch 1576/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0142 - accuracy: 0.9969 - val_loss: 0.0394 - val_accuracy: 0.9814\n",
            "Epoch 1577/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.0402 - val_accuracy: 0.9814\n",
            "Epoch 1578/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0141 - accuracy: 0.9969 - val_loss: 0.0437 - val_accuracy: 0.9814\n",
            "Epoch 1579/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.0419 - val_accuracy: 0.9814\n",
            "Epoch 1580/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0149 - accuracy: 0.9954 - val_loss: 0.0396 - val_accuracy: 0.9845\n",
            "Epoch 1581/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.0400 - val_accuracy: 0.9814\n",
            "Epoch 1582/3500\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.0144 - accuracy: 0.9969 - val_loss: 0.0424 - val_accuracy: 0.9814\n",
            "Epoch 1583/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0142 - accuracy: 0.9969 - val_loss: 0.0448 - val_accuracy: 0.9814\n",
            "Epoch 1584/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.0430 - val_accuracy: 0.9814\n",
            "Epoch 1585/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0140 - accuracy: 0.9969 - val_loss: 0.0402 - val_accuracy: 0.9845\n",
            "Epoch 1586/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.0409 - val_accuracy: 0.9845\n",
            "Epoch 1587/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.0458 - val_accuracy: 0.9814\n",
            "Epoch 1588/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.0561 - val_accuracy: 0.9783\n",
            "Epoch 1589/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0202 - accuracy: 0.9954 - val_loss: 0.0447 - val_accuracy: 0.9814\n",
            "Epoch 1590/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0133 - accuracy: 0.9969 - val_loss: 0.0428 - val_accuracy: 0.9876\n",
            "Epoch 1591/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0215 - accuracy: 0.9939 - val_loss: 0.0408 - val_accuracy: 0.9814\n",
            "Epoch 1592/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0137 - accuracy: 0.9969 - val_loss: 0.0577 - val_accuracy: 0.9783\n",
            "Epoch 1593/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0219 - accuracy: 0.9954 - val_loss: 0.0607 - val_accuracy: 0.9814\n",
            "Epoch 1594/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0232 - accuracy: 0.9939 - val_loss: 0.0441 - val_accuracy: 0.9814\n",
            "Epoch 1595/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0141 - accuracy: 0.9954 - val_loss: 0.0416 - val_accuracy: 0.9876\n",
            "Epoch 1596/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0223 - accuracy: 0.9954 - val_loss: 0.0386 - val_accuracy: 0.9845\n",
            "Epoch 1597/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0168 - accuracy: 0.9954 - val_loss: 0.0437 - val_accuracy: 0.9814\n",
            "Epoch 1598/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.0468 - val_accuracy: 0.9783\n",
            "Epoch 1599/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0173 - accuracy: 0.9954 - val_loss: 0.0394 - val_accuracy: 0.9814\n",
            "Epoch 1600/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0169 - accuracy: 0.9939 - val_loss: 0.0409 - val_accuracy: 0.9907\n",
            "Epoch 1601/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0206 - accuracy: 0.9939 - val_loss: 0.0443 - val_accuracy: 0.9814\n",
            "Epoch 1602/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0178 - accuracy: 0.9939 - val_loss: 0.0541 - val_accuracy: 0.9783\n",
            "Epoch 1603/3500\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.0190 - accuracy: 0.9954 - val_loss: 0.0411 - val_accuracy: 0.9814\n",
            "Epoch 1604/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0145 - accuracy: 0.9969 - val_loss: 0.0390 - val_accuracy: 0.9845\n",
            "Epoch 1605/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0182 - accuracy: 0.9954 - val_loss: 0.0381 - val_accuracy: 0.9845\n",
            "Epoch 1606/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0139 - accuracy: 0.9969 - val_loss: 0.0474 - val_accuracy: 0.9783\n",
            "Epoch 1607/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.0529 - val_accuracy: 0.9783\n",
            "Epoch 1608/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0201 - accuracy: 0.9954 - val_loss: 0.0408 - val_accuracy: 0.9814\n",
            "Epoch 1609/3500\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0147 - accuracy: 0.9969 - val_loss: 0.0462 - val_accuracy: 0.9845\n",
            "Epoch 1610/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0253 - accuracy: 0.9923 - val_loss: 0.0403 - val_accuracy: 0.9814\n",
            "Epoch 1611/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0139 - accuracy: 0.9954 - val_loss: 0.0580 - val_accuracy: 0.9783\n",
            "Epoch 1612/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0230 - accuracy: 0.9954 - val_loss: 0.0566 - val_accuracy: 0.9783\n",
            "Epoch 1613/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0193 - accuracy: 0.9954 - val_loss: 0.0403 - val_accuracy: 0.9814\n",
            "Epoch 1614/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0166 - accuracy: 0.9939 - val_loss: 0.0410 - val_accuracy: 0.9876\n",
            "Epoch 1615/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.0460 - val_accuracy: 0.9814\n",
            "Epoch 1616/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0201 - accuracy: 0.9954 - val_loss: 0.0522 - val_accuracy: 0.9783\n",
            "Epoch 1617/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.0398 - val_accuracy: 0.9814\n",
            "Epoch 1618/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.0418 - val_accuracy: 0.9876\n",
            "Epoch 1619/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0198 - accuracy: 0.9939 - val_loss: 0.0430 - val_accuracy: 0.9814\n",
            "Epoch 1620/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0178 - accuracy: 0.9954 - val_loss: 0.0569 - val_accuracy: 0.9783\n",
            "Epoch 1621/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0206 - accuracy: 0.9954 - val_loss: 0.0431 - val_accuracy: 0.9814\n",
            "Epoch 1622/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.0405 - val_accuracy: 0.9907\n",
            "Epoch 1623/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0202 - accuracy: 0.9954 - val_loss: 0.0385 - val_accuracy: 0.9845\n",
            "Epoch 1624/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0154 - accuracy: 0.9939 - val_loss: 0.0436 - val_accuracy: 0.9814\n",
            "Epoch 1625/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0154 - accuracy: 0.9954 - val_loss: 0.0502 - val_accuracy: 0.9783\n",
            "Epoch 1626/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.0454 - val_accuracy: 0.9814\n",
            "Epoch 1627/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.0404 - val_accuracy: 0.9814\n",
            "Epoch 1628/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.0394 - val_accuracy: 0.9845\n",
            "Epoch 1629/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.0407 - val_accuracy: 0.9814\n",
            "Epoch 1630/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 0.0509 - val_accuracy: 0.9783\n",
            "Epoch 1631/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.0532 - val_accuracy: 0.9783\n",
            "Epoch 1632/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0187 - accuracy: 0.9954 - val_loss: 0.0439 - val_accuracy: 0.9814\n",
            "Epoch 1633/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0145 - accuracy: 0.9954 - val_loss: 0.0396 - val_accuracy: 0.9814\n",
            "Epoch 1634/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0141 - accuracy: 0.9954 - val_loss: 0.0394 - val_accuracy: 0.9814\n",
            "Epoch 1635/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.0416 - val_accuracy: 0.9814\n",
            "Epoch 1636/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0137 - accuracy: 0.9969 - val_loss: 0.0457 - val_accuracy: 0.9814\n",
            "Epoch 1637/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.0436 - val_accuracy: 0.9814\n",
            "Epoch 1638/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.0397 - val_accuracy: 0.9845\n",
            "Epoch 1639/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.0404 - val_accuracy: 0.9814\n",
            "Epoch 1640/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0131 - accuracy: 0.9969 - val_loss: 0.0480 - val_accuracy: 0.9814\n",
            "Epoch 1641/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.0468 - val_accuracy: 0.9814\n",
            "Epoch 1642/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0137 - accuracy: 0.9969 - val_loss: 0.0404 - val_accuracy: 0.9845\n",
            "Epoch 1643/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.0401 - val_accuracy: 0.9814\n",
            "Epoch 1644/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.0528 - val_accuracy: 0.9783\n",
            "Epoch 1645/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.0526 - val_accuracy: 0.9783\n",
            "Epoch 1646/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0187 - accuracy: 0.9939 - val_loss: 0.0423 - val_accuracy: 0.9814\n",
            "Epoch 1647/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.0386 - val_accuracy: 0.9845\n",
            "Epoch 1648/3500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0148 - accuracy: 0.9954 - val_loss: 0.0391 - val_accuracy: 0.9814\n",
            "Epoch 1649/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0132 - accuracy: 0.9985 - val_loss: 0.0445 - val_accuracy: 0.9814\n",
            "Epoch 1650/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.0481 - val_accuracy: 0.9783\n",
            "Epoch 1651/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.0424 - val_accuracy: 0.9814\n",
            "Epoch 1652/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.0389 - val_accuracy: 0.9845\n",
            "Epoch 1653/3500\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.0159 - accuracy: 0.9954 - val_loss: 0.0389 - val_accuracy: 0.9845\n",
            "Epoch 1654/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0141 - accuracy: 0.9939 - val_loss: 0.0456 - val_accuracy: 0.9814\n",
            "Epoch 1655/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.0445 - val_accuracy: 0.9814\n",
            "Epoch 1656/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0141 - accuracy: 0.9954 - val_loss: 0.0388 - val_accuracy: 0.9845\n",
            "Epoch 1657/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.0387 - val_accuracy: 0.9845\n",
            "Epoch 1658/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0143 - accuracy: 0.9969 - val_loss: 0.0426 - val_accuracy: 0.9814\n",
            "Epoch 1659/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0144 - accuracy: 0.9954 - val_loss: 0.0428 - val_accuracy: 0.9814\n",
            "Epoch 1660/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0144 - accuracy: 0.9954 - val_loss: 0.0395 - val_accuracy: 0.9814\n",
            "Epoch 1661/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0136 - accuracy: 0.9969 - val_loss: 0.0401 - val_accuracy: 0.9814\n",
            "Epoch 1662/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0133 - accuracy: 0.9969 - val_loss: 0.0428 - val_accuracy: 0.9814\n",
            "Epoch 1663/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0139 - accuracy: 0.9954 - val_loss: 0.0430 - val_accuracy: 0.9814\n",
            "Epoch 1664/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0139 - accuracy: 0.9954 - val_loss: 0.0407 - val_accuracy: 0.9814\n",
            "Epoch 1665/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0131 - accuracy: 0.9969 - val_loss: 0.0389 - val_accuracy: 0.9845\n",
            "Epoch 1666/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.0393 - val_accuracy: 0.9845\n",
            "Epoch 1667/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0144 - accuracy: 0.9969 - val_loss: 0.0463 - val_accuracy: 0.9814\n",
            "Epoch 1668/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0148 - accuracy: 0.9954 - val_loss: 0.0447 - val_accuracy: 0.9814\n",
            "Epoch 1669/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0142 - accuracy: 0.9954 - val_loss: 0.0411 - val_accuracy: 0.9814\n",
            "Epoch 1670/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0139 - accuracy: 0.9985 - val_loss: 0.0421 - val_accuracy: 0.9814\n",
            "Epoch 1671/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0449 - val_accuracy: 0.9814\n",
            "Epoch 1672/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0138 - accuracy: 0.9969 - val_loss: 0.0421 - val_accuracy: 0.9814\n",
            "Epoch 1673/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0137 - accuracy: 0.9969 - val_loss: 0.0406 - val_accuracy: 0.9814\n",
            "Epoch 1674/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0135 - accuracy: 0.9985 - val_loss: 0.0438 - val_accuracy: 0.9814\n",
            "Epoch 1675/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0148 - accuracy: 0.9954 - val_loss: 0.0469 - val_accuracy: 0.9814\n",
            "Epoch 1676/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.0401 - val_accuracy: 0.9814\n",
            "Epoch 1677/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.0401 - val_accuracy: 0.9845\n",
            "Epoch 1678/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.0409 - val_accuracy: 0.9814\n",
            "Epoch 1679/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0130 - accuracy: 0.9969 - val_loss: 0.0493 - val_accuracy: 0.9783\n",
            "Epoch 1680/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0165 - accuracy: 0.9954 - val_loss: 0.0462 - val_accuracy: 0.9814\n",
            "Epoch 1681/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0141 - accuracy: 0.9954 - val_loss: 0.0396 - val_accuracy: 0.9845\n",
            "Epoch 1682/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.0396 - val_accuracy: 0.9845\n",
            "Epoch 1683/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.0452 - val_accuracy: 0.9814\n",
            "Epoch 1684/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0144 - accuracy: 0.9954 - val_loss: 0.0405 - val_accuracy: 0.9814\n",
            "Epoch 1685/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.0388 - val_accuracy: 0.9845\n",
            "Epoch 1686/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0142 - accuracy: 0.9969 - val_loss: 0.0397 - val_accuracy: 0.9814\n",
            "Epoch 1687/3500\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.0131 - accuracy: 0.9969 - val_loss: 0.0432 - val_accuracy: 0.9814\n",
            "Epoch 1688/3500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.0139 - accuracy: 0.9954 - val_loss: 0.0432 - val_accuracy: 0.9814\n",
            "Epoch 1689/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.0393 - val_accuracy: 0.9845\n",
            "Epoch 1690/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0140 - accuracy: 0.9985 - val_loss: 0.0406 - val_accuracy: 0.9845\n",
            "Epoch 1691/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.0418 - val_accuracy: 0.9814\n",
            "Epoch 1692/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0132 - accuracy: 0.9969 - val_loss: 0.0546 - val_accuracy: 0.9783\n",
            "Epoch 1693/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0193 - accuracy: 0.9954 - val_loss: 0.0499 - val_accuracy: 0.9783\n",
            "Epoch 1694/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0165 - accuracy: 0.9954 - val_loss: 0.0397 - val_accuracy: 0.9845\n",
            "Epoch 1695/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0141 - accuracy: 0.9969 - val_loss: 0.0397 - val_accuracy: 0.9845\n",
            "Epoch 1696/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0148 - accuracy: 0.9954 - val_loss: 0.0421 - val_accuracy: 0.9814\n",
            "Epoch 1697/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0129 - accuracy: 0.9969 - val_loss: 0.0486 - val_accuracy: 0.9783\n",
            "Epoch 1698/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0159 - accuracy: 0.9954 - val_loss: 0.0480 - val_accuracy: 0.9783\n",
            "Epoch 1699/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.0408 - val_accuracy: 0.9814\n",
            "Epoch 1700/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0127 - accuracy: 0.9969 - val_loss: 0.0391 - val_accuracy: 0.9845\n",
            "Epoch 1701/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.0388 - val_accuracy: 0.9845\n",
            "Epoch 1702/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.0400 - val_accuracy: 0.9814\n",
            "Epoch 1703/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0130 - accuracy: 0.9969 - val_loss: 0.0422 - val_accuracy: 0.9814\n",
            "Epoch 1704/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0131 - accuracy: 0.9969 - val_loss: 0.0452 - val_accuracy: 0.9814\n",
            "Epoch 1705/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0142 - accuracy: 0.9954 - val_loss: 0.0424 - val_accuracy: 0.9814\n",
            "Epoch 1706/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0127 - accuracy: 0.9969 - val_loss: 0.0399 - val_accuracy: 0.9845\n",
            "Epoch 1707/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 0.0403 - val_accuracy: 0.9814\n",
            "Epoch 1708/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0126 - accuracy: 0.9969 - val_loss: 0.0496 - val_accuracy: 0.9783\n",
            "Epoch 1709/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.0510 - val_accuracy: 0.9783\n",
            "Epoch 1710/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.0396 - val_accuracy: 0.9814\n",
            "Epoch 1711/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.0418 - val_accuracy: 0.9845\n",
            "Epoch 1712/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0171 - accuracy: 0.9954 - val_loss: 0.0455 - val_accuracy: 0.9814\n",
            "Epoch 1713/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.0623 - val_accuracy: 0.9783\n",
            "Epoch 1714/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0222 - accuracy: 0.9939 - val_loss: 0.0483 - val_accuracy: 0.9814\n",
            "Epoch 1715/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.0417 - val_accuracy: 0.9814\n",
            "Epoch 1716/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0177 - accuracy: 0.9954 - val_loss: 0.0404 - val_accuracy: 0.9845\n",
            "Epoch 1717/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0139 - accuracy: 0.9969 - val_loss: 0.0469 - val_accuracy: 0.9814\n",
            "Epoch 1718/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0159 - accuracy: 0.9954 - val_loss: 0.0517 - val_accuracy: 0.9783\n",
            "Epoch 1719/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.0409 - val_accuracy: 0.9814\n",
            "Epoch 1720/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0122 - accuracy: 0.9969 - val_loss: 0.0408 - val_accuracy: 0.9907\n",
            "Epoch 1721/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0196 - accuracy: 0.9954 - val_loss: 0.0381 - val_accuracy: 0.9845\n",
            "Epoch 1722/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0146 - accuracy: 0.9939 - val_loss: 0.0455 - val_accuracy: 0.9783\n",
            "Epoch 1723/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.0449 - val_accuracy: 0.9783\n",
            "Epoch 1724/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0145 - accuracy: 0.9954 - val_loss: 0.0383 - val_accuracy: 0.9814\n",
            "Epoch 1725/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0129 - accuracy: 0.9954 - val_loss: 0.0373 - val_accuracy: 0.9845\n",
            "Epoch 1726/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.0373 - val_accuracy: 0.9845\n",
            "Epoch 1727/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0138 - accuracy: 0.9954 - val_loss: 0.0387 - val_accuracy: 0.9814\n",
            "Epoch 1728/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0127 - accuracy: 0.9985 - val_loss: 0.0434 - val_accuracy: 0.9814\n",
            "Epoch 1729/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.0464 - val_accuracy: 0.9783\n",
            "Epoch 1730/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0149 - accuracy: 0.9954 - val_loss: 0.0427 - val_accuracy: 0.9814\n",
            "Epoch 1731/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.0383 - val_accuracy: 0.9814\n",
            "Epoch 1732/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0131 - accuracy: 0.9954 - val_loss: 0.0377 - val_accuracy: 0.9845\n",
            "Epoch 1733/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0399 - val_accuracy: 0.9814\n",
            "Epoch 1734/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0132 - accuracy: 0.9969 - val_loss: 0.0445 - val_accuracy: 0.9814\n",
            "Epoch 1735/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0141 - accuracy: 0.9954 - val_loss: 0.0404 - val_accuracy: 0.9814\n",
            "Epoch 1736/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0125 - accuracy: 0.9969 - val_loss: 0.0384 - val_accuracy: 0.9845\n",
            "Epoch 1737/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.0386 - val_accuracy: 0.9845\n",
            "Epoch 1738/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.0402 - val_accuracy: 0.9814\n",
            "Epoch 1739/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0124 - accuracy: 0.9969 - val_loss: 0.0465 - val_accuracy: 0.9814\n",
            "Epoch 1740/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.0462 - val_accuracy: 0.9814\n",
            "Epoch 1741/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.0397 - val_accuracy: 0.9814\n",
            "Epoch 1742/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0137 - accuracy: 0.9969 - val_loss: 0.0396 - val_accuracy: 0.9845\n",
            "Epoch 1743/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.0418 - val_accuracy: 0.9814\n",
            "Epoch 1744/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0126 - accuracy: 0.9969 - val_loss: 0.0459 - val_accuracy: 0.9814\n",
            "Epoch 1745/3500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.0445 - val_accuracy: 0.9814\n",
            "Epoch 1746/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0398 - val_accuracy: 0.9814\n",
            "Epoch 1747/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0132 - accuracy: 0.9969 - val_loss: 0.0402 - val_accuracy: 0.9845\n",
            "Epoch 1748/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.0398 - val_accuracy: 0.9814\n",
            "Epoch 1749/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0130 - accuracy: 0.9969 - val_loss: 0.0470 - val_accuracy: 0.9814\n",
            "Epoch 1750/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0141 - accuracy: 0.9954 - val_loss: 0.0515 - val_accuracy: 0.9783\n",
            "Epoch 1751/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 0.0431 - val_accuracy: 0.9814\n",
            "Epoch 1752/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0129 - accuracy: 0.9969 - val_loss: 0.0397 - val_accuracy: 0.9845\n",
            "Epoch 1753/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.0388 - val_accuracy: 0.9845\n",
            "Epoch 1754/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.0426 - val_accuracy: 0.9814\n",
            "Epoch 1755/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.0427 - val_accuracy: 0.9814\n",
            "Epoch 1756/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 0.0392 - val_accuracy: 0.9814\n",
            "Epoch 1757/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 0.0393 - val_accuracy: 0.9814\n",
            "Epoch 1758/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0122 - accuracy: 0.9969 - val_loss: 0.0442 - val_accuracy: 0.9814\n",
            "Epoch 1759/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.0447 - val_accuracy: 0.9814\n",
            "Epoch 1760/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0142 - accuracy: 0.9954 - val_loss: 0.0390 - val_accuracy: 0.9814\n",
            "Epoch 1761/3500\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.0125 - accuracy: 0.9969 - val_loss: 0.0380 - val_accuracy: 0.9845\n",
            "Epoch 1762/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.0392 - val_accuracy: 0.9814\n",
            "Epoch 1763/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0423 - val_accuracy: 0.9814\n",
            "Epoch 1764/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0128 - accuracy: 0.9954 - val_loss: 0.0393 - val_accuracy: 0.9814\n",
            "Epoch 1765/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0124 - accuracy: 0.9985 - val_loss: 0.0383 - val_accuracy: 0.9845\n",
            "Epoch 1766/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.0392 - val_accuracy: 0.9814\n",
            "Epoch 1767/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0132 - accuracy: 0.9969 - val_loss: 0.0411 - val_accuracy: 0.9814\n",
            "Epoch 1768/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0127 - accuracy: 0.9969 - val_loss: 0.0398 - val_accuracy: 0.9814\n",
            "Epoch 1769/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0125 - accuracy: 0.9969 - val_loss: 0.0393 - val_accuracy: 0.9814\n",
            "Epoch 1770/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0127 - accuracy: 0.9985 - val_loss: 0.0381 - val_accuracy: 0.9845\n",
            "Epoch 1771/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0131 - accuracy: 0.9954 - val_loss: 0.0380 - val_accuracy: 0.9845\n",
            "Epoch 1772/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0145 - accuracy: 0.9954 - val_loss: 0.0390 - val_accuracy: 0.9814\n",
            "Epoch 1773/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0129 - accuracy: 0.9969 - val_loss: 0.0421 - val_accuracy: 0.9814\n",
            "Epoch 1774/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0131 - accuracy: 0.9969 - val_loss: 0.0406 - val_accuracy: 0.9814\n",
            "Epoch 1775/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0125 - accuracy: 0.9969 - val_loss: 0.0397 - val_accuracy: 0.9814\n",
            "Epoch 1776/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.0382 - val_accuracy: 0.9814\n",
            "Epoch 1777/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0126 - accuracy: 0.9985 - val_loss: 0.0381 - val_accuracy: 0.9845\n",
            "Epoch 1778/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0128 - accuracy: 0.9985 - val_loss: 0.0392 - val_accuracy: 0.9814\n",
            "Epoch 1779/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.0398 - val_accuracy: 0.9814\n",
            "Epoch 1780/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0122 - accuracy: 0.9969 - val_loss: 0.0396 - val_accuracy: 0.9814\n",
            "Epoch 1781/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.0391 - val_accuracy: 0.9814\n",
            "Epoch 1782/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0124 - accuracy: 0.9985 - val_loss: 0.0389 - val_accuracy: 0.9814\n",
            "Epoch 1783/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0127 - accuracy: 0.9985 - val_loss: 0.0399 - val_accuracy: 0.9814\n",
            "Epoch 1784/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0124 - accuracy: 0.9969 - val_loss: 0.0396 - val_accuracy: 0.9814\n",
            "Epoch 1785/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.0428 - val_accuracy: 0.9814\n",
            "Epoch 1786/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.0428 - val_accuracy: 0.9814\n",
            "Epoch 1787/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0121 - accuracy: 0.9969 - val_loss: 0.0391 - val_accuracy: 0.9845\n",
            "Epoch 1788/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0145 - accuracy: 0.9954 - val_loss: 0.0391 - val_accuracy: 0.9845\n",
            "Epoch 1789/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0125 - accuracy: 0.9969 - val_loss: 0.0457 - val_accuracy: 0.9814\n",
            "Epoch 1790/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.0512 - val_accuracy: 0.9783\n",
            "Epoch 1791/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.0404 - val_accuracy: 0.9814\n",
            "Epoch 1792/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0133 - accuracy: 0.9969 - val_loss: 0.0406 - val_accuracy: 0.9845\n",
            "Epoch 1793/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0178 - accuracy: 0.9939 - val_loss: 0.0397 - val_accuracy: 0.9814\n",
            "Epoch 1794/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.0506 - val_accuracy: 0.9783\n",
            "Epoch 1795/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0172 - accuracy: 0.9954 - val_loss: 0.0481 - val_accuracy: 0.9783\n",
            "Epoch 1796/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0149 - accuracy: 0.9954 - val_loss: 0.0374 - val_accuracy: 0.9845\n",
            "Epoch 1797/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.0373 - val_accuracy: 0.9845\n",
            "Epoch 1798/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0125 - accuracy: 0.9985 - val_loss: 0.0465 - val_accuracy: 0.9783\n",
            "Epoch 1799/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0151 - accuracy: 0.9954 - val_loss: 0.0481 - val_accuracy: 0.9783\n",
            "Epoch 1800/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.0387 - val_accuracy: 0.9814\n",
            "Epoch 1801/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0124 - accuracy: 0.9969 - val_loss: 0.0386 - val_accuracy: 0.9876\n",
            "Epoch 1802/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.0372 - val_accuracy: 0.9845\n",
            "Epoch 1803/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0115 - accuracy: 0.9985 - val_loss: 0.0507 - val_accuracy: 0.9783\n",
            "Epoch 1804/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0202 - accuracy: 0.9954 - val_loss: 0.0474 - val_accuracy: 0.9783\n",
            "Epoch 1805/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.0395 - val_accuracy: 0.9907\n",
            "Epoch 1806/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0200 - accuracy: 0.9939 - val_loss: 0.0380 - val_accuracy: 0.9845\n",
            "Epoch 1807/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 0.0469 - val_accuracy: 0.9783\n",
            "Epoch 1808/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.0533 - val_accuracy: 0.9783\n",
            "Epoch 1809/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0184 - accuracy: 0.9954 - val_loss: 0.0392 - val_accuracy: 0.9814\n",
            "Epoch 1810/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0389 - val_accuracy: 0.9845\n",
            "Epoch 1811/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0179 - accuracy: 0.9954 - val_loss: 0.0367 - val_accuracy: 0.9845\n",
            "Epoch 1812/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0139 - accuracy: 0.9954 - val_loss: 0.0439 - val_accuracy: 0.9783\n",
            "Epoch 1813/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0139 - accuracy: 0.9954 - val_loss: 0.0454 - val_accuracy: 0.9783\n",
            "Epoch 1814/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.0428 - val_accuracy: 0.9814\n",
            "Epoch 1815/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0132 - accuracy: 0.9969 - val_loss: 0.0405 - val_accuracy: 0.9814\n",
            "Epoch 1816/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.0420 - val_accuracy: 0.9814\n",
            "Epoch 1817/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0125 - accuracy: 0.9969 - val_loss: 0.0447 - val_accuracy: 0.9814\n",
            "Epoch 1818/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0131 - accuracy: 0.9954 - val_loss: 0.0410 - val_accuracy: 0.9814\n",
            "Epoch 1819/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0130 - accuracy: 0.9969 - val_loss: 0.0393 - val_accuracy: 0.9814\n",
            "Epoch 1820/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0130 - accuracy: 0.9954 - val_loss: 0.0417 - val_accuracy: 0.9814\n",
            "Epoch 1821/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0121 - accuracy: 0.9985 - val_loss: 0.0446 - val_accuracy: 0.9814\n",
            "Epoch 1822/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.0400 - val_accuracy: 0.9814\n",
            "Epoch 1823/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0141 - accuracy: 0.9985 - val_loss: 0.0389 - val_accuracy: 0.9845\n",
            "Epoch 1824/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.0415 - val_accuracy: 0.9814\n",
            "Epoch 1825/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0129 - accuracy: 0.9969 - val_loss: 0.0423 - val_accuracy: 0.9814\n",
            "Epoch 1826/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0130 - accuracy: 0.9954 - val_loss: 0.0387 - val_accuracy: 0.9814\n",
            "Epoch 1827/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0124 - accuracy: 0.9985 - val_loss: 0.0382 - val_accuracy: 0.9814\n",
            "Epoch 1828/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0120 - accuracy: 0.9985 - val_loss: 0.0400 - val_accuracy: 0.9814\n",
            "Epoch 1829/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.0404 - val_accuracy: 0.9814\n",
            "Epoch 1830/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0125 - accuracy: 0.9969 - val_loss: 0.0390 - val_accuracy: 0.9814\n",
            "Epoch 1831/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0119 - accuracy: 0.9985 - val_loss: 0.0395 - val_accuracy: 0.9814\n",
            "Epoch 1832/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0119 - accuracy: 0.9969 - val_loss: 0.0404 - val_accuracy: 0.9814\n",
            "Epoch 1833/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0121 - accuracy: 0.9969 - val_loss: 0.0402 - val_accuracy: 0.9814\n",
            "Epoch 1834/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0120 - accuracy: 0.9969 - val_loss: 0.0380 - val_accuracy: 0.9814\n",
            "Epoch 1835/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0119 - accuracy: 0.9985 - val_loss: 0.0369 - val_accuracy: 0.9845\n",
            "Epoch 1836/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0131 - accuracy: 0.9954 - val_loss: 0.0378 - val_accuracy: 0.9814\n",
            "Epoch 1837/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0121 - accuracy: 0.9985 - val_loss: 0.0419 - val_accuracy: 0.9814\n",
            "Epoch 1838/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.0414 - val_accuracy: 0.9814\n",
            "Epoch 1839/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.0396 - val_accuracy: 0.9814\n",
            "Epoch 1840/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0119 - accuracy: 0.9969 - val_loss: 0.0380 - val_accuracy: 0.9814\n",
            "Epoch 1841/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0120 - accuracy: 0.9985 - val_loss: 0.0378 - val_accuracy: 0.9845\n",
            "Epoch 1842/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0120 - accuracy: 0.9985 - val_loss: 0.0400 - val_accuracy: 0.9814\n",
            "Epoch 1843/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0117 - accuracy: 0.9969 - val_loss: 0.0434 - val_accuracy: 0.9814\n",
            "Epoch 1844/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0128 - accuracy: 0.9954 - val_loss: 0.0436 - val_accuracy: 0.9814\n",
            "Epoch 1845/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0129 - accuracy: 0.9954 - val_loss: 0.0403 - val_accuracy: 0.9814\n",
            "Epoch 1846/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0119 - accuracy: 0.9969 - val_loss: 0.0387 - val_accuracy: 0.9814\n",
            "Epoch 1847/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0121 - accuracy: 0.9985 - val_loss: 0.0381 - val_accuracy: 0.9845\n",
            "Epoch 1848/3500\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.0123 - accuracy: 0.9985 - val_loss: 0.0381 - val_accuracy: 0.9845\n",
            "Epoch 1849/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0136 - accuracy: 0.9969 - val_loss: 0.0385 - val_accuracy: 0.9814\n",
            "Epoch 1850/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0122 - accuracy: 0.9985 - val_loss: 0.0394 - val_accuracy: 0.9814\n",
            "Epoch 1851/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0116 - accuracy: 0.9985 - val_loss: 0.0437 - val_accuracy: 0.9814\n",
            "Epoch 1852/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0125 - accuracy: 0.9954 - val_loss: 0.0459 - val_accuracy: 0.9814\n",
            "Epoch 1853/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 0.0413 - val_accuracy: 0.9814\n",
            "Epoch 1854/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0121 - accuracy: 0.9969 - val_loss: 0.0381 - val_accuracy: 0.9845\n",
            "Epoch 1855/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0121 - accuracy: 0.9985 - val_loss: 0.0379 - val_accuracy: 0.9845\n",
            "Epoch 1856/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0122 - accuracy: 0.9985 - val_loss: 0.0384 - val_accuracy: 0.9814\n",
            "Epoch 1857/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0130 - accuracy: 0.9969 - val_loss: 0.0383 - val_accuracy: 0.9814\n",
            "Epoch 1858/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0126 - accuracy: 0.9985 - val_loss: 0.0381 - val_accuracy: 0.9845\n",
            "Epoch 1859/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0391 - val_accuracy: 0.9814\n",
            "Epoch 1860/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0118 - accuracy: 0.9985 - val_loss: 0.0391 - val_accuracy: 0.9814\n",
            "Epoch 1861/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0124 - accuracy: 0.9985 - val_loss: 0.0399 - val_accuracy: 0.9814\n",
            "Epoch 1862/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0120 - accuracy: 0.9985 - val_loss: 0.0407 - val_accuracy: 0.9814\n",
            "Epoch 1863/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0117 - accuracy: 0.9985 - val_loss: 0.0437 - val_accuracy: 0.9814\n",
            "Epoch 1864/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0128 - accuracy: 0.9954 - val_loss: 0.0427 - val_accuracy: 0.9814\n",
            "Epoch 1865/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.0389 - val_accuracy: 0.9814\n",
            "Epoch 1866/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.0400 - val_accuracy: 0.9814\n",
            "Epoch 1867/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.0447 - val_accuracy: 0.9814\n",
            "Epoch 1868/3500\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0131 - accuracy: 0.9954 - val_loss: 0.0452 - val_accuracy: 0.9814\n",
            "Epoch 1869/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0403 - val_accuracy: 0.9814\n",
            "Epoch 1870/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0124 - accuracy: 0.9969 - val_loss: 0.0378 - val_accuracy: 0.9845\n",
            "Epoch 1871/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0126 - accuracy: 0.9969 - val_loss: 0.0387 - val_accuracy: 0.9814\n",
            "Epoch 1872/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0116 - accuracy: 0.9985 - val_loss: 0.0427 - val_accuracy: 0.9814\n",
            "Epoch 1873/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0122 - accuracy: 0.9969 - val_loss: 0.0456 - val_accuracy: 0.9814\n",
            "Epoch 1874/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0129 - accuracy: 0.9954 - val_loss: 0.0424 - val_accuracy: 0.9814\n",
            "Epoch 1875/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.0413 - val_accuracy: 0.9814\n",
            "Epoch 1876/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0116 - accuracy: 0.9985 - val_loss: 0.0423 - val_accuracy: 0.9814\n",
            "Epoch 1877/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0117 - accuracy: 0.9969 - val_loss: 0.0430 - val_accuracy: 0.9814\n",
            "Epoch 1878/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0117 - accuracy: 0.9969 - val_loss: 0.0407 - val_accuracy: 0.9814\n",
            "Epoch 1879/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0115 - accuracy: 0.9985 - val_loss: 0.0395 - val_accuracy: 0.9845\n",
            "Epoch 1880/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0128 - accuracy: 0.9969 - val_loss: 0.0409 - val_accuracy: 0.9814\n",
            "Epoch 1881/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0115 - accuracy: 0.9969 - val_loss: 0.0480 - val_accuracy: 0.9814\n",
            "Epoch 1882/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0141 - accuracy: 0.9954 - val_loss: 0.0459 - val_accuracy: 0.9814\n",
            "Epoch 1883/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.0392 - val_accuracy: 0.9814\n",
            "Epoch 1884/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0118 - accuracy: 0.9985 - val_loss: 0.0384 - val_accuracy: 0.9845\n",
            "Epoch 1885/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0121 - accuracy: 0.9985 - val_loss: 0.0401 - val_accuracy: 0.9814\n",
            "Epoch 1886/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0115 - accuracy: 0.9969 - val_loss: 0.0444 - val_accuracy: 0.9814\n",
            "Epoch 1887/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0131 - accuracy: 0.9954 - val_loss: 0.0435 - val_accuracy: 0.9814\n",
            "Epoch 1888/3500\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.0394 - val_accuracy: 0.9814\n",
            "Epoch 1889/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0111 - accuracy: 0.9969 - val_loss: 0.0376 - val_accuracy: 0.9845\n",
            "Epoch 1890/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0134 - accuracy: 0.9969 - val_loss: 0.0378 - val_accuracy: 0.9845\n",
            "Epoch 1891/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0125 - accuracy: 0.9969 - val_loss: 0.0421 - val_accuracy: 0.9814\n",
            "Epoch 1892/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.0486 - val_accuracy: 0.9783\n",
            "Epoch 1893/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0148 - accuracy: 0.9954 - val_loss: 0.0456 - val_accuracy: 0.9814\n",
            "Epoch 1894/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.0393 - val_accuracy: 0.9814\n",
            "Epoch 1895/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0111 - accuracy: 0.9969 - val_loss: 0.0377 - val_accuracy: 0.9845\n",
            "Epoch 1896/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.0387 - val_accuracy: 0.9814\n",
            "Epoch 1897/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0125 - accuracy: 0.9969 - val_loss: 0.0472 - val_accuracy: 0.9783\n",
            "Epoch 1898/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.0414 - val_accuracy: 0.9814\n",
            "Epoch 1899/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0128 - accuracy: 0.9954 - val_loss: 0.0390 - val_accuracy: 0.9845\n",
            "Epoch 1900/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0144 - accuracy: 0.9954 - val_loss: 0.0388 - val_accuracy: 0.9845\n",
            "Epoch 1901/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0117 - accuracy: 0.9985 - val_loss: 0.0434 - val_accuracy: 0.9814\n",
            "Epoch 1902/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0124 - accuracy: 0.9954 - val_loss: 0.0473 - val_accuracy: 0.9814\n",
            "Epoch 1903/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.0418 - val_accuracy: 0.9814\n",
            "Epoch 1904/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.0403 - val_accuracy: 0.9814\n",
            "Epoch 1905/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.0426 - val_accuracy: 0.9814\n",
            "Epoch 1906/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.0436 - val_accuracy: 0.9814\n",
            "Epoch 1907/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0124 - accuracy: 0.9954 - val_loss: 0.0428 - val_accuracy: 0.9814\n",
            "Epoch 1908/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0123 - accuracy: 0.9954 - val_loss: 0.0384 - val_accuracy: 0.9814\n",
            "Epoch 1909/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0134 - accuracy: 0.9969 - val_loss: 0.0383 - val_accuracy: 0.9845\n",
            "Epoch 1910/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0123 - accuracy: 0.9985 - val_loss: 0.0433 - val_accuracy: 0.9814\n",
            "Epoch 1911/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0127 - accuracy: 0.9954 - val_loss: 0.0422 - val_accuracy: 0.9814\n",
            "Epoch 1912/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0116 - accuracy: 0.9969 - val_loss: 0.0395 - val_accuracy: 0.9845\n",
            "Epoch 1913/3500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0129 - accuracy: 0.9969 - val_loss: 0.0401 - val_accuracy: 0.9814\n",
            "Epoch 1914/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.0447 - val_accuracy: 0.9814\n",
            "Epoch 1915/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0120 - accuracy: 0.9954 - val_loss: 0.0486 - val_accuracy: 0.9814\n",
            "Epoch 1916/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0431 - val_accuracy: 0.9814\n",
            "Epoch 1917/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0109 - accuracy: 0.9969 - val_loss: 0.0393 - val_accuracy: 0.9845\n",
            "Epoch 1918/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.0388 - val_accuracy: 0.9814\n",
            "Epoch 1919/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0132 - accuracy: 0.9969 - val_loss: 0.0447 - val_accuracy: 0.9814\n",
            "Epoch 1920/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.0413 - val_accuracy: 0.9814\n",
            "Epoch 1921/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.0399 - val_accuracy: 0.9845\n",
            "Epoch 1922/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0166 - accuracy: 0.9939 - val_loss: 0.0389 - val_accuracy: 0.9814\n",
            "Epoch 1923/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.0512 - val_accuracy: 0.9783\n",
            "Epoch 1924/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0168 - accuracy: 0.9954 - val_loss: 0.0510 - val_accuracy: 0.9783\n",
            "Epoch 1925/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0373 - val_accuracy: 0.9845\n",
            "Epoch 1926/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0159 - accuracy: 0.9954 - val_loss: 0.0380 - val_accuracy: 0.9845\n",
            "Epoch 1927/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0151 - accuracy: 0.9969 - val_loss: 0.0428 - val_accuracy: 0.9814\n",
            "Epoch 1928/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0142 - accuracy: 0.9954 - val_loss: 0.0430 - val_accuracy: 0.9814\n",
            "Epoch 1929/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0121 - accuracy: 0.9954 - val_loss: 0.0372 - val_accuracy: 0.9845\n",
            "Epoch 1930/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.0373 - val_accuracy: 0.9845\n",
            "Epoch 1931/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0116 - accuracy: 0.9969 - val_loss: 0.0447 - val_accuracy: 0.9814\n",
            "Epoch 1932/3500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0129 - accuracy: 0.9954 - val_loss: 0.0492 - val_accuracy: 0.9783\n",
            "Epoch 1933/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.0429 - val_accuracy: 0.9814\n",
            "Epoch 1934/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0124 - accuracy: 0.9954 - val_loss: 0.0372 - val_accuracy: 0.9845\n",
            "Epoch 1935/3500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0116 - accuracy: 0.9985 - val_loss: 0.0367 - val_accuracy: 0.9845\n",
            "Epoch 1936/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.0388 - val_accuracy: 0.9814\n",
            "Epoch 1937/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0117 - accuracy: 0.9969 - val_loss: 0.0415 - val_accuracy: 0.9814\n",
            "Epoch 1938/3500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0119 - accuracy: 0.9954 - val_loss: 0.0390 - val_accuracy: 0.9814\n",
            "Epoch 1939/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.0382 - val_accuracy: 0.9814\n",
            "Epoch 1940/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0116 - accuracy: 0.9985 - val_loss: 0.0376 - val_accuracy: 0.9814\n",
            "Epoch 1941/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.0372 - val_accuracy: 0.9845\n",
            "Epoch 1942/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0120 - accuracy: 0.9969 - val_loss: 0.0395 - val_accuracy: 0.9814\n",
            "Epoch 1943/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0109 - accuracy: 0.9969 - val_loss: 0.0464 - val_accuracy: 0.9814\n",
            "Epoch 1944/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.0402 - val_accuracy: 0.9814\n",
            "Epoch 1945/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0099 - accuracy: 0.9969 - val_loss: 0.0434 - val_accuracy: 0.9876\n",
            "Epoch 1946/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0226 - accuracy: 0.9923 - val_loss: 0.0398 - val_accuracy: 0.9814\n",
            "Epoch 1947/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0170 - accuracy: 0.9954 - val_loss: 0.0601 - val_accuracy: 0.9814\n",
            "Epoch 1948/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0213 - accuracy: 0.9954 - val_loss: 0.0458 - val_accuracy: 0.9783\n",
            "Epoch 1949/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0137 - accuracy: 0.9969 - val_loss: 0.0364 - val_accuracy: 0.9845\n",
            "Epoch 1950/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0125 - accuracy: 0.9969 - val_loss: 0.0374 - val_accuracy: 0.9845\n",
            "Epoch 1951/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0396 - val_accuracy: 0.9814\n",
            "Epoch 1952/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0121 - accuracy: 0.9954 - val_loss: 0.0496 - val_accuracy: 0.9783\n",
            "Epoch 1953/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0148 - accuracy: 0.9954 - val_loss: 0.0426 - val_accuracy: 0.9814\n",
            "Epoch 1954/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0108 - accuracy: 0.9985 - val_loss: 0.0391 - val_accuracy: 0.9876\n",
            "Epoch 1955/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0171 - accuracy: 0.9939 - val_loss: 0.0383 - val_accuracy: 0.9814\n",
            "Epoch 1956/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0121 - accuracy: 0.9969 - val_loss: 0.0495 - val_accuracy: 0.9845\n",
            "Epoch 1957/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.0521 - val_accuracy: 0.9814\n",
            "Epoch 1958/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.0404 - val_accuracy: 0.9814\n",
            "Epoch 1959/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0110 - accuracy: 0.9969 - val_loss: 0.0368 - val_accuracy: 0.9845\n",
            "Epoch 1960/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.0363 - val_accuracy: 0.9845\n",
            "Epoch 1961/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.0493 - val_accuracy: 0.9783\n",
            "Epoch 1962/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.0544 - val_accuracy: 0.9783\n",
            "Epoch 1963/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0190 - accuracy: 0.9954 - val_loss: 0.0412 - val_accuracy: 0.9783\n",
            "Epoch 1964/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.0361 - val_accuracy: 0.9876\n",
            "Epoch 1965/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.0351 - val_accuracy: 0.9845\n",
            "Epoch 1966/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.0458 - val_accuracy: 0.9783\n",
            "Epoch 1967/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.0489 - val_accuracy: 0.9814\n",
            "Epoch 1968/3500\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.0375 - val_accuracy: 0.9814\n",
            "Epoch 1969/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0370 - val_accuracy: 0.9814\n",
            "Epoch 1970/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0129 - accuracy: 0.9954 - val_loss: 0.0421 - val_accuracy: 0.9814\n",
            "Epoch 1971/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.0451 - val_accuracy: 0.9814\n",
            "Epoch 1972/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0127 - accuracy: 0.9954 - val_loss: 0.0407 - val_accuracy: 0.9814\n",
            "Epoch 1973/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.0385 - val_accuracy: 0.9814\n",
            "Epoch 1974/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.0405 - val_accuracy: 0.9814\n",
            "Epoch 1975/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.0407 - val_accuracy: 0.9814\n",
            "Epoch 1976/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0119 - accuracy: 0.9969 - val_loss: 0.0388 - val_accuracy: 0.9814\n",
            "Epoch 1977/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.0390 - val_accuracy: 0.9814\n",
            "Epoch 1978/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.0392 - val_accuracy: 0.9814\n",
            "Epoch 1979/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0112 - accuracy: 0.9985 - val_loss: 0.0381 - val_accuracy: 0.9814\n",
            "Epoch 1980/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0111 - accuracy: 0.9985 - val_loss: 0.0387 - val_accuracy: 0.9814\n",
            "Epoch 1981/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0110 - accuracy: 0.9985 - val_loss: 0.0400 - val_accuracy: 0.9814\n",
            "Epoch 1982/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0110 - accuracy: 0.9985 - val_loss: 0.0395 - val_accuracy: 0.9814\n",
            "Epoch 1983/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0110 - accuracy: 0.9985 - val_loss: 0.0388 - val_accuracy: 0.9814\n",
            "Epoch 1984/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0111 - accuracy: 0.9985 - val_loss: 0.0398 - val_accuracy: 0.9814\n",
            "Epoch 1985/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0110 - accuracy: 0.9985 - val_loss: 0.0415 - val_accuracy: 0.9814\n",
            "Epoch 1986/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0113 - accuracy: 0.9969 - val_loss: 0.0383 - val_accuracy: 0.9814\n",
            "Epoch 1987/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.0373 - val_accuracy: 0.9845\n",
            "Epoch 1988/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.0388 - val_accuracy: 0.9814\n",
            "Epoch 1989/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0107 - accuracy: 0.9985 - val_loss: 0.0427 - val_accuracy: 0.9814\n",
            "Epoch 1990/3500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.0128 - accuracy: 0.9954 - val_loss: 0.0395 - val_accuracy: 0.9814\n",
            "Epoch 1991/3500\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0104 - accuracy: 0.9985 - val_loss: 0.0381 - val_accuracy: 0.9845\n",
            "Epoch 1992/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0145 - accuracy: 0.9954 - val_loss: 0.0376 - val_accuracy: 0.9845\n",
            "Epoch 1993/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0111 - accuracy: 0.9985 - val_loss: 0.0484 - val_accuracy: 0.9783\n",
            "Epoch 1994/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0138 - accuracy: 0.9954 - val_loss: 0.0524 - val_accuracy: 0.9814\n",
            "Epoch 1995/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.0409 - val_accuracy: 0.9814\n",
            "Epoch 1996/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.0413 - val_accuracy: 0.9907\n",
            "Epoch 1997/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0208 - accuracy: 0.9923 - val_loss: 0.0394 - val_accuracy: 0.9814\n",
            "Epoch 1998/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0124 - accuracy: 0.9985 - val_loss: 0.0678 - val_accuracy: 0.9845\n",
            "Epoch 1999/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0257 - accuracy: 0.9939 - val_loss: 0.0617 - val_accuracy: 0.9814\n",
            "Epoch 2000/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0210 - accuracy: 0.9954 - val_loss: 0.0377 - val_accuracy: 0.9814\n",
            "Epoch 2001/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.0506 - val_accuracy: 0.9876\n",
            "Epoch 2002/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0275 - accuracy: 0.9893 - val_loss: 0.0366 - val_accuracy: 0.9845\n",
            "Epoch 2003/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0130 - accuracy: 0.9969 - val_loss: 0.0518 - val_accuracy: 0.9783\n",
            "Epoch 2004/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0168 - accuracy: 0.9954 - val_loss: 0.0471 - val_accuracy: 0.9783\n",
            "Epoch 2005/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0369 - val_accuracy: 0.9845\n",
            "Epoch 2006/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0368 - val_accuracy: 0.9814\n",
            "Epoch 2007/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0431 - val_accuracy: 0.9814\n",
            "Epoch 2008/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0126 - accuracy: 0.9969 - val_loss: 0.0550 - val_accuracy: 0.9814\n",
            "Epoch 2009/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.0472 - val_accuracy: 0.9845\n",
            "Epoch 2010/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.0374 - val_accuracy: 0.9814\n",
            "Epoch 2011/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0120 - accuracy: 0.9954 - val_loss: 0.0371 - val_accuracy: 0.9814\n",
            "Epoch 2012/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.0378 - val_accuracy: 0.9814\n",
            "Epoch 2013/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0111 - accuracy: 0.9969 - val_loss: 0.0422 - val_accuracy: 0.9814\n",
            "Epoch 2014/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0111 - accuracy: 0.9969 - val_loss: 0.0490 - val_accuracy: 0.9783\n",
            "Epoch 2015/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0138 - accuracy: 0.9954 - val_loss: 0.0510 - val_accuracy: 0.9814\n",
            "Epoch 2016/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0144 - accuracy: 0.9954 - val_loss: 0.0416 - val_accuracy: 0.9814\n",
            "Epoch 2017/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0114 - accuracy: 0.9969 - val_loss: 0.0398 - val_accuracy: 0.9814\n",
            "Epoch 2018/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0168 - accuracy: 0.9939 - val_loss: 0.0381 - val_accuracy: 0.9845\n",
            "Epoch 2019/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 0.0431 - val_accuracy: 0.9814\n",
            "Epoch 2020/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0114 - accuracy: 0.9969 - val_loss: 0.0424 - val_accuracy: 0.9814\n",
            "Epoch 2021/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0110 - accuracy: 0.9969 - val_loss: 0.0407 - val_accuracy: 0.9814\n",
            "Epoch 2022/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.0387 - val_accuracy: 0.9814\n",
            "Epoch 2023/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0117 - accuracy: 0.9969 - val_loss: 0.0387 - val_accuracy: 0.9814\n",
            "Epoch 2024/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0113 - accuracy: 0.9969 - val_loss: 0.0429 - val_accuracy: 0.9814\n",
            "Epoch 2025/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0110 - accuracy: 0.9969 - val_loss: 0.0463 - val_accuracy: 0.9814\n",
            "Epoch 2026/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0123 - accuracy: 0.9954 - val_loss: 0.0434 - val_accuracy: 0.9814\n",
            "Epoch 2027/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0112 - accuracy: 0.9954 - val_loss: 0.0380 - val_accuracy: 0.9814\n",
            "Epoch 2028/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.0365 - val_accuracy: 0.9845\n",
            "Epoch 2029/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0127 - accuracy: 0.9969 - val_loss: 0.0369 - val_accuracy: 0.9814\n",
            "Epoch 2030/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.0431 - val_accuracy: 0.9814\n",
            "Epoch 2031/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0125 - accuracy: 0.9954 - val_loss: 0.0404 - val_accuracy: 0.9814\n",
            "Epoch 2032/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0118 - accuracy: 0.9954 - val_loss: 0.0362 - val_accuracy: 0.9845\n",
            "Epoch 2033/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0109 - accuracy: 0.9985 - val_loss: 0.0361 - val_accuracy: 0.9845\n",
            "Epoch 2034/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0111 - accuracy: 0.9985 - val_loss: 0.0377 - val_accuracy: 0.9814\n",
            "Epoch 2035/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0108 - accuracy: 0.9969 - val_loss: 0.0387 - val_accuracy: 0.9814\n",
            "Epoch 2036/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.0373 - val_accuracy: 0.9814\n",
            "Epoch 2037/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0108 - accuracy: 0.9985 - val_loss: 0.0374 - val_accuracy: 0.9814\n",
            "Epoch 2038/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.0377 - val_accuracy: 0.9814\n",
            "Epoch 2039/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.0365 - val_accuracy: 0.9845\n",
            "Epoch 2040/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0113 - accuracy: 0.9969 - val_loss: 0.0374 - val_accuracy: 0.9814\n",
            "Epoch 2041/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0115 - accuracy: 0.9969 - val_loss: 0.0387 - val_accuracy: 0.9814\n",
            "Epoch 2042/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.0365 - val_accuracy: 0.9845\n",
            "Epoch 2043/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0120 - accuracy: 0.9954 - val_loss: 0.0369 - val_accuracy: 0.9814\n",
            "Epoch 2044/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0121 - accuracy: 0.9969 - val_loss: 0.0402 - val_accuracy: 0.9814\n",
            "Epoch 2045/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0107 - accuracy: 0.9969 - val_loss: 0.0388 - val_accuracy: 0.9814\n",
            "Epoch 2046/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.0385 - val_accuracy: 0.9814\n",
            "Epoch 2047/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.0400 - val_accuracy: 0.9814\n",
            "Epoch 2048/3500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0105 - accuracy: 0.9969 - val_loss: 0.0424 - val_accuracy: 0.9814\n",
            "Epoch 2049/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0114 - accuracy: 0.9954 - val_loss: 0.0423 - val_accuracy: 0.9814\n",
            "Epoch 2050/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0114 - accuracy: 0.9954 - val_loss: 0.0389 - val_accuracy: 0.9814\n",
            "Epoch 2051/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.0363 - val_accuracy: 0.9845\n",
            "Epoch 2052/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.0362 - val_accuracy: 0.9845\n",
            "Epoch 2053/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.0404 - val_accuracy: 0.9814\n",
            "Epoch 2054/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0115 - accuracy: 0.9969 - val_loss: 0.0458 - val_accuracy: 0.9814\n",
            "Epoch 2055/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0125 - accuracy: 0.9954 - val_loss: 0.0388 - val_accuracy: 0.9814\n",
            "Epoch 2056/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0109 - accuracy: 0.9985 - val_loss: 0.0363 - val_accuracy: 0.9845\n",
            "Epoch 2057/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0126 - accuracy: 0.9969 - val_loss: 0.0365 - val_accuracy: 0.9845\n",
            "Epoch 2058/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0112 - accuracy: 0.9985 - val_loss: 0.0376 - val_accuracy: 0.9814\n",
            "Epoch 2059/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0104 - accuracy: 0.9985 - val_loss: 0.0413 - val_accuracy: 0.9814\n",
            "Epoch 2060/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0111 - accuracy: 0.9969 - val_loss: 0.0424 - val_accuracy: 0.9814\n",
            "Epoch 2061/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0119 - accuracy: 0.9954 - val_loss: 0.0378 - val_accuracy: 0.9814\n",
            "Epoch 2062/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0110 - accuracy: 0.9985 - val_loss: 0.0361 - val_accuracy: 0.9845\n",
            "Epoch 2063/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0127 - accuracy: 0.9969 - val_loss: 0.0374 - val_accuracy: 0.9814\n",
            "Epoch 2064/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0110 - accuracy: 0.9969 - val_loss: 0.0456 - val_accuracy: 0.9814\n",
            "Epoch 2065/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 0.0414 - val_accuracy: 0.9814\n",
            "Epoch 2066/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0109 - accuracy: 0.9969 - val_loss: 0.0356 - val_accuracy: 0.9845\n",
            "Epoch 2067/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0120 - accuracy: 0.9954 - val_loss: 0.0356 - val_accuracy: 0.9845\n",
            "Epoch 2068/3500\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0114 - accuracy: 0.9969 - val_loss: 0.0402 - val_accuracy: 0.9814\n",
            "Epoch 2069/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0113 - accuracy: 0.9954 - val_loss: 0.0459 - val_accuracy: 0.9814\n",
            "Epoch 2070/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.0395 - val_accuracy: 0.9814\n",
            "Epoch 2071/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0110 - accuracy: 0.9969 - val_loss: 0.0358 - val_accuracy: 0.9845\n",
            "Epoch 2072/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0119 - accuracy: 0.9969 - val_loss: 0.0370 - val_accuracy: 0.9814\n",
            "Epoch 2073/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.0444 - val_accuracy: 0.9814\n",
            "Epoch 2074/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0125 - accuracy: 0.9954 - val_loss: 0.0421 - val_accuracy: 0.9814\n",
            "Epoch 2075/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.0352 - val_accuracy: 0.9845\n",
            "Epoch 2076/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0111 - accuracy: 0.9969 - val_loss: 0.0348 - val_accuracy: 0.9845\n",
            "Epoch 2077/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0122 - accuracy: 0.9954 - val_loss: 0.0360 - val_accuracy: 0.9814\n",
            "Epoch 2078/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0104 - accuracy: 0.9985 - val_loss: 0.0405 - val_accuracy: 0.9814\n",
            "Epoch 2079/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0114 - accuracy: 0.9954 - val_loss: 0.0414 - val_accuracy: 0.9814\n",
            "Epoch 2080/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0118 - accuracy: 0.9954 - val_loss: 0.0380 - val_accuracy: 0.9814\n",
            "Epoch 2081/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.0352 - val_accuracy: 0.9845\n",
            "Epoch 2082/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0111 - accuracy: 0.9985 - val_loss: 0.0340 - val_accuracy: 0.9845\n",
            "Epoch 2083/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0119 - accuracy: 0.9969 - val_loss: 0.0342 - val_accuracy: 0.9845\n",
            "Epoch 2084/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0117 - accuracy: 0.9969 - val_loss: 0.0385 - val_accuracy: 0.9814\n",
            "Epoch 2085/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0115 - accuracy: 0.9954 - val_loss: 0.0447 - val_accuracy: 0.9783\n",
            "Epoch 2086/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0128 - accuracy: 0.9954 - val_loss: 0.0391 - val_accuracy: 0.9814\n",
            "Epoch 2087/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0108 - accuracy: 0.9985 - val_loss: 0.0357 - val_accuracy: 0.9845\n",
            "Epoch 2088/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.0354 - val_accuracy: 0.9845\n",
            "Epoch 2089/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0107 - accuracy: 0.9985 - val_loss: 0.0364 - val_accuracy: 0.9814\n",
            "Epoch 2090/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0103 - accuracy: 0.9985 - val_loss: 0.0392 - val_accuracy: 0.9814\n",
            "Epoch 2091/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0106 - accuracy: 0.9969 - val_loss: 0.0429 - val_accuracy: 0.9814\n",
            "Epoch 2092/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0119 - accuracy: 0.9954 - val_loss: 0.0407 - val_accuracy: 0.9814\n",
            "Epoch 2093/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.0360 - val_accuracy: 0.9845\n",
            "Epoch 2094/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.0356 - val_accuracy: 0.9845\n",
            "Epoch 2095/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0107 - accuracy: 0.9985 - val_loss: 0.0375 - val_accuracy: 0.9814\n",
            "Epoch 2096/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.0411 - val_accuracy: 0.9814\n",
            "Epoch 2097/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0116 - accuracy: 0.9954 - val_loss: 0.0388 - val_accuracy: 0.9814\n",
            "Epoch 2098/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0104 - accuracy: 0.9985 - val_loss: 0.0359 - val_accuracy: 0.9845\n",
            "Epoch 2099/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0123 - accuracy: 0.9969 - val_loss: 0.0362 - val_accuracy: 0.9845\n",
            "Epoch 2100/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0102 - accuracy: 0.9985 - val_loss: 0.0420 - val_accuracy: 0.9814\n",
            "Epoch 2101/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0117 - accuracy: 0.9954 - val_loss: 0.0431 - val_accuracy: 0.9814\n",
            "Epoch 2102/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0118 - accuracy: 0.9954 - val_loss: 0.0369 - val_accuracy: 0.9814\n",
            "Epoch 2103/3500\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.0107 - accuracy: 0.9985 - val_loss: 0.0362 - val_accuracy: 0.9845\n",
            "Epoch 2104/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0104 - accuracy: 0.9985 - val_loss: 0.0399 - val_accuracy: 0.9814\n",
            "Epoch 2105/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0104 - accuracy: 0.9985 - val_loss: 0.0443 - val_accuracy: 0.9845\n",
            "Epoch 2106/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0120 - accuracy: 0.9954 - val_loss: 0.0403 - val_accuracy: 0.9814\n",
            "Epoch 2107/3500\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.0099 - accuracy: 0.9969 - val_loss: 0.0354 - val_accuracy: 0.9845\n",
            "Epoch 2108/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0123 - accuracy: 0.9954 - val_loss: 0.0360 - val_accuracy: 0.9845\n",
            "Epoch 2109/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0139 - accuracy: 0.9969 - val_loss: 0.0380 - val_accuracy: 0.9814\n",
            "Epoch 2110/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.0456 - val_accuracy: 0.9814\n",
            "Epoch 2111/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0128 - accuracy: 0.9954 - val_loss: 0.0416 - val_accuracy: 0.9814\n",
            "Epoch 2112/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0115 - accuracy: 0.9954 - val_loss: 0.0354 - val_accuracy: 0.9845\n",
            "Epoch 2113/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0111 - accuracy: 0.9969 - val_loss: 0.0356 - val_accuracy: 0.9845\n",
            "Epoch 2114/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0102 - accuracy: 0.9985 - val_loss: 0.0412 - val_accuracy: 0.9814\n",
            "Epoch 2115/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0112 - accuracy: 0.9954 - val_loss: 0.0432 - val_accuracy: 0.9814\n",
            "Epoch 2116/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0119 - accuracy: 0.9954 - val_loss: 0.0375 - val_accuracy: 0.9814\n",
            "Epoch 2117/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0109 - accuracy: 0.9985 - val_loss: 0.0348 - val_accuracy: 0.9845\n",
            "Epoch 2118/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0112 - accuracy: 0.9985 - val_loss: 0.0352 - val_accuracy: 0.9845\n",
            "Epoch 2119/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.0355 - val_accuracy: 0.9845\n",
            "Epoch 2120/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0102 - accuracy: 0.9985 - val_loss: 0.0359 - val_accuracy: 0.9814\n",
            "Epoch 2121/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0102 - accuracy: 0.9985 - val_loss: 0.0378 - val_accuracy: 0.9814\n",
            "Epoch 2122/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0103 - accuracy: 0.9985 - val_loss: 0.0407 - val_accuracy: 0.9814\n",
            "Epoch 2123/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0110 - accuracy: 0.9969 - val_loss: 0.0382 - val_accuracy: 0.9814\n",
            "Epoch 2124/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9845\n",
            "Epoch 2125/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0111 - accuracy: 0.9969 - val_loss: 0.0346 - val_accuracy: 0.9845\n",
            "Epoch 2126/3500\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.0109 - accuracy: 0.9985 - val_loss: 0.0383 - val_accuracy: 0.9814\n",
            "Epoch 2127/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0108 - accuracy: 0.9969 - val_loss: 0.0426 - val_accuracy: 0.9814\n",
            "Epoch 2128/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0115 - accuracy: 0.9954 - val_loss: 0.0383 - val_accuracy: 0.9814\n",
            "Epoch 2129/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.0355 - val_accuracy: 0.9845\n",
            "Epoch 2130/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0107 - accuracy: 0.9985 - val_loss: 0.0357 - val_accuracy: 0.9845\n",
            "Epoch 2131/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0102 - accuracy: 0.9985 - val_loss: 0.0397 - val_accuracy: 0.9814\n",
            "Epoch 2132/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0105 - accuracy: 0.9969 - val_loss: 0.0416 - val_accuracy: 0.9814\n",
            "Epoch 2133/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0114 - accuracy: 0.9954 - val_loss: 0.0393 - val_accuracy: 0.9814\n",
            "Epoch 2134/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0107 - accuracy: 0.9969 - val_loss: 0.0360 - val_accuracy: 0.9814\n",
            "Epoch 2135/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0102 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9845\n",
            "Epoch 2136/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0103 - accuracy: 0.9985 - val_loss: 0.0355 - val_accuracy: 0.9845\n",
            "Epoch 2137/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.0383 - val_accuracy: 0.9814\n",
            "Epoch 2138/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0107 - accuracy: 0.9969 - val_loss: 0.0383 - val_accuracy: 0.9814\n",
            "Epoch 2139/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.0363 - val_accuracy: 0.9814\n",
            "Epoch 2140/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.0347 - val_accuracy: 0.9845\n",
            "Epoch 2141/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0102 - accuracy: 0.9985 - val_loss: 0.0345 - val_accuracy: 0.9845\n",
            "Epoch 2142/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0107 - accuracy: 0.9985 - val_loss: 0.0361 - val_accuracy: 0.9845\n",
            "Epoch 2143/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0104 - accuracy: 0.9985 - val_loss: 0.0393 - val_accuracy: 0.9814\n",
            "Epoch 2144/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0105 - accuracy: 0.9969 - val_loss: 0.0384 - val_accuracy: 0.9814\n",
            "Epoch 2145/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.0369 - val_accuracy: 0.9814\n",
            "Epoch 2146/3500\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 0.0358 - val_accuracy: 0.9845\n",
            "Epoch 2147/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.0361 - val_accuracy: 0.9845\n",
            "Epoch 2148/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0104 - accuracy: 0.9985 - val_loss: 0.0371 - val_accuracy: 0.9814\n",
            "Epoch 2149/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.0377 - val_accuracy: 0.9814\n",
            "Epoch 2150/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.0378 - val_accuracy: 0.9814\n",
            "Epoch 2151/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.0377 - val_accuracy: 0.9814\n",
            "Epoch 2152/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.0384 - val_accuracy: 0.9814\n",
            "Epoch 2153/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 0.0395 - val_accuracy: 0.9814\n",
            "Epoch 2154/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.0400 - val_accuracy: 0.9814\n",
            "Epoch 2155/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0104 - accuracy: 0.9969 - val_loss: 0.0389 - val_accuracy: 0.9814\n",
            "Epoch 2156/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.0364 - val_accuracy: 0.9845\n",
            "Epoch 2157/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 0.0354 - val_accuracy: 0.9845\n",
            "Epoch 2158/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0115 - accuracy: 0.9985 - val_loss: 0.0361 - val_accuracy: 0.9845\n",
            "Epoch 2159/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.0397 - val_accuracy: 0.9814\n",
            "Epoch 2160/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0106 - accuracy: 0.9969 - val_loss: 0.0384 - val_accuracy: 0.9814\n",
            "Epoch 2161/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.0362 - val_accuracy: 0.9814\n",
            "Epoch 2162/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.0351 - val_accuracy: 0.9845\n",
            "Epoch 2163/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0107 - accuracy: 0.9985 - val_loss: 0.0357 - val_accuracy: 0.9845\n",
            "Epoch 2164/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.0403 - val_accuracy: 0.9814\n",
            "Epoch 2165/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0110 - accuracy: 0.9954 - val_loss: 0.0429 - val_accuracy: 0.9814\n",
            "Epoch 2166/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0110 - accuracy: 0.9954 - val_loss: 0.0367 - val_accuracy: 0.9814\n",
            "Epoch 2167/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 0.0364 - val_accuracy: 0.9845\n",
            "Epoch 2168/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0127 - accuracy: 0.9969 - val_loss: 0.0380 - val_accuracy: 0.9814\n",
            "Epoch 2169/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0097 - accuracy: 0.9985 - val_loss: 0.0480 - val_accuracy: 0.9814\n",
            "Epoch 2170/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0159 - accuracy: 0.9954 - val_loss: 0.0394 - val_accuracy: 0.9814\n",
            "Epoch 2171/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.0394 - val_accuracy: 0.9814\n",
            "Epoch 2172/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0176 - accuracy: 0.9939 - val_loss: 0.0387 - val_accuracy: 0.9814\n",
            "Epoch 2173/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0120 - accuracy: 0.9954 - val_loss: 0.0613 - val_accuracy: 0.9845\n",
            "Epoch 2174/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0206 - accuracy: 0.9954 - val_loss: 0.0509 - val_accuracy: 0.9814\n",
            "Epoch 2175/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0338 - val_accuracy: 0.9845\n",
            "Epoch 2176/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0123 - accuracy: 0.9954 - val_loss: 0.0450 - val_accuracy: 0.9845\n",
            "Epoch 2177/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0218 - accuracy: 0.9954 - val_loss: 0.0427 - val_accuracy: 0.9814\n",
            "Epoch 2178/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0124 - accuracy: 0.9954 - val_loss: 0.0902 - val_accuracy: 0.9814\n",
            "Epoch 2179/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0383 - accuracy: 0.9877 - val_loss: 0.0722 - val_accuracy: 0.9845\n",
            "Epoch 2180/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0221 - accuracy: 0.9939 - val_loss: 0.0341 - val_accuracy: 0.9845\n",
            "Epoch 2181/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.0455 - val_accuracy: 0.9845\n",
            "Epoch 2182/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0238 - accuracy: 0.9939 - val_loss: 0.0431 - val_accuracy: 0.9783\n",
            "Epoch 2183/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.0648 - val_accuracy: 0.9845\n",
            "Epoch 2184/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0242 - accuracy: 0.9939 - val_loss: 0.0417 - val_accuracy: 0.9783\n",
            "Epoch 2185/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0110 - accuracy: 0.9954 - val_loss: 0.0446 - val_accuracy: 0.9876\n",
            "Epoch 2186/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0300 - accuracy: 0.9908 - val_loss: 0.0338 - val_accuracy: 0.9845\n",
            "Epoch 2187/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.0456 - val_accuracy: 0.9814\n",
            "Epoch 2188/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.0550 - val_accuracy: 0.9814\n",
            "Epoch 2189/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0164 - accuracy: 0.9954 - val_loss: 0.0442 - val_accuracy: 0.9845\n",
            "Epoch 2190/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0133 - accuracy: 0.9939 - val_loss: 0.0355 - val_accuracy: 0.9845\n",
            "Epoch 2191/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0122 - accuracy: 0.9954 - val_loss: 0.0360 - val_accuracy: 0.9814\n",
            "Epoch 2192/3500\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.0111 - accuracy: 0.9954 - val_loss: 0.0414 - val_accuracy: 0.9845\n",
            "Epoch 2193/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0117 - accuracy: 0.9954 - val_loss: 0.0418 - val_accuracy: 0.9845\n",
            "Epoch 2194/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0116 - accuracy: 0.9954 - val_loss: 0.0359 - val_accuracy: 0.9814\n",
            "Epoch 2195/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0103 - accuracy: 0.9985 - val_loss: 0.0340 - val_accuracy: 0.9814\n",
            "Epoch 2196/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0116 - accuracy: 0.9954 - val_loss: 0.0345 - val_accuracy: 0.9845\n",
            "Epoch 2197/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0111 - accuracy: 0.9969 - val_loss: 0.0378 - val_accuracy: 0.9814\n",
            "Epoch 2198/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0107 - accuracy: 0.9969 - val_loss: 0.0365 - val_accuracy: 0.9814\n",
            "Epoch 2199/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.0352 - val_accuracy: 0.9845\n",
            "Epoch 2200/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0106 - accuracy: 0.9969 - val_loss: 0.0380 - val_accuracy: 0.9814\n",
            "Epoch 2201/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 0.0444 - val_accuracy: 0.9845\n",
            "Epoch 2202/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0121 - accuracy: 0.9954 - val_loss: 0.0415 - val_accuracy: 0.9814\n",
            "Epoch 2203/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0102 - accuracy: 0.9985 - val_loss: 0.0361 - val_accuracy: 0.9845\n",
            "Epoch 2204/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0115 - accuracy: 0.9969 - val_loss: 0.0365 - val_accuracy: 0.9845\n",
            "Epoch 2205/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0121 - accuracy: 0.9954 - val_loss: 0.0410 - val_accuracy: 0.9814\n",
            "Epoch 2206/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0139 - accuracy: 0.9954 - val_loss: 0.0477 - val_accuracy: 0.9845\n",
            "Epoch 2207/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0115 - accuracy: 0.9969 - val_loss: 0.0378 - val_accuracy: 0.9814\n",
            "Epoch 2208/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0104 - accuracy: 0.9985 - val_loss: 0.0370 - val_accuracy: 0.9845\n",
            "Epoch 2209/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0127 - accuracy: 0.9954 - val_loss: 0.0393 - val_accuracy: 0.9814\n",
            "Epoch 2210/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.0477 - val_accuracy: 0.9845\n",
            "Epoch 2211/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0124 - accuracy: 0.9954 - val_loss: 0.0471 - val_accuracy: 0.9814\n",
            "Epoch 2212/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0121 - accuracy: 0.9954 - val_loss: 0.0379 - val_accuracy: 0.9814\n",
            "Epoch 2213/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.0359 - val_accuracy: 0.9814\n",
            "Epoch 2214/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0140 - accuracy: 0.9939 - val_loss: 0.0347 - val_accuracy: 0.9845\n",
            "Epoch 2215/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.0450 - val_accuracy: 0.9783\n",
            "Epoch 2216/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0141 - accuracy: 0.9954 - val_loss: 0.0480 - val_accuracy: 0.9783\n",
            "Epoch 2217/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.0338 - val_accuracy: 0.9845\n",
            "Epoch 2218/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0126 - accuracy: 0.9985 - val_loss: 0.0459 - val_accuracy: 0.9845\n",
            "Epoch 2219/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0258 - accuracy: 0.9862 - val_loss: 0.0415 - val_accuracy: 0.9845\n",
            "Epoch 2220/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0114 - accuracy: 0.9969 - val_loss: 0.0669 - val_accuracy: 0.9845\n",
            "Epoch 2221/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0228 - accuracy: 0.9939 - val_loss: 0.0540 - val_accuracy: 0.9814\n",
            "Epoch 2222/3500\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.0146 - accuracy: 0.9969 - val_loss: 0.0335 - val_accuracy: 0.9845\n",
            "Epoch 2223/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0154 - accuracy: 0.9954 - val_loss: 0.0333 - val_accuracy: 0.9876\n",
            "Epoch 2224/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0104 - accuracy: 0.9969 - val_loss: 0.0431 - val_accuracy: 0.9783\n",
            "Epoch 2225/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.0497 - val_accuracy: 0.9783\n",
            "Epoch 2226/3500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.0371 - val_accuracy: 0.9814\n",
            "Epoch 2227/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0106 - accuracy: 0.9969 - val_loss: 0.0339 - val_accuracy: 0.9845\n",
            "Epoch 2228/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0142 - accuracy: 0.9954 - val_loss: 0.0336 - val_accuracy: 0.9845\n",
            "Epoch 2229/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0117 - accuracy: 0.9969 - val_loss: 0.0399 - val_accuracy: 0.9814\n",
            "Epoch 2230/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0138 - accuracy: 0.9954 - val_loss: 0.0443 - val_accuracy: 0.9814\n",
            "Epoch 2231/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0122 - accuracy: 0.9954 - val_loss: 0.0347 - val_accuracy: 0.9845\n",
            "Epoch 2232/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0106 - accuracy: 0.9969 - val_loss: 0.0347 - val_accuracy: 0.9814\n",
            "Epoch 2233/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0128 - accuracy: 0.9954 - val_loss: 0.0364 - val_accuracy: 0.9814\n",
            "Epoch 2234/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0121 - accuracy: 0.9969 - val_loss: 0.0416 - val_accuracy: 0.9845\n",
            "Epoch 2235/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0104 - accuracy: 0.9969 - val_loss: 0.0366 - val_accuracy: 0.9814\n",
            "Epoch 2236/3500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.0352 - val_accuracy: 0.9814\n",
            "Epoch 2237/3500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0123 - accuracy: 0.9954 - val_loss: 0.0355 - val_accuracy: 0.9845\n",
            "Epoch 2238/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0107 - accuracy: 0.9969 - val_loss: 0.0393 - val_accuracy: 0.9814\n",
            "Epoch 2239/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.0430 - val_accuracy: 0.9845\n",
            "Epoch 2240/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0108 - accuracy: 0.9954 - val_loss: 0.0407 - val_accuracy: 0.9814\n",
            "Epoch 2241/3500\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 0.0376 - val_accuracy: 0.9814\n",
            "Epoch 2242/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0102 - accuracy: 0.9985 - val_loss: 0.0370 - val_accuracy: 0.9814\n",
            "Epoch 2243/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.0382 - val_accuracy: 0.9814\n",
            "Epoch 2244/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.0366 - val_accuracy: 0.9845\n",
            "Epoch 2245/3500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.0364 - val_accuracy: 0.9845\n",
            "Epoch 2246/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0107 - accuracy: 0.9969 - val_loss: 0.0377 - val_accuracy: 0.9814\n",
            "Epoch 2247/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0097 - accuracy: 0.9985 - val_loss: 0.0410 - val_accuracy: 0.9845\n",
            "Epoch 2248/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0099 - accuracy: 0.9969 - val_loss: 0.0422 - val_accuracy: 0.9845\n",
            "Epoch 2249/3500\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.0387 - val_accuracy: 0.9814\n",
            "Epoch 2250/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.0361 - val_accuracy: 0.9845\n",
            "Epoch 2251/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.0363 - val_accuracy: 0.9814\n",
            "Epoch 2252/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0097 - accuracy: 0.9985 - val_loss: 0.0373 - val_accuracy: 0.9814\n",
            "Epoch 2253/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.0377 - val_accuracy: 0.9814\n",
            "Epoch 2254/3500\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.0369 - val_accuracy: 0.9814\n",
            "Epoch 2255/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.0371 - val_accuracy: 0.9814\n",
            "Epoch 2256/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.0382 - val_accuracy: 0.9814\n",
            "Epoch 2257/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0097 - accuracy: 0.9985 - val_loss: 0.0394 - val_accuracy: 0.9814\n",
            "Epoch 2258/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.0387 - val_accuracy: 0.9814\n",
            "Epoch 2259/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0099 - accuracy: 0.9969 - val_loss: 0.0365 - val_accuracy: 0.9814\n",
            "Epoch 2260/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.0354 - val_accuracy: 0.9845\n",
            "Epoch 2261/3500\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.0354 - val_accuracy: 0.9814\n",
            "Epoch 2262/3500\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.0354 - val_accuracy: 0.9814\n",
            "Epoch 2263/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.0362 - val_accuracy: 0.9814\n",
            "Epoch 2264/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0097 - accuracy: 0.9985 - val_loss: 0.0373 - val_accuracy: 0.9814\n",
            "Epoch 2265/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 0.0365 - val_accuracy: 0.9814\n",
            "Epoch 2266/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.0354 - val_accuracy: 0.9845\n",
            "Epoch 2267/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.0345 - val_accuracy: 0.9845\n",
            "Epoch 2268/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0097 - accuracy: 0.9985 - val_loss: 0.0377 - val_accuracy: 0.9814\n",
            "Epoch 2269/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.0430 - val_accuracy: 0.9845\n",
            "Epoch 2270/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0112 - accuracy: 0.9954 - val_loss: 0.0400 - val_accuracy: 0.9845\n",
            "Epoch 2271/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0103 - accuracy: 0.9969 - val_loss: 0.0351 - val_accuracy: 0.9845\n",
            "Epoch 2272/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.0349 - val_accuracy: 0.9845\n",
            "Epoch 2273/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.0374 - val_accuracy: 0.9814\n",
            "Epoch 2274/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.0390 - val_accuracy: 0.9814\n",
            "Epoch 2275/3500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.0394 - val_accuracy: 0.9814\n",
            "Epoch 2276/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.0361 - val_accuracy: 0.9845\n",
            "Epoch 2277/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.0349 - val_accuracy: 0.9845\n",
            "Epoch 2278/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.0356 - val_accuracy: 0.9845\n",
            "Epoch 2279/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0097 - accuracy: 0.9985 - val_loss: 0.0383 - val_accuracy: 0.9814\n",
            "Epoch 2280/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.0405 - val_accuracy: 0.9814\n",
            "Epoch 2281/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.0398 - val_accuracy: 0.9814\n",
            "Epoch 2282/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.0358 - val_accuracy: 0.9845\n",
            "Epoch 2283/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.0360 - val_accuracy: 0.9845\n",
            "Epoch 2284/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.0382 - val_accuracy: 0.9814\n",
            "Epoch 2285/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0108 - accuracy: 0.9969 - val_loss: 0.0408 - val_accuracy: 0.9814\n",
            "Epoch 2286/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.0359 - val_accuracy: 0.9845\n",
            "Epoch 2287/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0097 - accuracy: 0.9985 - val_loss: 0.0351 - val_accuracy: 0.9845\n",
            "Epoch 2288/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0103 - accuracy: 0.9985 - val_loss: 0.0361 - val_accuracy: 0.9845\n",
            "Epoch 2289/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.0361 - val_accuracy: 0.9814\n",
            "Epoch 2290/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.0368 - val_accuracy: 0.9814\n",
            "Epoch 2291/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.0391 - val_accuracy: 0.9814\n",
            "Epoch 2292/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.0382 - val_accuracy: 0.9814\n",
            "Epoch 2293/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9845\n",
            "Epoch 2294/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 0.0347 - val_accuracy: 0.9845\n",
            "Epoch 2295/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.0389 - val_accuracy: 0.9814\n",
            "Epoch 2296/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0099 - accuracy: 0.9969 - val_loss: 0.0420 - val_accuracy: 0.9845\n",
            "Epoch 2297/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0112 - accuracy: 0.9954 - val_loss: 0.0380 - val_accuracy: 0.9814\n",
            "Epoch 2298/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.0336 - val_accuracy: 0.9845\n",
            "Epoch 2299/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.0340 - val_accuracy: 0.9876\n",
            "Epoch 2300/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0116 - accuracy: 0.9985 - val_loss: 0.0362 - val_accuracy: 0.9814\n",
            "Epoch 2301/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0476 - val_accuracy: 0.9814\n",
            "Epoch 2302/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0501 - val_accuracy: 0.9814\n",
            "Epoch 2303/3500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0142 - accuracy: 0.9954 - val_loss: 0.0375 - val_accuracy: 0.9814\n",
            "Epoch 2304/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0350 - val_accuracy: 0.9876\n",
            "Epoch 2305/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0141 - accuracy: 0.9954 - val_loss: 0.0341 - val_accuracy: 0.9876\n",
            "Epoch 2306/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.0375 - val_accuracy: 0.9814\n",
            "Epoch 2307/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0104 - accuracy: 0.9969 - val_loss: 0.0383 - val_accuracy: 0.9845\n",
            "Epoch 2308/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.0353 - val_accuracy: 0.9876\n",
            "Epoch 2309/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0109 - accuracy: 0.9969 - val_loss: 0.0358 - val_accuracy: 0.9876\n",
            "Epoch 2310/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0110 - accuracy: 0.9985 - val_loss: 0.0384 - val_accuracy: 0.9845\n",
            "Epoch 2311/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0097 - accuracy: 0.9985 - val_loss: 0.0433 - val_accuracy: 0.9845\n",
            "Epoch 2312/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0103 - accuracy: 0.9969 - val_loss: 0.0418 - val_accuracy: 0.9845\n",
            "Epoch 2313/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.0397 - val_accuracy: 0.9845\n",
            "Epoch 2314/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.0371 - val_accuracy: 0.9814\n",
            "Epoch 2315/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.0370 - val_accuracy: 0.9814\n",
            "Epoch 2316/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.0412 - val_accuracy: 0.9814\n",
            "Epoch 2317/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0107 - accuracy: 0.9954 - val_loss: 0.0392 - val_accuracy: 0.9814\n",
            "Epoch 2318/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.0337 - val_accuracy: 0.9845\n",
            "Epoch 2319/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9876\n",
            "Epoch 2320/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0125 - accuracy: 0.9954 - val_loss: 0.0368 - val_accuracy: 0.9814\n",
            "Epoch 2321/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0104 - accuracy: 0.9969 - val_loss: 0.0475 - val_accuracy: 0.9814\n",
            "Epoch 2322/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.0402 - val_accuracy: 0.9845\n",
            "Epoch 2323/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0110 - accuracy: 0.9954 - val_loss: 0.0333 - val_accuracy: 0.9845\n",
            "Epoch 2324/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.0334 - val_accuracy: 0.9845\n",
            "Epoch 2325/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0104 - accuracy: 0.9969 - val_loss: 0.0355 - val_accuracy: 0.9814\n",
            "Epoch 2326/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.0409 - val_accuracy: 0.9845\n",
            "Epoch 2327/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0107 - accuracy: 0.9954 - val_loss: 0.0410 - val_accuracy: 0.9814\n",
            "Epoch 2328/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0106 - accuracy: 0.9954 - val_loss: 0.0355 - val_accuracy: 0.9814\n",
            "Epoch 2329/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0347 - val_accuracy: 0.9845\n",
            "Epoch 2330/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.0337 - val_accuracy: 0.9845\n",
            "Epoch 2331/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.0432 - val_accuracy: 0.9845\n",
            "Epoch 2332/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0117 - accuracy: 0.9954 - val_loss: 0.0467 - val_accuracy: 0.9814\n",
            "Epoch 2333/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.0374 - val_accuracy: 0.9814\n",
            "Epoch 2334/3500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.0325 - val_accuracy: 0.9845\n",
            "Epoch 2335/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0114 - accuracy: 0.9969 - val_loss: 0.0326 - val_accuracy: 0.9845\n",
            "Epoch 2336/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.0414 - val_accuracy: 0.9845\n",
            "Epoch 2337/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0107 - accuracy: 0.9954 - val_loss: 0.0505 - val_accuracy: 0.9814\n",
            "Epoch 2338/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0139 - accuracy: 0.9954 - val_loss: 0.0438 - val_accuracy: 0.9845\n",
            "Epoch 2339/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.0345 - val_accuracy: 0.9845\n",
            "Epoch 2340/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0126 - accuracy: 0.9939 - val_loss: 0.0344 - val_accuracy: 0.9845\n",
            "Epoch 2341/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0106 - accuracy: 0.9969 - val_loss: 0.0423 - val_accuracy: 0.9845\n",
            "Epoch 2342/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0105 - accuracy: 0.9969 - val_loss: 0.0471 - val_accuracy: 0.9814\n",
            "Epoch 2343/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0126 - accuracy: 0.9954 - val_loss: 0.0413 - val_accuracy: 0.9814\n",
            "Epoch 2344/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.0347 - val_accuracy: 0.9845\n",
            "Epoch 2345/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0107 - accuracy: 0.9954 - val_loss: 0.0341 - val_accuracy: 0.9845\n",
            "Epoch 2346/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.0377 - val_accuracy: 0.9814\n",
            "Epoch 2347/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0104 - accuracy: 0.9985 - val_loss: 0.0395 - val_accuracy: 0.9814\n",
            "Epoch 2348/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0101 - accuracy: 0.9954 - val_loss: 0.0341 - val_accuracy: 0.9845\n",
            "Epoch 2349/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0109 - accuracy: 0.9969 - val_loss: 0.0335 - val_accuracy: 0.9845\n",
            "Epoch 2350/3500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.0396 - val_accuracy: 0.9845\n",
            "Epoch 2351/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0110 - accuracy: 0.9954 - val_loss: 0.0414 - val_accuracy: 0.9845\n",
            "Epoch 2352/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0104 - accuracy: 0.9969 - val_loss: 0.0349 - val_accuracy: 0.9845\n",
            "Epoch 2353/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.0345 - val_accuracy: 0.9845\n",
            "Epoch 2354/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0102 - accuracy: 0.9985 - val_loss: 0.0370 - val_accuracy: 0.9845\n",
            "Epoch 2355/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.0396 - val_accuracy: 0.9845\n",
            "Epoch 2356/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.0347 - val_accuracy: 0.9845\n",
            "Epoch 2357/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.0348 - val_accuracy: 0.9845\n",
            "Epoch 2358/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0119 - accuracy: 0.9969 - val_loss: 0.0364 - val_accuracy: 0.9845\n",
            "Epoch 2359/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0470 - val_accuracy: 0.9814\n",
            "Epoch 2360/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 0.0441 - val_accuracy: 0.9845\n",
            "Epoch 2361/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0109 - accuracy: 0.9954 - val_loss: 0.0339 - val_accuracy: 0.9845\n",
            "Epoch 2362/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.0362 - val_accuracy: 0.9845\n",
            "Epoch 2363/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0141 - accuracy: 0.9969 - val_loss: 0.0387 - val_accuracy: 0.9845\n",
            "Epoch 2364/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.0434 - val_accuracy: 0.9845\n",
            "Epoch 2365/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0105 - accuracy: 0.9954 - val_loss: 0.0394 - val_accuracy: 0.9845\n",
            "Epoch 2366/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0383 - val_accuracy: 0.9845\n",
            "Epoch 2367/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0389 - val_accuracy: 0.9845\n",
            "Epoch 2368/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0396 - val_accuracy: 0.9845\n",
            "Epoch 2369/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0384 - val_accuracy: 0.9845\n",
            "Epoch 2370/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0372 - val_accuracy: 0.9814\n",
            "Epoch 2371/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.0372 - val_accuracy: 0.9814\n",
            "Epoch 2372/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0372 - val_accuracy: 0.9814\n",
            "Epoch 2373/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.0387 - val_accuracy: 0.9814\n",
            "Epoch 2374/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.0382 - val_accuracy: 0.9814\n",
            "Epoch 2375/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.0343 - val_accuracy: 0.9845\n",
            "Epoch 2376/3500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.0343 - val_accuracy: 0.9845\n",
            "Epoch 2377/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0120 - accuracy: 0.9969 - val_loss: 0.0341 - val_accuracy: 0.9845\n",
            "Epoch 2378/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0397 - val_accuracy: 0.9845\n",
            "Epoch 2379/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.0434 - val_accuracy: 0.9845\n",
            "Epoch 2380/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0107 - accuracy: 0.9954 - val_loss: 0.0363 - val_accuracy: 0.9845\n",
            "Epoch 2381/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0338 - val_accuracy: 0.9845\n",
            "Epoch 2382/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.0352 - val_accuracy: 0.9845\n",
            "Epoch 2383/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0464 - val_accuracy: 0.9845\n",
            "Epoch 2384/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0122 - accuracy: 0.9954 - val_loss: 0.0499 - val_accuracy: 0.9814\n",
            "Epoch 2385/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0382 - val_accuracy: 0.9845\n",
            "Epoch 2386/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.0335 - val_accuracy: 0.9876\n",
            "Epoch 2387/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0119 - accuracy: 0.9969 - val_loss: 0.0330 - val_accuracy: 0.9845\n",
            "Epoch 2388/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0107 - accuracy: 0.9985 - val_loss: 0.0366 - val_accuracy: 0.9814\n",
            "Epoch 2389/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.0352 - val_accuracy: 0.9814\n",
            "Epoch 2390/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0334 - val_accuracy: 0.9845\n",
            "Epoch 2391/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.0336 - val_accuracy: 0.9845\n",
            "Epoch 2392/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0106 - accuracy: 0.9969 - val_loss: 0.0359 - val_accuracy: 0.9845\n",
            "Epoch 2393/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0448 - val_accuracy: 0.9845\n",
            "Epoch 2394/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0112 - accuracy: 0.9954 - val_loss: 0.0450 - val_accuracy: 0.9845\n",
            "Epoch 2395/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0113 - accuracy: 0.9954 - val_loss: 0.0374 - val_accuracy: 0.9845\n",
            "Epoch 2396/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.0336 - val_accuracy: 0.9845\n",
            "Epoch 2397/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.0338 - val_accuracy: 0.9845\n",
            "Epoch 2398/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 0.0354 - val_accuracy: 0.9814\n",
            "Epoch 2399/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0365 - val_accuracy: 0.9814\n",
            "Epoch 2400/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.0370 - val_accuracy: 0.9814\n",
            "Epoch 2401/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0338 - val_accuracy: 0.9845\n",
            "Epoch 2402/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.0345 - val_accuracy: 0.9876\n",
            "Epoch 2403/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0121 - accuracy: 0.9969 - val_loss: 0.0373 - val_accuracy: 0.9845\n",
            "Epoch 2404/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0427 - val_accuracy: 0.9845\n",
            "Epoch 2405/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0103 - accuracy: 0.9954 - val_loss: 0.0392 - val_accuracy: 0.9845\n",
            "Epoch 2406/3500\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0343 - val_accuracy: 0.9845\n",
            "Epoch 2407/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.0349 - val_accuracy: 0.9845\n",
            "Epoch 2408/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0121 - accuracy: 0.9969 - val_loss: 0.0361 - val_accuracy: 0.9814\n",
            "Epoch 2409/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0102 - accuracy: 0.9954 - val_loss: 0.0393 - val_accuracy: 0.9845\n",
            "Epoch 2410/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.0347 - val_accuracy: 0.9845\n",
            "Epoch 2411/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0335 - val_accuracy: 0.9845\n",
            "Epoch 2412/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0107 - accuracy: 0.9969 - val_loss: 0.0332 - val_accuracy: 0.9845\n",
            "Epoch 2413/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 0.0376 - val_accuracy: 0.9845\n",
            "Epoch 2414/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.0422 - val_accuracy: 0.9845\n",
            "Epoch 2415/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0107 - accuracy: 0.9954 - val_loss: 0.0355 - val_accuracy: 0.9814\n",
            "Epoch 2416/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.0330 - val_accuracy: 0.9845\n",
            "Epoch 2417/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0105 - accuracy: 0.9969 - val_loss: 0.0342 - val_accuracy: 0.9845\n",
            "Epoch 2418/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.0377 - val_accuracy: 0.9845\n",
            "Epoch 2419/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.0359 - val_accuracy: 0.9845\n",
            "Epoch 2420/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0354 - val_accuracy: 0.9845\n",
            "Epoch 2421/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0360 - val_accuracy: 0.9845\n",
            "Epoch 2422/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.0361 - val_accuracy: 0.9845\n",
            "Epoch 2423/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.0354 - val_accuracy: 0.9814\n",
            "Epoch 2424/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.0353 - val_accuracy: 0.9814\n",
            "Epoch 2425/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.0362 - val_accuracy: 0.9814\n",
            "Epoch 2426/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.0355 - val_accuracy: 0.9814\n",
            "Epoch 2427/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.0338 - val_accuracy: 0.9845\n",
            "Epoch 2428/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0338 - val_accuracy: 0.9845\n",
            "Epoch 2429/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.0362 - val_accuracy: 0.9845\n",
            "Epoch 2430/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0367 - val_accuracy: 0.9845\n",
            "Epoch 2431/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0352 - val_accuracy: 0.9876\n",
            "Epoch 2432/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0351 - val_accuracy: 0.9876\n",
            "Epoch 2433/3500\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.0097 - accuracy: 0.9985 - val_loss: 0.0342 - val_accuracy: 0.9845\n",
            "Epoch 2434/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.0345 - val_accuracy: 0.9876\n",
            "Epoch 2435/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0432 - val_accuracy: 0.9845\n",
            "Epoch 2436/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0107 - accuracy: 0.9954 - val_loss: 0.0451 - val_accuracy: 0.9845\n",
            "Epoch 2437/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0115 - accuracy: 0.9954 - val_loss: 0.0343 - val_accuracy: 0.9845\n",
            "Epoch 2438/3500\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0373 - val_accuracy: 0.9845\n",
            "Epoch 2439/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0168 - accuracy: 0.9939 - val_loss: 0.0332 - val_accuracy: 0.9845\n",
            "Epoch 2440/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.0386 - val_accuracy: 0.9845\n",
            "Epoch 2441/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0098 - accuracy: 0.9969 - val_loss: 0.0368 - val_accuracy: 0.9845\n",
            "Epoch 2442/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.0335 - val_accuracy: 0.9845\n",
            "Epoch 2443/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0335 - val_accuracy: 0.9845\n",
            "Epoch 2444/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0347 - val_accuracy: 0.9814\n",
            "Epoch 2445/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.0365 - val_accuracy: 0.9845\n",
            "Epoch 2446/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0348 - val_accuracy: 0.9845\n",
            "Epoch 2447/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.0349 - val_accuracy: 0.9876\n",
            "Epoch 2448/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0408 - val_accuracy: 0.9845\n",
            "Epoch 2449/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.0426 - val_accuracy: 0.9845\n",
            "Epoch 2450/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0104 - accuracy: 0.9954 - val_loss: 0.0375 - val_accuracy: 0.9845\n",
            "Epoch 2451/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0333 - val_accuracy: 0.9845\n",
            "Epoch 2452/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0113 - accuracy: 0.9969 - val_loss: 0.0331 - val_accuracy: 0.9845\n",
            "Epoch 2453/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.0374 - val_accuracy: 0.9845\n",
            "Epoch 2454/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0099 - accuracy: 0.9969 - val_loss: 0.0372 - val_accuracy: 0.9845\n",
            "Epoch 2455/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.0338 - val_accuracy: 0.9845\n",
            "Epoch 2456/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0341 - val_accuracy: 0.9876\n",
            "Epoch 2457/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0341 - val_accuracy: 0.9876\n",
            "Epoch 2458/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0349 - val_accuracy: 0.9876\n",
            "Epoch 2459/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0375 - val_accuracy: 0.9845\n",
            "Epoch 2460/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0398 - val_accuracy: 0.9845\n",
            "Epoch 2461/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0379 - val_accuracy: 0.9845\n",
            "Epoch 2462/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0350 - val_accuracy: 0.9845\n",
            "Epoch 2463/3500\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.0109 - accuracy: 0.9985 - val_loss: 0.0353 - val_accuracy: 0.9845\n",
            "Epoch 2464/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0442 - val_accuracy: 0.9845\n",
            "Epoch 2465/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0123 - accuracy: 0.9954 - val_loss: 0.0439 - val_accuracy: 0.9845\n",
            "Epoch 2466/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0111 - accuracy: 0.9954 - val_loss: 0.0344 - val_accuracy: 0.9845\n",
            "Epoch 2467/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.0338 - val_accuracy: 0.9845\n",
            "Epoch 2468/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.0348 - val_accuracy: 0.9814\n",
            "Epoch 2469/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0370 - val_accuracy: 0.9845\n",
            "Epoch 2470/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0358 - val_accuracy: 0.9845\n",
            "Epoch 2471/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0338 - val_accuracy: 0.9876\n",
            "Epoch 2472/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0327 - val_accuracy: 0.9845\n",
            "Epoch 2473/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9876\n",
            "Epoch 2474/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0394 - val_accuracy: 0.9845\n",
            "Epoch 2475/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0098 - accuracy: 0.9954 - val_loss: 0.0386 - val_accuracy: 0.9845\n",
            "Epoch 2476/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.0333 - val_accuracy: 0.9876\n",
            "Epoch 2477/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0321 - val_accuracy: 0.9845\n",
            "Epoch 2478/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0097 - accuracy: 0.9985 - val_loss: 0.0326 - val_accuracy: 0.9845\n",
            "Epoch 2479/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0360 - val_accuracy: 0.9845\n",
            "Epoch 2480/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0093 - accuracy: 0.9969 - val_loss: 0.0379 - val_accuracy: 0.9845\n",
            "Epoch 2481/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0098 - accuracy: 0.9969 - val_loss: 0.0347 - val_accuracy: 0.9814\n",
            "Epoch 2482/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0330 - val_accuracy: 0.9845\n",
            "Epoch 2483/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0328 - val_accuracy: 0.9845\n",
            "Epoch 2484/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0356 - val_accuracy: 0.9845\n",
            "Epoch 2485/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0404 - val_accuracy: 0.9845\n",
            "Epoch 2486/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0099 - accuracy: 0.9954 - val_loss: 0.0374 - val_accuracy: 0.9845\n",
            "Epoch 2487/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.0337 - val_accuracy: 0.9845\n",
            "Epoch 2488/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0337 - val_accuracy: 0.9845\n",
            "Epoch 2489/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0358 - val_accuracy: 0.9845\n",
            "Epoch 2490/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0390 - val_accuracy: 0.9845\n",
            "Epoch 2491/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.0376 - val_accuracy: 0.9845\n",
            "Epoch 2492/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.0340 - val_accuracy: 0.9845\n",
            "Epoch 2493/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.0339 - val_accuracy: 0.9845\n",
            "Epoch 2494/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0372 - val_accuracy: 0.9845\n",
            "Epoch 2495/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0101 - accuracy: 0.9954 - val_loss: 0.0353 - val_accuracy: 0.9845\n",
            "Epoch 2496/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0323 - val_accuracy: 0.9845\n",
            "Epoch 2497/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0104 - accuracy: 0.9985 - val_loss: 0.0323 - val_accuracy: 0.9845\n",
            "Epoch 2498/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.0344 - val_accuracy: 0.9876\n",
            "Epoch 2499/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0356 - val_accuracy: 0.9845\n",
            "Epoch 2500/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0355 - val_accuracy: 0.9845\n",
            "Epoch 2501/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0329 - val_accuracy: 0.9845\n",
            "Epoch 2502/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0328 - val_accuracy: 0.9845\n",
            "Epoch 2503/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0102 - accuracy: 0.9985 - val_loss: 0.0344 - val_accuracy: 0.9876\n",
            "Epoch 2504/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0399 - val_accuracy: 0.9845\n",
            "Epoch 2505/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.0378 - val_accuracy: 0.9845\n",
            "Epoch 2506/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0335 - val_accuracy: 0.9845\n",
            "Epoch 2507/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9876\n",
            "Epoch 2508/3500\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.0125 - accuracy: 0.9954 - val_loss: 0.0358 - val_accuracy: 0.9845\n",
            "Epoch 2509/3500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0503 - val_accuracy: 0.9814\n",
            "Epoch 2510/3500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0141 - accuracy: 0.9954 - val_loss: 0.0460 - val_accuracy: 0.9814\n",
            "Epoch 2511/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0113 - accuracy: 0.9954 - val_loss: 0.0332 - val_accuracy: 0.9845\n",
            "Epoch 2512/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0365 - val_accuracy: 0.9845\n",
            "Epoch 2513/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0146 - accuracy: 0.9939 - val_loss: 0.0349 - val_accuracy: 0.9876\n",
            "Epoch 2514/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0485 - val_accuracy: 0.9814\n",
            "Epoch 2515/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.0469 - val_accuracy: 0.9845\n",
            "Epoch 2516/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0120 - accuracy: 0.9954 - val_loss: 0.0343 - val_accuracy: 0.9876\n",
            "Epoch 2517/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0343 - val_accuracy: 0.9845\n",
            "Epoch 2518/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0117 - accuracy: 0.9969 - val_loss: 0.0362 - val_accuracy: 0.9845\n",
            "Epoch 2519/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.0471 - val_accuracy: 0.9845\n",
            "Epoch 2520/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0125 - accuracy: 0.9954 - val_loss: 0.0366 - val_accuracy: 0.9845\n",
            "Epoch 2521/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0367 - val_accuracy: 0.9845\n",
            "Epoch 2522/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0158 - accuracy: 0.9923 - val_loss: 0.0329 - val_accuracy: 0.9845\n",
            "Epoch 2523/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.0359 - val_accuracy: 0.9845\n",
            "Epoch 2524/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0395 - val_accuracy: 0.9845\n",
            "Epoch 2525/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.0394 - val_accuracy: 0.9845\n",
            "Epoch 2526/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0096 - accuracy: 0.9969 - val_loss: 0.0367 - val_accuracy: 0.9845\n",
            "Epoch 2527/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0347 - val_accuracy: 0.9845\n",
            "Epoch 2528/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0339 - val_accuracy: 0.9876\n",
            "Epoch 2529/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0359 - val_accuracy: 0.9845\n",
            "Epoch 2530/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0385 - val_accuracy: 0.9845\n",
            "Epoch 2531/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0359 - val_accuracy: 0.9845\n",
            "Epoch 2532/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0356 - val_accuracy: 0.9845\n",
            "Epoch 2533/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0406 - val_accuracy: 0.9845\n",
            "Epoch 2534/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.0415 - val_accuracy: 0.9845\n",
            "Epoch 2535/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0091 - accuracy: 0.9954 - val_loss: 0.0346 - val_accuracy: 0.9876\n",
            "Epoch 2536/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.0341 - val_accuracy: 0.9845\n",
            "Epoch 2537/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0102 - accuracy: 0.9985 - val_loss: 0.0349 - val_accuracy: 0.9876\n",
            "Epoch 2538/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.0343 - val_accuracy: 0.9845\n",
            "Epoch 2539/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9845\n",
            "Epoch 2540/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.0352 - val_accuracy: 0.9845\n",
            "Epoch 2541/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0417 - val_accuracy: 0.9845\n",
            "Epoch 2542/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0095 - accuracy: 0.9954 - val_loss: 0.0422 - val_accuracy: 0.9845\n",
            "Epoch 2543/3500\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.0095 - accuracy: 0.9954 - val_loss: 0.0364 - val_accuracy: 0.9845\n",
            "Epoch 2544/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 0.0345 - val_accuracy: 0.9876\n",
            "Epoch 2545/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0408 - val_accuracy: 0.9845\n",
            "Epoch 2546/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0099 - accuracy: 0.9969 - val_loss: 0.0453 - val_accuracy: 0.9845\n",
            "Epoch 2547/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0109 - accuracy: 0.9954 - val_loss: 0.0369 - val_accuracy: 0.9845\n",
            "Epoch 2548/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0339 - val_accuracy: 0.9876\n",
            "Epoch 2549/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0352 - val_accuracy: 0.9845\n",
            "Epoch 2550/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.0374 - val_accuracy: 0.9845\n",
            "Epoch 2551/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0385 - val_accuracy: 0.9845\n",
            "Epoch 2552/3500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.0089 - accuracy: 0.9969 - val_loss: 0.0373 - val_accuracy: 0.9845\n",
            "Epoch 2553/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0347 - val_accuracy: 0.9845\n",
            "Epoch 2554/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0339 - val_accuracy: 0.9876\n",
            "Epoch 2555/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0335 - val_accuracy: 0.9876\n",
            "Epoch 2556/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.0338 - val_accuracy: 0.9876\n",
            "Epoch 2557/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0357 - val_accuracy: 0.9845\n",
            "Epoch 2558/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0093 - accuracy: 0.9969 - val_loss: 0.0365 - val_accuracy: 0.9845\n",
            "Epoch 2559/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.0336 - val_accuracy: 0.9876\n",
            "Epoch 2560/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.0339 - val_accuracy: 0.9876\n",
            "Epoch 2561/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0373 - val_accuracy: 0.9845\n",
            "Epoch 2562/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.0355 - val_accuracy: 0.9845\n",
            "Epoch 2563/3500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0332 - val_accuracy: 0.9845\n",
            "Epoch 2564/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.0333 - val_accuracy: 0.9876\n",
            "Epoch 2565/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0399 - val_accuracy: 0.9845\n",
            "Epoch 2566/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0104 - accuracy: 0.9954 - val_loss: 0.0429 - val_accuracy: 0.9845\n",
            "Epoch 2567/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0109 - accuracy: 0.9954 - val_loss: 0.0338 - val_accuracy: 0.9876\n",
            "Epoch 2568/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0324 - val_accuracy: 0.9845\n",
            "Epoch 2569/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0347 - val_accuracy: 0.9845\n",
            "Epoch 2570/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0381 - val_accuracy: 0.9845\n",
            "Epoch 2571/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0093 - accuracy: 0.9954 - val_loss: 0.0372 - val_accuracy: 0.9845\n",
            "Epoch 2572/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0326 - val_accuracy: 0.9845\n",
            "Epoch 2573/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0128 - accuracy: 0.9954 - val_loss: 0.0326 - val_accuracy: 0.9876\n",
            "Epoch 2574/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.0493 - val_accuracy: 0.9814\n",
            "Epoch 2575/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0488 - val_accuracy: 0.9814\n",
            "Epoch 2576/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 0.0348 - val_accuracy: 0.9845\n",
            "Epoch 2577/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0319 - val_accuracy: 0.9876\n",
            "Epoch 2578/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0124 - accuracy: 0.9954 - val_loss: 0.0344 - val_accuracy: 0.9845\n",
            "Epoch 2579/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.0596 - val_accuracy: 0.9845\n",
            "Epoch 2580/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0186 - accuracy: 0.9954 - val_loss: 0.0486 - val_accuracy: 0.9814\n",
            "Epoch 2581/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0114 - accuracy: 0.9969 - val_loss: 0.0332 - val_accuracy: 0.9814\n",
            "Epoch 2582/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0147 - accuracy: 0.9939 - val_loss: 0.0337 - val_accuracy: 0.9814\n",
            "Epoch 2583/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0616 - val_accuracy: 0.9845\n",
            "Epoch 2584/3500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0207 - accuracy: 0.9939 - val_loss: 0.0682 - val_accuracy: 0.9845\n",
            "Epoch 2585/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0245 - accuracy: 0.9954 - val_loss: 0.0430 - val_accuracy: 0.9814\n",
            "Epoch 2586/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.0339 - val_accuracy: 0.9876\n",
            "Epoch 2587/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0138 - accuracy: 0.9923 - val_loss: 0.0322 - val_accuracy: 0.9845\n",
            "Epoch 2588/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0105 - accuracy: 0.9954 - val_loss: 0.0413 - val_accuracy: 0.9845\n",
            "Epoch 2589/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0121 - accuracy: 0.9954 - val_loss: 0.0449 - val_accuracy: 0.9845\n",
            "Epoch 2590/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0106 - accuracy: 0.9954 - val_loss: 0.0362 - val_accuracy: 0.9876\n",
            "Epoch 2591/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0108 - accuracy: 0.9969 - val_loss: 0.0376 - val_accuracy: 0.9845\n",
            "Epoch 2592/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0098 - accuracy: 0.9969 - val_loss: 0.0467 - val_accuracy: 0.9814\n",
            "Epoch 2593/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.0521 - val_accuracy: 0.9814\n",
            "Epoch 2594/3500\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0110 - accuracy: 0.9954 - val_loss: 0.0427 - val_accuracy: 0.9845\n",
            "Epoch 2595/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0343 - val_accuracy: 0.9814\n",
            "Epoch 2596/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.0343 - val_accuracy: 0.9814\n",
            "Epoch 2597/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0103 - accuracy: 0.9969 - val_loss: 0.0348 - val_accuracy: 0.9814\n",
            "Epoch 2598/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0383 - val_accuracy: 0.9814\n",
            "Epoch 2599/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0093 - accuracy: 0.9969 - val_loss: 0.0412 - val_accuracy: 0.9845\n",
            "Epoch 2600/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0097 - accuracy: 0.9954 - val_loss: 0.0367 - val_accuracy: 0.9845\n",
            "Epoch 2601/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.0340 - val_accuracy: 0.9876\n",
            "Epoch 2602/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0334 - val_accuracy: 0.9876\n",
            "Epoch 2603/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.0345 - val_accuracy: 0.9876\n",
            "Epoch 2604/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0382 - val_accuracy: 0.9845\n",
            "Epoch 2605/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0391 - val_accuracy: 0.9845\n",
            "Epoch 2606/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0372 - val_accuracy: 0.9845\n",
            "Epoch 2607/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0361 - val_accuracy: 0.9845\n",
            "Epoch 2608/3500\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.0349 - val_accuracy: 0.9876\n",
            "Epoch 2609/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0347 - val_accuracy: 0.9876\n",
            "Epoch 2610/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0366 - val_accuracy: 0.9845\n",
            "Epoch 2611/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0363 - val_accuracy: 0.9845\n",
            "Epoch 2612/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0350 - val_accuracy: 0.9876\n",
            "Epoch 2613/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0365 - val_accuracy: 0.9845\n",
            "Epoch 2614/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0399 - val_accuracy: 0.9845\n",
            "Epoch 2615/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0087 - accuracy: 0.9969 - val_loss: 0.0416 - val_accuracy: 0.9845\n",
            "Epoch 2616/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0358 - val_accuracy: 0.9845\n",
            "Epoch 2617/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0338 - val_accuracy: 0.9876\n",
            "Epoch 2618/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.0368 - val_accuracy: 0.9845\n",
            "Epoch 2619/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0455 - val_accuracy: 0.9845\n",
            "Epoch 2620/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0112 - accuracy: 0.9954 - val_loss: 0.0445 - val_accuracy: 0.9845\n",
            "Epoch 2621/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0109 - accuracy: 0.9954 - val_loss: 0.0344 - val_accuracy: 0.9814\n",
            "Epoch 2622/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0331 - val_accuracy: 0.9845\n",
            "Epoch 2623/3500\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.0105 - accuracy: 0.9969 - val_loss: 0.0322 - val_accuracy: 0.9845\n",
            "Epoch 2624/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0349 - val_accuracy: 0.9845\n",
            "Epoch 2625/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0397 - val_accuracy: 0.9845\n",
            "Epoch 2626/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0102 - accuracy: 0.9954 - val_loss: 0.0384 - val_accuracy: 0.9845\n",
            "Epoch 2627/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0093 - accuracy: 0.9954 - val_loss: 0.0319 - val_accuracy: 0.9845\n",
            "Epoch 2628/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0317 - val_accuracy: 0.9876\n",
            "Epoch 2629/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0106 - accuracy: 0.9969 - val_loss: 0.0318 - val_accuracy: 0.9876\n",
            "Epoch 2630/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0342 - val_accuracy: 0.9845\n",
            "Epoch 2631/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0366 - val_accuracy: 0.9845\n",
            "Epoch 2632/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0361 - val_accuracy: 0.9845\n",
            "Epoch 2633/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0337 - val_accuracy: 0.9876\n",
            "Epoch 2634/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0320 - val_accuracy: 0.9876\n",
            "Epoch 2635/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0318 - val_accuracy: 0.9876\n",
            "Epoch 2636/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0321 - val_accuracy: 0.9876\n",
            "Epoch 2637/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.0342 - val_accuracy: 0.9845\n",
            "Epoch 2638/3500\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.0370 - val_accuracy: 0.9845\n",
            "Epoch 2639/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0345 - val_accuracy: 0.9845\n",
            "Epoch 2640/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.0330 - val_accuracy: 0.9876\n",
            "Epoch 2641/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9845\n",
            "Epoch 2642/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0373 - val_accuracy: 0.9845\n",
            "Epoch 2643/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0350 - val_accuracy: 0.9845\n",
            "Epoch 2644/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0326 - val_accuracy: 0.9876\n",
            "Epoch 2645/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0326 - val_accuracy: 0.9876\n",
            "Epoch 2646/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0334 - val_accuracy: 0.9876\n",
            "Epoch 2647/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0338 - val_accuracy: 0.9876\n",
            "Epoch 2648/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0333 - val_accuracy: 0.9876\n",
            "Epoch 2649/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9876\n",
            "Epoch 2650/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.0351 - val_accuracy: 0.9845\n",
            "Epoch 2651/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0329 - val_accuracy: 0.9876\n",
            "Epoch 2652/3500\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0332 - val_accuracy: 0.9876\n",
            "Epoch 2653/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0355 - val_accuracy: 0.9845\n",
            "Epoch 2654/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.0371 - val_accuracy: 0.9845\n",
            "Epoch 2655/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0088 - accuracy: 0.9969 - val_loss: 0.0368 - val_accuracy: 0.9845\n",
            "Epoch 2656/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0335 - val_accuracy: 0.9876\n",
            "Epoch 2657/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0318 - val_accuracy: 0.9876\n",
            "Epoch 2658/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.0324 - val_accuracy: 0.9876\n",
            "Epoch 2659/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0402 - val_accuracy: 0.9845\n",
            "Epoch 2660/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0091 - accuracy: 0.9969 - val_loss: 0.0450 - val_accuracy: 0.9845\n",
            "Epoch 2661/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0106 - accuracy: 0.9954 - val_loss: 0.0383 - val_accuracy: 0.9845\n",
            "Epoch 2662/3500\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0326 - val_accuracy: 0.9876\n",
            "Epoch 2663/3500\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0319 - val_accuracy: 0.9876\n",
            "Epoch 2664/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0338 - val_accuracy: 0.9876\n",
            "Epoch 2665/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0350 - val_accuracy: 0.9845\n",
            "Epoch 2666/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0336 - val_accuracy: 0.9876\n",
            "Epoch 2667/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0315 - val_accuracy: 0.9876\n",
            "Epoch 2668/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0314 - val_accuracy: 0.9876\n",
            "Epoch 2669/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0360 - val_accuracy: 0.9845\n",
            "Epoch 2670/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.0378 - val_accuracy: 0.9845\n",
            "Epoch 2671/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.0332 - val_accuracy: 0.9876\n",
            "Epoch 2672/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0324 - val_accuracy: 0.9907\n",
            "Epoch 2673/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0115 - accuracy: 0.9985 - val_loss: 0.0335 - val_accuracy: 0.9876\n",
            "Epoch 2674/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0405 - val_accuracy: 0.9845\n",
            "Epoch 2675/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0095 - accuracy: 0.9954 - val_loss: 0.0372 - val_accuracy: 0.9845\n",
            "Epoch 2676/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0334 - val_accuracy: 0.9876\n",
            "Epoch 2677/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0324 - val_accuracy: 0.9876\n",
            "Epoch 2678/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0316 - val_accuracy: 0.9876\n",
            "Epoch 2679/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0309 - val_accuracy: 0.9876\n",
            "Epoch 2680/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0333 - val_accuracy: 0.9876\n",
            "Epoch 2681/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0362 - val_accuracy: 0.9845\n",
            "Epoch 2682/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0328 - val_accuracy: 0.9876\n",
            "Epoch 2683/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0321 - val_accuracy: 0.9876\n",
            "Epoch 2684/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0330 - val_accuracy: 0.9876\n",
            "Epoch 2685/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0328 - val_accuracy: 0.9876\n",
            "Epoch 2686/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0343 - val_accuracy: 0.9876\n",
            "Epoch 2687/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0331 - val_accuracy: 0.9876\n",
            "Epoch 2688/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0313 - val_accuracy: 0.9876\n",
            "Epoch 2689/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.0334 - val_accuracy: 0.9876\n",
            "Epoch 2690/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0389 - val_accuracy: 0.9845\n",
            "Epoch 2691/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0092 - accuracy: 0.9954 - val_loss: 0.0384 - val_accuracy: 0.9845\n",
            "Epoch 2692/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0315 - val_accuracy: 0.9876\n",
            "Epoch 2693/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.0321 - val_accuracy: 0.9876\n",
            "Epoch 2694/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0105 - accuracy: 0.9969 - val_loss: 0.0342 - val_accuracy: 0.9845\n",
            "Epoch 2695/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0483 - val_accuracy: 0.9814\n",
            "Epoch 2696/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.0418 - val_accuracy: 0.9845\n",
            "Epoch 2697/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0092 - accuracy: 0.9969 - val_loss: 0.0306 - val_accuracy: 0.9907\n",
            "Epoch 2698/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.0304 - val_accuracy: 0.9907\n",
            "Epoch 2699/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.0403 - val_accuracy: 0.9845\n",
            "Epoch 2700/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0102 - accuracy: 0.9954 - val_loss: 0.0495 - val_accuracy: 0.9814\n",
            "Epoch 2701/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0139 - accuracy: 0.9954 - val_loss: 0.0398 - val_accuracy: 0.9845\n",
            "Epoch 2702/3500\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.0100 - accuracy: 0.9954 - val_loss: 0.0309 - val_accuracy: 0.9876\n",
            "Epoch 2703/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0306 - val_accuracy: 0.9845\n",
            "Epoch 2704/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.0328 - val_accuracy: 0.9876\n",
            "Epoch 2705/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0342 - val_accuracy: 0.9845\n",
            "Epoch 2706/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0334 - val_accuracy: 0.9876\n",
            "Epoch 2707/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0345 - val_accuracy: 0.9876\n",
            "Epoch 2708/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0348 - val_accuracy: 0.9845\n",
            "Epoch 2709/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0351 - val_accuracy: 0.9845\n",
            "Epoch 2710/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0369 - val_accuracy: 0.9845\n",
            "Epoch 2711/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0336 - val_accuracy: 0.9876\n",
            "Epoch 2712/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0345 - val_accuracy: 0.9876\n",
            "Epoch 2713/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0136 - accuracy: 0.9939 - val_loss: 0.0337 - val_accuracy: 0.9876\n",
            "Epoch 2714/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0409 - val_accuracy: 0.9845\n",
            "Epoch 2715/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.0416 - val_accuracy: 0.9845\n",
            "Epoch 2716/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0095 - accuracy: 0.9954 - val_loss: 0.0343 - val_accuracy: 0.9876\n",
            "Epoch 2717/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0333 - val_accuracy: 0.9845\n",
            "Epoch 2718/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.0351 - val_accuracy: 0.9876\n",
            "Epoch 2719/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0400 - val_accuracy: 0.9845\n",
            "Epoch 2720/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0091 - accuracy: 0.9954 - val_loss: 0.0400 - val_accuracy: 0.9845\n",
            "Epoch 2721/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0092 - accuracy: 0.9954 - val_loss: 0.0348 - val_accuracy: 0.9876\n",
            "Epoch 2722/3500\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0330 - val_accuracy: 0.9876\n",
            "Epoch 2723/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0347 - val_accuracy: 0.9876\n",
            "Epoch 2724/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0376 - val_accuracy: 0.9845\n",
            "Epoch 2725/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0085 - accuracy: 0.9969 - val_loss: 0.0358 - val_accuracy: 0.9845\n",
            "Epoch 2726/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0325 - val_accuracy: 0.9876\n",
            "Epoch 2727/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0331 - val_accuracy: 0.9876\n",
            "Epoch 2728/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0382 - val_accuracy: 0.9845\n",
            "Epoch 2729/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0095 - accuracy: 0.9954 - val_loss: 0.0360 - val_accuracy: 0.9845\n",
            "Epoch 2730/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0317 - val_accuracy: 0.9876\n",
            "Epoch 2731/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.0327 - val_accuracy: 0.9876\n",
            "Epoch 2732/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.0362 - val_accuracy: 0.9845\n",
            "Epoch 2733/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0342 - val_accuracy: 0.9876\n",
            "Epoch 2734/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0335 - val_accuracy: 0.9876\n",
            "Epoch 2735/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0351 - val_accuracy: 0.9845\n",
            "Epoch 2736/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0353 - val_accuracy: 0.9845\n",
            "Epoch 2737/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0326 - val_accuracy: 0.9876\n",
            "Epoch 2738/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0316 - val_accuracy: 0.9876\n",
            "Epoch 2739/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0330 - val_accuracy: 0.9876\n",
            "Epoch 2740/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0395 - val_accuracy: 0.9845\n",
            "Epoch 2741/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0095 - accuracy: 0.9954 - val_loss: 0.0382 - val_accuracy: 0.9845\n",
            "Epoch 2742/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0317 - val_accuracy: 0.9876\n",
            "Epoch 2743/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0311 - val_accuracy: 0.9876\n",
            "Epoch 2744/3500\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0358 - val_accuracy: 0.9845\n",
            "Epoch 2745/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0093 - accuracy: 0.9954 - val_loss: 0.0342 - val_accuracy: 0.9876\n",
            "Epoch 2746/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0308 - val_accuracy: 0.9907\n",
            "Epoch 2747/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.0309 - val_accuracy: 0.9876\n",
            "Epoch 2748/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0390 - val_accuracy: 0.9845\n",
            "Epoch 2749/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.0412 - val_accuracy: 0.9845\n",
            "Epoch 2750/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0098 - accuracy: 0.9954 - val_loss: 0.0322 - val_accuracy: 0.9876\n",
            "Epoch 2751/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0308 - val_accuracy: 0.9876\n",
            "Epoch 2752/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0334 - val_accuracy: 0.9876\n",
            "Epoch 2753/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0344 - val_accuracy: 0.9845\n",
            "Epoch 2754/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0334 - val_accuracy: 0.9876\n",
            "Epoch 2755/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0324 - val_accuracy: 0.9876\n",
            "Epoch 2756/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0313 - val_accuracy: 0.9876\n",
            "Epoch 2757/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0305 - val_accuracy: 0.9876\n",
            "Epoch 2758/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0307 - val_accuracy: 0.9876\n",
            "Epoch 2759/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0337 - val_accuracy: 0.9876\n",
            "Epoch 2760/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0353 - val_accuracy: 0.9845\n",
            "Epoch 2761/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0348 - val_accuracy: 0.9845\n",
            "Epoch 2762/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0330 - val_accuracy: 0.9876\n",
            "Epoch 2763/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0318 - val_accuracy: 0.9876\n",
            "Epoch 2764/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.0342 - val_accuracy: 0.9876\n",
            "Epoch 2765/3500\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0424 - val_accuracy: 0.9845\n",
            "Epoch 2766/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0101 - accuracy: 0.9954 - val_loss: 0.0392 - val_accuracy: 0.9845\n",
            "Epoch 2767/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0315 - val_accuracy: 0.9876\n",
            "Epoch 2768/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0110 - accuracy: 0.9985 - val_loss: 0.0312 - val_accuracy: 0.9876\n",
            "Epoch 2769/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0461 - val_accuracy: 0.9814\n",
            "Epoch 2770/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0117 - accuracy: 0.9954 - val_loss: 0.0524 - val_accuracy: 0.9814\n",
            "Epoch 2771/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.0390 - val_accuracy: 0.9845\n",
            "Epoch 2772/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0087 - accuracy: 0.9954 - val_loss: 0.0303 - val_accuracy: 0.9907\n",
            "Epoch 2773/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.0308 - val_accuracy: 0.9907\n",
            "Epoch 2774/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0402 - val_accuracy: 0.9845\n",
            "Epoch 2775/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0514 - val_accuracy: 0.9814\n",
            "Epoch 2776/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0125 - accuracy: 0.9954 - val_loss: 0.0388 - val_accuracy: 0.9845\n",
            "Epoch 2777/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0348 - val_accuracy: 0.9876\n",
            "Epoch 2778/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0136 - accuracy: 0.9954 - val_loss: 0.0360 - val_accuracy: 0.9845\n",
            "Epoch 2779/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0528 - val_accuracy: 0.9814\n",
            "Epoch 2780/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.0488 - val_accuracy: 0.9814\n",
            "Epoch 2781/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0108 - accuracy: 0.9954 - val_loss: 0.0322 - val_accuracy: 0.9845\n",
            "Epoch 2782/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0107 - accuracy: 0.9954 - val_loss: 0.0372 - val_accuracy: 0.9845\n",
            "Epoch 2783/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0139 - accuracy: 0.9954 - val_loss: 0.0383 - val_accuracy: 0.9845\n",
            "Epoch 2784/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0087 - accuracy: 0.9954 - val_loss: 0.0565 - val_accuracy: 0.9814\n",
            "Epoch 2785/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0174 - accuracy: 0.9954 - val_loss: 0.0463 - val_accuracy: 0.9814\n",
            "Epoch 2786/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0111 - accuracy: 0.9969 - val_loss: 0.0312 - val_accuracy: 0.9845\n",
            "Epoch 2787/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0110 - accuracy: 0.9954 - val_loss: 0.0333 - val_accuracy: 0.9845\n",
            "Epoch 2788/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0109 - accuracy: 0.9969 - val_loss: 0.0393 - val_accuracy: 0.9845\n",
            "Epoch 2789/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0097 - accuracy: 0.9954 - val_loss: 0.0586 - val_accuracy: 0.9814\n",
            "Epoch 2790/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0170 - accuracy: 0.9954 - val_loss: 0.0474 - val_accuracy: 0.9814\n",
            "Epoch 2791/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0108 - accuracy: 0.9954 - val_loss: 0.0300 - val_accuracy: 0.9876\n",
            "Epoch 2792/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0335 - val_accuracy: 0.9876\n",
            "Epoch 2793/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0164 - accuracy: 0.9923 - val_loss: 0.0342 - val_accuracy: 0.9845\n",
            "Epoch 2794/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0088 - accuracy: 0.9969 - val_loss: 0.0455 - val_accuracy: 0.9814\n",
            "Epoch 2795/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0110 - accuracy: 0.9954 - val_loss: 0.0369 - val_accuracy: 0.9845\n",
            "Epoch 2796/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.0307 - val_accuracy: 0.9876\n",
            "Epoch 2797/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0091 - accuracy: 0.9969 - val_loss: 0.0334 - val_accuracy: 0.9845\n",
            "Epoch 2798/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0454 - val_accuracy: 0.9814\n",
            "Epoch 2799/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0114 - accuracy: 0.9954 - val_loss: 0.0416 - val_accuracy: 0.9845\n",
            "Epoch 2800/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0100 - accuracy: 0.9954 - val_loss: 0.0314 - val_accuracy: 0.9876\n",
            "Epoch 2801/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0310 - val_accuracy: 0.9845\n",
            "Epoch 2802/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0340 - val_accuracy: 0.9845\n",
            "Epoch 2803/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0412 - val_accuracy: 0.9845\n",
            "Epoch 2804/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0097 - accuracy: 0.9954 - val_loss: 0.0375 - val_accuracy: 0.9845\n",
            "Epoch 2805/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.0310 - val_accuracy: 0.9876\n",
            "Epoch 2806/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0305 - val_accuracy: 0.9907\n",
            "Epoch 2807/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.0318 - val_accuracy: 0.9876\n",
            "Epoch 2808/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0392 - val_accuracy: 0.9845\n",
            "Epoch 2809/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0096 - accuracy: 0.9954 - val_loss: 0.0345 - val_accuracy: 0.9845\n",
            "Epoch 2810/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.0305 - val_accuracy: 0.9876\n",
            "Epoch 2811/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0315 - val_accuracy: 0.9876\n",
            "Epoch 2812/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0333 - val_accuracy: 0.9876\n",
            "Epoch 2813/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0343 - val_accuracy: 0.9876\n",
            "Epoch 2814/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0353 - val_accuracy: 0.9845\n",
            "Epoch 2815/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0358 - val_accuracy: 0.9845\n",
            "Epoch 2816/3500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0339 - val_accuracy: 0.9876\n",
            "Epoch 2817/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0331 - val_accuracy: 0.9876\n",
            "Epoch 2818/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0344 - val_accuracy: 0.9876\n",
            "Epoch 2819/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0348 - val_accuracy: 0.9845\n",
            "Epoch 2820/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0335 - val_accuracy: 0.9876\n",
            "Epoch 2821/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0370 - val_accuracy: 0.9845\n",
            "Epoch 2822/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0395 - val_accuracy: 0.9845\n",
            "Epoch 2823/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9845\n",
            "Epoch 2824/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0329 - val_accuracy: 0.9876\n",
            "Epoch 2825/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0341 - val_accuracy: 0.9876\n",
            "Epoch 2826/3500\n",
            "2/2 [==============================] - 0s 76ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0350 - val_accuracy: 0.9845\n",
            "Epoch 2827/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9845\n",
            "Epoch 2828/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0323 - val_accuracy: 0.9876\n",
            "Epoch 2829/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0309 - val_accuracy: 0.9876\n",
            "Epoch 2830/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0312 - val_accuracy: 0.9876\n",
            "Epoch 2831/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0337 - val_accuracy: 0.9876\n",
            "Epoch 2832/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0400 - val_accuracy: 0.9845\n",
            "Epoch 2833/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0090 - accuracy: 0.9954 - val_loss: 0.0380 - val_accuracy: 0.9845\n",
            "Epoch 2834/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0320 - val_accuracy: 0.9876\n",
            "Epoch 2835/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0312 - val_accuracy: 0.9876\n",
            "Epoch 2836/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0317 - val_accuracy: 0.9876\n",
            "Epoch 2837/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0348 - val_accuracy: 0.9845\n",
            "Epoch 2838/3500\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0375 - val_accuracy: 0.9845\n",
            "Epoch 2839/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0329 - val_accuracy: 0.9876\n",
            "Epoch 2840/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0312 - val_accuracy: 0.9876\n",
            "Epoch 2841/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0340 - val_accuracy: 0.9876\n",
            "Epoch 2842/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0389 - val_accuracy: 0.9845\n",
            "Epoch 2843/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0091 - accuracy: 0.9954 - val_loss: 0.0340 - val_accuracy: 0.9876\n",
            "Epoch 2844/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0311 - val_accuracy: 0.9876\n",
            "Epoch 2845/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0312 - val_accuracy: 0.9876\n",
            "Epoch 2846/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0352 - val_accuracy: 0.9845\n",
            "Epoch 2847/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0090 - accuracy: 0.9954 - val_loss: 0.0373 - val_accuracy: 0.9845\n",
            "Epoch 2848/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0312 - val_accuracy: 0.9876\n",
            "Epoch 2849/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.0313 - val_accuracy: 0.9907\n",
            "Epoch 2850/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0345 - val_accuracy: 0.9845\n",
            "Epoch 2851/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0090 - accuracy: 0.9969 - val_loss: 0.0372 - val_accuracy: 0.9845\n",
            "Epoch 2852/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0310 - val_accuracy: 0.9876\n",
            "Epoch 2853/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0085 - accuracy: 0.9969 - val_loss: 0.0308 - val_accuracy: 0.9876\n",
            "Epoch 2854/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0360 - val_accuracy: 0.9845\n",
            "Epoch 2855/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0429 - val_accuracy: 0.9845\n",
            "Epoch 2856/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0100 - accuracy: 0.9954 - val_loss: 0.0389 - val_accuracy: 0.9845\n",
            "Epoch 2857/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0088 - accuracy: 0.9969 - val_loss: 0.0315 - val_accuracy: 0.9876\n",
            "Epoch 2858/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0298 - val_accuracy: 0.9876\n",
            "Epoch 2859/3500\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0303 - val_accuracy: 0.9876\n",
            "Epoch 2860/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0331 - val_accuracy: 0.9876\n",
            "Epoch 2861/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0323 - val_accuracy: 0.9876\n",
            "Epoch 2862/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0297 - val_accuracy: 0.9876\n",
            "Epoch 2863/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0307 - val_accuracy: 0.9876\n",
            "Epoch 2864/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0341 - val_accuracy: 0.9876\n",
            "Epoch 2865/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0364 - val_accuracy: 0.9845\n",
            "Epoch 2866/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0348 - val_accuracy: 0.9845\n",
            "Epoch 2867/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0334 - val_accuracy: 0.9876\n",
            "Epoch 2868/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0329 - val_accuracy: 0.9876\n",
            "Epoch 2869/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0361 - val_accuracy: 0.9845\n",
            "Epoch 2870/3500\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.0088 - accuracy: 0.9969 - val_loss: 0.0332 - val_accuracy: 0.9876\n",
            "Epoch 2871/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0308 - val_accuracy: 0.9907\n",
            "Epoch 2872/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0103 - accuracy: 0.9969 - val_loss: 0.0328 - val_accuracy: 0.9876\n",
            "Epoch 2873/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0115 - accuracy: 0.9954 - val_loss: 0.0376 - val_accuracy: 0.9845\n",
            "Epoch 2874/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0304 - val_accuracy: 0.9907\n",
            "Epoch 2875/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0118 - accuracy: 0.9954 - val_loss: 0.0307 - val_accuracy: 0.9876\n",
            "Epoch 2876/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0457 - val_accuracy: 0.9814\n",
            "Epoch 2877/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.0389 - val_accuracy: 0.9845\n",
            "Epoch 2878/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0102 - accuracy: 0.9954 - val_loss: 0.0294 - val_accuracy: 0.9907\n",
            "Epoch 2879/3500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.0299 - val_accuracy: 0.9876\n",
            "Epoch 2880/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0416 - val_accuracy: 0.9845\n",
            "Epoch 2881/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0124 - accuracy: 0.9954 - val_loss: 0.0375 - val_accuracy: 0.9845\n",
            "Epoch 2882/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.0297 - val_accuracy: 0.9907\n",
            "Epoch 2883/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0091 - accuracy: 0.9969 - val_loss: 0.0299 - val_accuracy: 0.9907\n",
            "Epoch 2884/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.0314 - val_accuracy: 0.9876\n",
            "Epoch 2885/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0328 - val_accuracy: 0.9876\n",
            "Epoch 2886/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0348 - val_accuracy: 0.9845\n",
            "Epoch 2887/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0365 - val_accuracy: 0.9845\n",
            "Epoch 2888/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0341 - val_accuracy: 0.9876\n",
            "Epoch 2889/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0322 - val_accuracy: 0.9876\n",
            "Epoch 2890/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0348 - val_accuracy: 0.9876\n",
            "Epoch 2891/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0385 - val_accuracy: 0.9845\n",
            "Epoch 2892/3500\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0352 - val_accuracy: 0.9845\n",
            "Epoch 2893/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0322 - val_accuracy: 0.9876\n",
            "Epoch 2894/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9876\n",
            "Epoch 2895/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0377 - val_accuracy: 0.9845\n",
            "Epoch 2896/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0327 - val_accuracy: 0.9876\n",
            "Epoch 2897/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0311 - val_accuracy: 0.9907\n",
            "Epoch 2898/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.0322 - val_accuracy: 0.9876\n",
            "Epoch 2899/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0385 - val_accuracy: 0.9845\n",
            "Epoch 2900/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0411 - val_accuracy: 0.9845\n",
            "Epoch 2901/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0095 - accuracy: 0.9954 - val_loss: 0.0336 - val_accuracy: 0.9876\n",
            "Epoch 2902/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0306 - val_accuracy: 0.9907\n",
            "Epoch 2903/3500\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.0309 - val_accuracy: 0.9876\n",
            "Epoch 2904/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0386 - val_accuracy: 0.9845\n",
            "Epoch 2905/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0091 - accuracy: 0.9954 - val_loss: 0.0393 - val_accuracy: 0.9845\n",
            "Epoch 2906/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0084 - accuracy: 0.9969 - val_loss: 0.0316 - val_accuracy: 0.9876\n",
            "Epoch 2907/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0308 - val_accuracy: 0.9907\n",
            "Epoch 2908/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0328 - val_accuracy: 0.9876\n",
            "Epoch 2909/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0397 - val_accuracy: 0.9845\n",
            "Epoch 2910/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0090 - accuracy: 0.9954 - val_loss: 0.0388 - val_accuracy: 0.9845\n",
            "Epoch 2911/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0091 - accuracy: 0.9954 - val_loss: 0.0320 - val_accuracy: 0.9876\n",
            "Epoch 2912/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0299 - val_accuracy: 0.9876\n",
            "Epoch 2913/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0296 - val_accuracy: 0.9876\n",
            "Epoch 2914/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0303 - val_accuracy: 0.9876\n",
            "Epoch 2915/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0341 - val_accuracy: 0.9876\n",
            "Epoch 2916/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0379 - val_accuracy: 0.9845\n",
            "Epoch 2917/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0342 - val_accuracy: 0.9876\n",
            "Epoch 2918/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0306 - val_accuracy: 0.9876\n",
            "Epoch 2919/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0303 - val_accuracy: 0.9876\n",
            "Epoch 2920/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0300 - val_accuracy: 0.9876\n",
            "Epoch 2921/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0312 - val_accuracy: 0.9876\n",
            "Epoch 2922/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0380 - val_accuracy: 0.9845\n",
            "Epoch 2923/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0379 - val_accuracy: 0.9845\n",
            "Epoch 2924/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0323 - val_accuracy: 0.9876\n",
            "Epoch 2925/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0319 - val_accuracy: 0.9876\n",
            "Epoch 2926/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0340 - val_accuracy: 0.9876\n",
            "Epoch 2927/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0390 - val_accuracy: 0.9845\n",
            "Epoch 2928/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0354 - val_accuracy: 0.9876\n",
            "Epoch 2929/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0327 - val_accuracy: 0.9876\n",
            "Epoch 2930/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0321 - val_accuracy: 0.9876\n",
            "Epoch 2931/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0344 - val_accuracy: 0.9876\n",
            "Epoch 2932/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0381 - val_accuracy: 0.9845\n",
            "Epoch 2933/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0363 - val_accuracy: 0.9845\n",
            "Epoch 2934/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0330 - val_accuracy: 0.9876\n",
            "Epoch 2935/3500\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0340 - val_accuracy: 0.9876\n",
            "Epoch 2936/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0352 - val_accuracy: 0.9876\n",
            "Epoch 2937/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0323 - val_accuracy: 0.9876\n",
            "Epoch 2938/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0313 - val_accuracy: 0.9876\n",
            "Epoch 2939/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0335 - val_accuracy: 0.9876\n",
            "Epoch 2940/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0365 - val_accuracy: 0.9845\n",
            "Epoch 2941/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0342 - val_accuracy: 0.9876\n",
            "Epoch 2942/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0312 - val_accuracy: 0.9876\n",
            "Epoch 2943/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0311 - val_accuracy: 0.9907\n",
            "Epoch 2944/3500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0327 - val_accuracy: 0.9876\n",
            "Epoch 2945/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0391 - val_accuracy: 0.9845\n",
            "Epoch 2946/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0085 - accuracy: 0.9969 - val_loss: 0.0338 - val_accuracy: 0.9876\n",
            "Epoch 2947/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.0312 - val_accuracy: 0.9876\n",
            "Epoch 2948/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0327 - val_accuracy: 0.9876\n",
            "Epoch 2949/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0326 - val_accuracy: 0.9876\n",
            "Epoch 2950/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0320 - val_accuracy: 0.9876\n",
            "Epoch 2951/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0318 - val_accuracy: 0.9876\n",
            "Epoch 2952/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0338 - val_accuracy: 0.9876\n",
            "Epoch 2953/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0329 - val_accuracy: 0.9876\n",
            "Epoch 2954/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0312 - val_accuracy: 0.9876\n",
            "Epoch 2955/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0328 - val_accuracy: 0.9876\n",
            "Epoch 2956/3500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0387 - val_accuracy: 0.9845\n",
            "Epoch 2957/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0344 - val_accuracy: 0.9845\n",
            "Epoch 2958/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0310 - val_accuracy: 0.9876\n",
            "Epoch 2959/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0310 - val_accuracy: 0.9876\n",
            "Epoch 2960/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0351 - val_accuracy: 0.9845\n",
            "Epoch 2961/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0414 - val_accuracy: 0.9845\n",
            "Epoch 2962/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0091 - accuracy: 0.9954 - val_loss: 0.0334 - val_accuracy: 0.9876\n",
            "Epoch 2963/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0308 - val_accuracy: 0.9876\n",
            "Epoch 2964/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0328 - val_accuracy: 0.9876\n",
            "Epoch 2965/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0360 - val_accuracy: 0.9845\n",
            "Epoch 2966/3500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0329 - val_accuracy: 0.9876\n",
            "Epoch 2967/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0305 - val_accuracy: 0.9876\n",
            "Epoch 2968/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0313 - val_accuracy: 0.9876\n",
            "Epoch 2969/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0359 - val_accuracy: 0.9845\n",
            "Epoch 2970/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0324 - val_accuracy: 0.9876\n",
            "Epoch 2971/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0313 - val_accuracy: 0.9907\n",
            "Epoch 2972/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0097 - accuracy: 0.9985 - val_loss: 0.0315 - val_accuracy: 0.9876\n",
            "Epoch 2973/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0390 - val_accuracy: 0.9845\n",
            "Epoch 2974/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0412 - val_accuracy: 0.9845\n",
            "Epoch 2975/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0088 - accuracy: 0.9954 - val_loss: 0.0320 - val_accuracy: 0.9876\n",
            "Epoch 2976/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0316 - val_accuracy: 0.9907\n",
            "Epoch 2977/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0105 - accuracy: 0.9969 - val_loss: 0.0307 - val_accuracy: 0.9876\n",
            "Epoch 2978/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0377 - val_accuracy: 0.9845\n",
            "Epoch 2979/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0087 - accuracy: 0.9954 - val_loss: 0.0361 - val_accuracy: 0.9845\n",
            "Epoch 2980/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0312 - val_accuracy: 0.9876\n",
            "Epoch 2981/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0315 - val_accuracy: 0.9876\n",
            "Epoch 2982/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0335 - val_accuracy: 0.9876\n",
            "Epoch 2983/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0343 - val_accuracy: 0.9876\n",
            "Epoch 2984/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0350 - val_accuracy: 0.9845\n",
            "Epoch 2985/3500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0318 - val_accuracy: 0.9876\n",
            "Epoch 2986/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0311 - val_accuracy: 0.9876\n",
            "Epoch 2987/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0362 - val_accuracy: 0.9845\n",
            "Epoch 2988/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0451 - val_accuracy: 0.9845\n",
            "Epoch 2989/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0106 - accuracy: 0.9954 - val_loss: 0.0371 - val_accuracy: 0.9845\n",
            "Epoch 2990/3500\n",
            "2/2 [==============================] - 0s 76ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0300 - val_accuracy: 0.9907\n",
            "Epoch 2991/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0298 - val_accuracy: 0.9907\n",
            "Epoch 2992/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0359 - val_accuracy: 0.9845\n",
            "Epoch 2993/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0470 - val_accuracy: 0.9814\n",
            "Epoch 2994/3500\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.0118 - accuracy: 0.9954 - val_loss: 0.0418 - val_accuracy: 0.9845\n",
            "Epoch 2995/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0091 - accuracy: 0.9969 - val_loss: 0.0306 - val_accuracy: 0.9876\n",
            "Epoch 2996/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0306 - val_accuracy: 0.9907\n",
            "Epoch 2997/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.0325 - val_accuracy: 0.9876\n",
            "Epoch 2998/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0080 - accuracy: 0.9969 - val_loss: 0.0425 - val_accuracy: 0.9845\n",
            "Epoch 2999/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0100 - accuracy: 0.9954 - val_loss: 0.0337 - val_accuracy: 0.9845\n",
            "Epoch 3000/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0294 - val_accuracy: 0.9907\n",
            "Epoch 3001/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0294 - val_accuracy: 0.9876\n",
            "Epoch 3002/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0349 - val_accuracy: 0.9845\n",
            "Epoch 3003/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0436 - val_accuracy: 0.9845\n",
            "Epoch 3004/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0108 - accuracy: 0.9954 - val_loss: 0.0337 - val_accuracy: 0.9845\n",
            "Epoch 3005/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0317 - val_accuracy: 0.9907\n",
            "Epoch 3006/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0134 - accuracy: 0.9939 - val_loss: 0.0294 - val_accuracy: 0.9876\n",
            "Epoch 3007/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0339 - val_accuracy: 0.9845\n",
            "Epoch 3008/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0318 - val_accuracy: 0.9876\n",
            "Epoch 3009/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0297 - val_accuracy: 0.9907\n",
            "Epoch 3010/3500\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0302 - val_accuracy: 0.9876\n",
            "Epoch 3011/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0333 - val_accuracy: 0.9876\n",
            "Epoch 3012/3500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0393 - val_accuracy: 0.9845\n",
            "Epoch 3013/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9845\n",
            "Epoch 3014/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0311 - val_accuracy: 0.9876\n",
            "Epoch 3015/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0320 - val_accuracy: 0.9876\n",
            "Epoch 3016/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0324 - val_accuracy: 0.9876\n",
            "Epoch 3017/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0314 - val_accuracy: 0.9876\n",
            "Epoch 3018/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0339 - val_accuracy: 0.9876\n",
            "Epoch 3019/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0364 - val_accuracy: 0.9845\n",
            "Epoch 3020/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0319 - val_accuracy: 0.9876\n",
            "Epoch 3021/3500\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0322 - val_accuracy: 0.9876\n",
            "Epoch 3022/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0404 - val_accuracy: 0.9845\n",
            "Epoch 3023/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0446 - val_accuracy: 0.9845\n",
            "Epoch 3024/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0091 - accuracy: 0.9954 - val_loss: 0.0391 - val_accuracy: 0.9845\n",
            "Epoch 3025/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0341 - val_accuracy: 0.9876\n",
            "Epoch 3026/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0329 - val_accuracy: 0.9876\n",
            "Epoch 3027/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0339 - val_accuracy: 0.9876\n",
            "Epoch 3028/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0348 - val_accuracy: 0.9845\n",
            "Epoch 3029/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0315 - val_accuracy: 0.9876\n",
            "Epoch 3030/3500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0303 - val_accuracy: 0.9876\n",
            "Epoch 3031/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0305 - val_accuracy: 0.9876\n",
            "Epoch 3032/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0320 - val_accuracy: 0.9876\n",
            "Epoch 3033/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0383 - val_accuracy: 0.9845\n",
            "Epoch 3034/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0410 - val_accuracy: 0.9845\n",
            "Epoch 3035/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0083 - accuracy: 0.9969 - val_loss: 0.0337 - val_accuracy: 0.9876\n",
            "Epoch 3036/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0309 - val_accuracy: 0.9876\n",
            "Epoch 3037/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0332 - val_accuracy: 0.9876\n",
            "Epoch 3038/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0334 - val_accuracy: 0.9876\n",
            "Epoch 3039/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0309 - val_accuracy: 0.9876\n",
            "Epoch 3040/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0311 - val_accuracy: 0.9876\n",
            "Epoch 3041/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0324 - val_accuracy: 0.9876\n",
            "Epoch 3042/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0317 - val_accuracy: 0.9876\n",
            "Epoch 3043/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0299 - val_accuracy: 0.9876\n",
            "Epoch 3044/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0297 - val_accuracy: 0.9876\n",
            "Epoch 3045/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0314 - val_accuracy: 0.9876\n",
            "Epoch 3046/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0383 - val_accuracy: 0.9845\n",
            "Epoch 3047/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0354 - val_accuracy: 0.9845\n",
            "Epoch 3048/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0294 - val_accuracy: 0.9907\n",
            "Epoch 3049/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0294 - val_accuracy: 0.9876\n",
            "Epoch 3050/3500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0337 - val_accuracy: 0.9876\n",
            "Epoch 3051/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0345 - val_accuracy: 0.9845\n",
            "Epoch 3052/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0317 - val_accuracy: 0.9876\n",
            "Epoch 3053/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0298 - val_accuracy: 0.9876\n",
            "Epoch 3054/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0303 - val_accuracy: 0.9907\n",
            "Epoch 3055/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0314 - val_accuracy: 0.9876\n",
            "Epoch 3056/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0366 - val_accuracy: 0.9845\n",
            "Epoch 3057/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0370 - val_accuracy: 0.9845\n",
            "Epoch 3058/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0329 - val_accuracy: 0.9876\n",
            "Epoch 3059/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.0362 - val_accuracy: 0.9845\n",
            "Epoch 3060/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0494 - val_accuracy: 0.9845\n",
            "Epoch 3061/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0113 - accuracy: 0.9954 - val_loss: 0.0407 - val_accuracy: 0.9845\n",
            "Epoch 3062/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0085 - accuracy: 0.9954 - val_loss: 0.0314 - val_accuracy: 0.9876\n",
            "Epoch 3063/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0314 - val_accuracy: 0.9876\n",
            "Epoch 3064/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.0321 - val_accuracy: 0.9876\n",
            "Epoch 3065/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0412 - val_accuracy: 0.9845\n",
            "Epoch 3066/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0089 - accuracy: 0.9954 - val_loss: 0.0389 - val_accuracy: 0.9845\n",
            "Epoch 3067/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.0339 - val_accuracy: 0.9876\n",
            "Epoch 3068/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0349 - val_accuracy: 0.9876\n",
            "Epoch 3069/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0373 - val_accuracy: 0.9845\n",
            "Epoch 3070/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0411 - val_accuracy: 0.9845\n",
            "Epoch 3071/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0411 - val_accuracy: 0.9845\n",
            "Epoch 3072/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0355 - val_accuracy: 0.9845\n",
            "Epoch 3073/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0333 - val_accuracy: 0.9876\n",
            "Epoch 3074/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0326 - val_accuracy: 0.9876\n",
            "Epoch 3075/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0317 - val_accuracy: 0.9876\n",
            "Epoch 3076/3500\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0314 - val_accuracy: 0.9876\n",
            "Epoch 3077/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0324 - val_accuracy: 0.9876\n",
            "Epoch 3078/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0330 - val_accuracy: 0.9876\n",
            "Epoch 3079/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0316 - val_accuracy: 0.9876\n",
            "Epoch 3080/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0329 - val_accuracy: 0.9876\n",
            "Epoch 3081/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0323 - val_accuracy: 0.9876\n",
            "Epoch 3082/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0310 - val_accuracy: 0.9876\n",
            "Epoch 3083/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0315 - val_accuracy: 0.9876\n",
            "Epoch 3084/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0374 - val_accuracy: 0.9845\n",
            "Epoch 3085/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0341 - val_accuracy: 0.9876\n",
            "Epoch 3086/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0312 - val_accuracy: 0.9876\n",
            "Epoch 3087/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0087 - accuracy: 0.9985 - val_loss: 0.0313 - val_accuracy: 0.9876\n",
            "Epoch 3088/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0347 - val_accuracy: 0.9845\n",
            "Epoch 3089/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0321 - val_accuracy: 0.9876\n",
            "Epoch 3090/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0323 - val_accuracy: 0.9876\n",
            "Epoch 3091/3500\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0333 - val_accuracy: 0.9876\n",
            "Epoch 3092/3500\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0338 - val_accuracy: 0.9876\n",
            "Epoch 3093/3500\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0335 - val_accuracy: 0.9876\n",
            "Epoch 3094/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0318 - val_accuracy: 0.9876\n",
            "Epoch 3095/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0333 - val_accuracy: 0.9876\n",
            "Epoch 3096/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0359 - val_accuracy: 0.9845\n",
            "Epoch 3097/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0343 - val_accuracy: 0.9876\n",
            "Epoch 3098/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0320 - val_accuracy: 0.9876\n",
            "Epoch 3099/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0315 - val_accuracy: 0.9876\n",
            "Epoch 3100/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0324 - val_accuracy: 0.9876\n",
            "Epoch 3101/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0343 - val_accuracy: 0.9876\n",
            "Epoch 3102/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0340 - val_accuracy: 0.9876\n",
            "Epoch 3103/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0314 - val_accuracy: 0.9876\n",
            "Epoch 3104/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0328 - val_accuracy: 0.9876\n",
            "Epoch 3105/3500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0397 - val_accuracy: 0.9845\n",
            "Epoch 3106/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0082 - accuracy: 0.9969 - val_loss: 0.0340 - val_accuracy: 0.9876\n",
            "Epoch 3107/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0307 - val_accuracy: 0.9876\n",
            "Epoch 3108/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0319 - val_accuracy: 0.9876\n",
            "Epoch 3109/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0342 - val_accuracy: 0.9876\n",
            "Epoch 3110/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0341 - val_accuracy: 0.9876\n",
            "Epoch 3111/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0330 - val_accuracy: 0.9876\n",
            "Epoch 3112/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0322 - val_accuracy: 0.9876\n",
            "Epoch 3113/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0331 - val_accuracy: 0.9876\n",
            "Epoch 3114/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0329 - val_accuracy: 0.9876\n",
            "Epoch 3115/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0311 - val_accuracy: 0.9876\n",
            "Epoch 3116/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0302 - val_accuracy: 0.9876\n",
            "Epoch 3117/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0309 - val_accuracy: 0.9876\n",
            "Epoch 3118/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9876\n",
            "Epoch 3119/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0353 - val_accuracy: 0.9845\n",
            "Epoch 3120/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0337 - val_accuracy: 0.9876\n",
            "Epoch 3121/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0317 - val_accuracy: 0.9876\n",
            "Epoch 3122/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0305 - val_accuracy: 0.9876\n",
            "Epoch 3123/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0301 - val_accuracy: 0.9876\n",
            "Epoch 3124/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0311 - val_accuracy: 0.9876\n",
            "Epoch 3125/3500\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0335 - val_accuracy: 0.9876\n",
            "Epoch 3126/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0341 - val_accuracy: 0.9876\n",
            "Epoch 3127/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0295 - val_accuracy: 0.9876\n",
            "Epoch 3128/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0291 - val_accuracy: 0.9876\n",
            "Epoch 3129/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0357 - val_accuracy: 0.9845\n",
            "Epoch 3130/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0340 - val_accuracy: 0.9876\n",
            "Epoch 3131/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0304 - val_accuracy: 0.9876\n",
            "Epoch 3132/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0326 - val_accuracy: 0.9876\n",
            "Epoch 3133/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0432 - val_accuracy: 0.9845\n",
            "Epoch 3134/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0095 - accuracy: 0.9954 - val_loss: 0.0381 - val_accuracy: 0.9845\n",
            "Epoch 3135/3500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0296 - val_accuracy: 0.9876\n",
            "Epoch 3136/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0295 - val_accuracy: 0.9907\n",
            "Epoch 3137/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0359 - val_accuracy: 0.9845\n",
            "Epoch 3138/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0082 - accuracy: 0.9969 - val_loss: 0.0414 - val_accuracy: 0.9845\n",
            "Epoch 3139/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0095 - accuracy: 0.9954 - val_loss: 0.0314 - val_accuracy: 0.9876\n",
            "Epoch 3140/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0297 - val_accuracy: 0.9907\n",
            "Epoch 3141/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0299 - val_accuracy: 0.9876\n",
            "Epoch 3142/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0334 - val_accuracy: 0.9876\n",
            "Epoch 3143/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0353 - val_accuracy: 0.9845\n",
            "Epoch 3144/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0329 - val_accuracy: 0.9876\n",
            "Epoch 3145/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0307 - val_accuracy: 0.9876\n",
            "Epoch 3146/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0317 - val_accuracy: 0.9876\n",
            "Epoch 3147/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0374 - val_accuracy: 0.9845\n",
            "Epoch 3148/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0375 - val_accuracy: 0.9845\n",
            "Epoch 3149/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0313 - val_accuracy: 0.9876\n",
            "Epoch 3150/3500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0311 - val_accuracy: 0.9907\n",
            "Epoch 3151/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0350 - val_accuracy: 0.9845\n",
            "Epoch 3152/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0435 - val_accuracy: 0.9845\n",
            "Epoch 3153/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0110 - accuracy: 0.9954 - val_loss: 0.0321 - val_accuracy: 0.9876\n",
            "Epoch 3154/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0363 - val_accuracy: 0.9876\n",
            "Epoch 3155/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0142 - accuracy: 0.9954 - val_loss: 0.0332 - val_accuracy: 0.9876\n",
            "Epoch 3156/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0080 - accuracy: 0.9969 - val_loss: 0.0450 - val_accuracy: 0.9845\n",
            "Epoch 3157/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0101 - accuracy: 0.9954 - val_loss: 0.0356 - val_accuracy: 0.9845\n",
            "Epoch 3158/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0311 - val_accuracy: 0.9907\n",
            "Epoch 3159/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.0314 - val_accuracy: 0.9876\n",
            "Epoch 3160/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0395 - val_accuracy: 0.9845\n",
            "Epoch 3161/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0087 - accuracy: 0.9954 - val_loss: 0.0368 - val_accuracy: 0.9845\n",
            "Epoch 3162/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0312 - val_accuracy: 0.9876\n",
            "Epoch 3163/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0348 - val_accuracy: 0.9845\n",
            "Epoch 3164/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0084 - accuracy: 0.9969 - val_loss: 0.0345 - val_accuracy: 0.9845\n",
            "Epoch 3165/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0306 - val_accuracy: 0.9876\n",
            "Epoch 3166/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0092 - accuracy: 0.9969 - val_loss: 0.0303 - val_accuracy: 0.9876\n",
            "Epoch 3167/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0387 - val_accuracy: 0.9845\n",
            "Epoch 3168/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0103 - accuracy: 0.9954 - val_loss: 0.0353 - val_accuracy: 0.9845\n",
            "Epoch 3169/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0342 - val_accuracy: 0.9845\n",
            "Epoch 3170/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0136 - accuracy: 0.9939 - val_loss: 0.0319 - val_accuracy: 0.9876\n",
            "Epoch 3171/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0491 - val_accuracy: 0.9814\n",
            "Epoch 3172/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0122 - accuracy: 0.9954 - val_loss: 0.0409 - val_accuracy: 0.9845\n",
            "Epoch 3173/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0087 - accuracy: 0.9969 - val_loss: 0.0295 - val_accuracy: 0.9876\n",
            "Epoch 3174/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0294 - val_accuracy: 0.9907\n",
            "Epoch 3175/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0343 - val_accuracy: 0.9845\n",
            "Epoch 3176/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0107 - accuracy: 0.9954 - val_loss: 0.0333 - val_accuracy: 0.9876\n",
            "Epoch 3177/3500\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0309 - val_accuracy: 0.9876\n",
            "Epoch 3178/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0091 - accuracy: 0.9969 - val_loss: 0.0342 - val_accuracy: 0.9845\n",
            "Epoch 3179/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0477 - val_accuracy: 0.9845\n",
            "Epoch 3180/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0319 - val_accuracy: 0.9876\n",
            "Epoch 3181/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.0289 - val_accuracy: 0.9876\n",
            "Epoch 3182/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0421 - val_accuracy: 0.9814\n",
            "Epoch 3183/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0125 - accuracy: 0.9954 - val_loss: 0.0372 - val_accuracy: 0.9814\n",
            "Epoch 3184/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0098 - accuracy: 0.9969 - val_loss: 0.0290 - val_accuracy: 0.9907\n",
            "Epoch 3185/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.0284 - val_accuracy: 0.9907\n",
            "Epoch 3186/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0398 - val_accuracy: 0.9845\n",
            "Epoch 3187/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0591 - val_accuracy: 0.9783\n",
            "Epoch 3188/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0127 - accuracy: 0.9954 - val_loss: 0.0486 - val_accuracy: 0.9814\n",
            "Epoch 3189/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0350 - val_accuracy: 0.9876\n",
            "Epoch 3190/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0322 - val_accuracy: 0.9907\n",
            "Epoch 3191/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0082 - accuracy: 0.9969 - val_loss: 0.0345 - val_accuracy: 0.9876\n",
            "Epoch 3192/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0356 - val_accuracy: 0.9845\n",
            "Epoch 3193/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0308 - val_accuracy: 0.9876\n",
            "Epoch 3194/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0291 - val_accuracy: 0.9907\n",
            "Epoch 3195/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0089 - accuracy: 0.9969 - val_loss: 0.0289 - val_accuracy: 0.9907\n",
            "Epoch 3196/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0303 - val_accuracy: 0.9876\n",
            "Epoch 3197/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0325 - val_accuracy: 0.9876\n",
            "Epoch 3198/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0318 - val_accuracy: 0.9876\n",
            "Epoch 3199/3500\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0312 - val_accuracy: 0.9876\n",
            "Epoch 3200/3500\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0320 - val_accuracy: 0.9876\n",
            "Epoch 3201/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0339 - val_accuracy: 0.9876\n",
            "Epoch 3202/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0371 - val_accuracy: 0.9845\n",
            "Epoch 3203/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0403 - val_accuracy: 0.9845\n",
            "Epoch 3204/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0372 - val_accuracy: 0.9845\n",
            "Epoch 3205/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0338 - val_accuracy: 0.9876\n",
            "Epoch 3206/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0336 - val_accuracy: 0.9876\n",
            "Epoch 3207/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0339 - val_accuracy: 0.9876\n",
            "Epoch 3208/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0367 - val_accuracy: 0.9845\n",
            "Epoch 3209/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0078 - accuracy: 0.9969 - val_loss: 0.0344 - val_accuracy: 0.9876\n",
            "Epoch 3210/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0316 - val_accuracy: 0.9876\n",
            "Epoch 3211/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.0367 - val_accuracy: 0.9845\n",
            "Epoch 3212/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0085 - accuracy: 0.9969 - val_loss: 0.0554 - val_accuracy: 0.9814\n",
            "Epoch 3213/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0138 - accuracy: 0.9954 - val_loss: 0.0395 - val_accuracy: 0.9845\n",
            "Epoch 3214/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0302 - val_accuracy: 0.9907\n",
            "Epoch 3215/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0124 - accuracy: 0.9969 - val_loss: 0.0308 - val_accuracy: 0.9876\n",
            "Epoch 3216/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0086 - accuracy: 0.9954 - val_loss: 0.0485 - val_accuracy: 0.9814\n",
            "Epoch 3217/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0109 - accuracy: 0.9954 - val_loss: 0.0377 - val_accuracy: 0.9845\n",
            "Epoch 3218/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0303 - val_accuracy: 0.9907\n",
            "Epoch 3219/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0083 - accuracy: 0.9969 - val_loss: 0.0306 - val_accuracy: 0.9845\n",
            "Epoch 3220/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0102 - accuracy: 0.9954 - val_loss: 0.0302 - val_accuracy: 0.9876\n",
            "Epoch 3221/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0353 - val_accuracy: 0.9845\n",
            "Epoch 3222/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0431 - val_accuracy: 0.9845\n",
            "Epoch 3223/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0100 - accuracy: 0.9954 - val_loss: 0.0376 - val_accuracy: 0.9845\n",
            "Epoch 3224/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0295 - val_accuracy: 0.9876\n",
            "Epoch 3225/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0076 - accuracy: 0.9969 - val_loss: 0.0290 - val_accuracy: 0.9876\n",
            "Epoch 3226/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0298 - val_accuracy: 0.9876\n",
            "Epoch 3227/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0297 - val_accuracy: 0.9876\n",
            "Epoch 3228/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0312 - val_accuracy: 0.9876\n",
            "Epoch 3229/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0310 - val_accuracy: 0.9876\n",
            "Epoch 3230/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0298 - val_accuracy: 0.9876\n",
            "Epoch 3231/3500\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0320 - val_accuracy: 0.9876\n",
            "Epoch 3232/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0338 - val_accuracy: 0.9876\n",
            "Epoch 3233/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0330 - val_accuracy: 0.9876\n",
            "Epoch 3234/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0325 - val_accuracy: 0.9876\n",
            "Epoch 3235/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0307 - val_accuracy: 0.9876\n",
            "Epoch 3236/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0304 - val_accuracy: 0.9876\n",
            "Epoch 3237/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0335 - val_accuracy: 0.9876\n",
            "Epoch 3238/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0355 - val_accuracy: 0.9845\n",
            "Epoch 3239/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0326 - val_accuracy: 0.9876\n",
            "Epoch 3240/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0304 - val_accuracy: 0.9876\n",
            "Epoch 3241/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0313 - val_accuracy: 0.9876\n",
            "Epoch 3242/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0344 - val_accuracy: 0.9876\n",
            "Epoch 3243/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0322 - val_accuracy: 0.9876\n",
            "Epoch 3244/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0299 - val_accuracy: 0.9876\n",
            "Epoch 3245/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0303 - val_accuracy: 0.9876\n",
            "Epoch 3246/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0318 - val_accuracy: 0.9876\n",
            "Epoch 3247/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0350 - val_accuracy: 0.9876\n",
            "Epoch 3248/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0344 - val_accuracy: 0.9876\n",
            "Epoch 3249/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0314 - val_accuracy: 0.9876\n",
            "Epoch 3250/3500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0327 - val_accuracy: 0.9876\n",
            "Epoch 3251/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0353 - val_accuracy: 0.9876\n",
            "Epoch 3252/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0350 - val_accuracy: 0.9876\n",
            "Epoch 3253/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0345 - val_accuracy: 0.9876\n",
            "Epoch 3254/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9876\n",
            "Epoch 3255/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0331 - val_accuracy: 0.9876\n",
            "Epoch 3256/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0306 - val_accuracy: 0.9876\n",
            "Epoch 3257/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0301 - val_accuracy: 0.9876\n",
            "Epoch 3258/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0317 - val_accuracy: 0.9876\n",
            "Epoch 3259/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0370 - val_accuracy: 0.9845\n",
            "Epoch 3260/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0374 - val_accuracy: 0.9845\n",
            "Epoch 3261/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0326 - val_accuracy: 0.9876\n",
            "Epoch 3262/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0302 - val_accuracy: 0.9876\n",
            "Epoch 3263/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0301 - val_accuracy: 0.9876\n",
            "Epoch 3264/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0331 - val_accuracy: 0.9876\n",
            "Epoch 3265/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0378 - val_accuracy: 0.9845\n",
            "Epoch 3266/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0347 - val_accuracy: 0.9876\n",
            "Epoch 3267/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0294 - val_accuracy: 0.9876\n",
            "Epoch 3268/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0297 - val_accuracy: 0.9907\n",
            "Epoch 3269/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0320 - val_accuracy: 0.9876\n",
            "Epoch 3270/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0352 - val_accuracy: 0.9845\n",
            "Epoch 3271/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0351 - val_accuracy: 0.9845\n",
            "Epoch 3272/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0331 - val_accuracy: 0.9876\n",
            "Epoch 3273/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0320 - val_accuracy: 0.9876\n",
            "Epoch 3274/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0311 - val_accuracy: 0.9876\n",
            "Epoch 3275/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0295 - val_accuracy: 0.9876\n",
            "Epoch 3276/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0315 - val_accuracy: 0.9876\n",
            "Epoch 3277/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0364 - val_accuracy: 0.9845\n",
            "Epoch 3278/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0079 - accuracy: 0.9969 - val_loss: 0.0325 - val_accuracy: 0.9876\n",
            "Epoch 3279/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0284 - val_accuracy: 0.9876\n",
            "Epoch 3280/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0291 - val_accuracy: 0.9876\n",
            "Epoch 3281/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0329 - val_accuracy: 0.9876\n",
            "Epoch 3282/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0307 - val_accuracy: 0.9876\n",
            "Epoch 3283/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0294 - val_accuracy: 0.9907\n",
            "Epoch 3284/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.0314 - val_accuracy: 0.9876\n",
            "Epoch 3285/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0382 - val_accuracy: 0.9845\n",
            "Epoch 3286/3500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0310 - val_accuracy: 0.9876\n",
            "Epoch 3287/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0289 - val_accuracy: 0.9907\n",
            "Epoch 3288/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0290 - val_accuracy: 0.9876\n",
            "Epoch 3289/3500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0297 - val_accuracy: 0.9876\n",
            "Epoch 3290/3500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0313 - val_accuracy: 0.9876\n",
            "Epoch 3291/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0344 - val_accuracy: 0.9876\n",
            "Epoch 3292/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0322 - val_accuracy: 0.9876\n",
            "Epoch 3293/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.0297 - val_accuracy: 0.9907\n",
            "Epoch 3294/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0301 - val_accuracy: 0.9907\n",
            "Epoch 3295/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0330 - val_accuracy: 0.9876\n",
            "Epoch 3296/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.0437 - val_accuracy: 0.9845\n",
            "Epoch 3297/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0091 - accuracy: 0.9954 - val_loss: 0.0403 - val_accuracy: 0.9845\n",
            "Epoch 3298/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0316 - val_accuracy: 0.9876\n",
            "Epoch 3299/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0338 - val_accuracy: 0.9845\n",
            "Epoch 3300/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0119 - accuracy: 0.9939 - val_loss: 0.0360 - val_accuracy: 0.9845\n",
            "Epoch 3301/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0472 - val_accuracy: 0.9845\n",
            "Epoch 3302/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0120 - accuracy: 0.9954 - val_loss: 0.0330 - val_accuracy: 0.9876\n",
            "Epoch 3303/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0087 - accuracy: 0.9969 - val_loss: 0.0337 - val_accuracy: 0.9876\n",
            "Epoch 3304/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0112 - accuracy: 0.9954 - val_loss: 0.0348 - val_accuracy: 0.9845\n",
            "Epoch 3305/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0550 - val_accuracy: 0.9845\n",
            "Epoch 3306/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.0431 - val_accuracy: 0.9814\n",
            "Epoch 3307/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0117 - accuracy: 0.9954 - val_loss: 0.0288 - val_accuracy: 0.9938\n",
            "Epoch 3308/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.0292 - val_accuracy: 0.9907\n",
            "Epoch 3309/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0414 - val_accuracy: 0.9845\n",
            "Epoch 3310/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0089 - accuracy: 0.9969 - val_loss: 0.0443 - val_accuracy: 0.9845\n",
            "Epoch 3311/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0335 - val_accuracy: 0.9876\n",
            "Epoch 3312/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0091 - accuracy: 0.9954 - val_loss: 0.0329 - val_accuracy: 0.9845\n",
            "Epoch 3313/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0088 - accuracy: 0.9969 - val_loss: 0.0407 - val_accuracy: 0.9845\n",
            "Epoch 3314/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0077 - accuracy: 0.9969 - val_loss: 0.0391 - val_accuracy: 0.9845\n",
            "Epoch 3315/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0311 - val_accuracy: 0.9845\n",
            "Epoch 3316/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0091 - accuracy: 0.9954 - val_loss: 0.0317 - val_accuracy: 0.9876\n",
            "Epoch 3317/3500\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 0.0477 - val_accuracy: 0.9814\n",
            "Epoch 3318/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0125 - accuracy: 0.9954 - val_loss: 0.0438 - val_accuracy: 0.9845\n",
            "Epoch 3319/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0108 - accuracy: 0.9954 - val_loss: 0.0294 - val_accuracy: 0.9907\n",
            "Epoch 3320/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0294 - val_accuracy: 0.9876\n",
            "Epoch 3321/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0410 - val_accuracy: 0.9845\n",
            "Epoch 3322/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0085 - accuracy: 0.9969 - val_loss: 0.0498 - val_accuracy: 0.9814\n",
            "Epoch 3323/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0104 - accuracy: 0.9954 - val_loss: 0.0375 - val_accuracy: 0.9845\n",
            "Epoch 3324/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0329 - val_accuracy: 0.9845\n",
            "Epoch 3325/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0118 - accuracy: 0.9954 - val_loss: 0.0315 - val_accuracy: 0.9876\n",
            "Epoch 3326/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0389 - val_accuracy: 0.9845\n",
            "Epoch 3327/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0078 - accuracy: 0.9969 - val_loss: 0.0402 - val_accuracy: 0.9845\n",
            "Epoch 3328/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0310 - val_accuracy: 0.9876\n",
            "Epoch 3329/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.0312 - val_accuracy: 0.9845\n",
            "Epoch 3330/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0100 - accuracy: 0.9954 - val_loss: 0.0306 - val_accuracy: 0.9876\n",
            "Epoch 3331/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0428 - val_accuracy: 0.9845\n",
            "Epoch 3332/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0087 - accuracy: 0.9969 - val_loss: 0.0414 - val_accuracy: 0.9845\n",
            "Epoch 3333/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0325 - val_accuracy: 0.9876\n",
            "Epoch 3334/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0323 - val_accuracy: 0.9876\n",
            "Epoch 3335/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0104 - accuracy: 0.9954 - val_loss: 0.0346 - val_accuracy: 0.9876\n",
            "Epoch 3336/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0074 - accuracy: 0.9954 - val_loss: 0.0445 - val_accuracy: 0.9845\n",
            "Epoch 3337/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0088 - accuracy: 0.9969 - val_loss: 0.0337 - val_accuracy: 0.9876\n",
            "Epoch 3338/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0300 - val_accuracy: 0.9907\n",
            "Epoch 3339/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0303 - val_accuracy: 0.9876\n",
            "Epoch 3340/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0354 - val_accuracy: 0.9845\n",
            "Epoch 3341/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0368 - val_accuracy: 0.9845\n",
            "Epoch 3342/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0082 - accuracy: 0.9969 - val_loss: 0.0316 - val_accuracy: 0.9876\n",
            "Epoch 3343/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0288 - val_accuracy: 0.9876\n",
            "Epoch 3344/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0290 - val_accuracy: 0.9907\n",
            "Epoch 3345/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0084 - accuracy: 0.9985 - val_loss: 0.0302 - val_accuracy: 0.9876\n",
            "Epoch 3346/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0423 - val_accuracy: 0.9845\n",
            "Epoch 3347/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0481 - val_accuracy: 0.9814\n",
            "Epoch 3348/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0092 - accuracy: 0.9954 - val_loss: 0.0386 - val_accuracy: 0.9845\n",
            "Epoch 3349/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0319 - val_accuracy: 0.9876\n",
            "Epoch 3350/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0312 - val_accuracy: 0.9876\n",
            "Epoch 3351/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0357 - val_accuracy: 0.9876\n",
            "Epoch 3352/3500\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0373 - val_accuracy: 0.9845\n",
            "Epoch 3353/3500\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0303 - val_accuracy: 0.9876\n",
            "Epoch 3354/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0293 - val_accuracy: 0.9907\n",
            "Epoch 3355/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0310 - val_accuracy: 0.9876\n",
            "Epoch 3356/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0343 - val_accuracy: 0.9876\n",
            "Epoch 3357/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9876\n",
            "Epoch 3358/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0322 - val_accuracy: 0.9876\n",
            "Epoch 3359/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0326 - val_accuracy: 0.9876\n",
            "Epoch 3360/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0356 - val_accuracy: 0.9845\n",
            "Epoch 3361/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0343 - val_accuracy: 0.9876\n",
            "Epoch 3362/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.0301 - val_accuracy: 0.9876\n",
            "Epoch 3363/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0305 - val_accuracy: 0.9876\n",
            "Epoch 3364/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0417 - val_accuracy: 0.9845\n",
            "Epoch 3365/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0088 - accuracy: 0.9954 - val_loss: 0.0414 - val_accuracy: 0.9845\n",
            "Epoch 3366/3500\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.0093 - accuracy: 0.9954 - val_loss: 0.0306 - val_accuracy: 0.9876\n",
            "Epoch 3367/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0283 - val_accuracy: 0.9907\n",
            "Epoch 3368/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0284 - val_accuracy: 0.9907\n",
            "Epoch 3369/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0345 - val_accuracy: 0.9876\n",
            "Epoch 3370/3500\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0405 - val_accuracy: 0.9845\n",
            "Epoch 3371/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0080 - accuracy: 0.9969 - val_loss: 0.0346 - val_accuracy: 0.9876\n",
            "Epoch 3372/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0304 - val_accuracy: 0.9876\n",
            "Epoch 3373/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0308 - val_accuracy: 0.9876\n",
            "Epoch 3374/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9876\n",
            "Epoch 3375/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0357 - val_accuracy: 0.9845\n",
            "Epoch 3376/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0310 - val_accuracy: 0.9876\n",
            "Epoch 3377/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.0296 - val_accuracy: 0.9907\n",
            "Epoch 3378/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0295 - val_accuracy: 0.9876\n",
            "Epoch 3379/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0319 - val_accuracy: 0.9876\n",
            "Epoch 3380/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0328 - val_accuracy: 0.9876\n",
            "Epoch 3381/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0332 - val_accuracy: 0.9876\n",
            "Epoch 3382/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0328 - val_accuracy: 0.9876\n",
            "Epoch 3383/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0325 - val_accuracy: 0.9876\n",
            "Epoch 3384/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0319 - val_accuracy: 0.9876\n",
            "Epoch 3385/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.0308 - val_accuracy: 0.9876\n",
            "Epoch 3386/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0306 - val_accuracy: 0.9876\n",
            "Epoch 3387/3500\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0319 - val_accuracy: 0.9876\n",
            "Epoch 3388/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.0383 - val_accuracy: 0.9845\n",
            "Epoch 3389/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0406 - val_accuracy: 0.9845\n",
            "Epoch 3390/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9876\n",
            "Epoch 3391/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0306 - val_accuracy: 0.9876\n",
            "Epoch 3392/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0306 - val_accuracy: 0.9876\n",
            "Epoch 3393/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0352 - val_accuracy: 0.9845\n",
            "Epoch 3394/3500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0372 - val_accuracy: 0.9845\n",
            "Epoch 3395/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0314 - val_accuracy: 0.9876\n",
            "Epoch 3396/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0313 - val_accuracy: 0.9876\n",
            "Epoch 3397/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0340 - val_accuracy: 0.9876\n",
            "Epoch 3398/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9876\n",
            "Epoch 3399/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0341 - val_accuracy: 0.9876\n",
            "Epoch 3400/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0334 - val_accuracy: 0.9876\n",
            "Epoch 3401/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0324 - val_accuracy: 0.9876\n",
            "Epoch 3402/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0312 - val_accuracy: 0.9876\n",
            "Epoch 3403/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0329 - val_accuracy: 0.9876\n",
            "Epoch 3404/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.0342 - val_accuracy: 0.9876\n",
            "Epoch 3405/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0337 - val_accuracy: 0.9876\n",
            "Epoch 3406/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0308 - val_accuracy: 0.9876\n",
            "Epoch 3407/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0305 - val_accuracy: 0.9876\n",
            "Epoch 3408/3500\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0355 - val_accuracy: 0.9845\n",
            "Epoch 3409/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0391 - val_accuracy: 0.9845\n",
            "Epoch 3410/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0369 - val_accuracy: 0.9845\n",
            "Epoch 3411/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0314 - val_accuracy: 0.9876\n",
            "Epoch 3412/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0297 - val_accuracy: 0.9876\n",
            "Epoch 3413/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0296 - val_accuracy: 0.9876\n",
            "Epoch 3414/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0310 - val_accuracy: 0.9876\n",
            "Epoch 3415/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0398 - val_accuracy: 0.9845\n",
            "Epoch 3416/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0081 - accuracy: 0.9954 - val_loss: 0.0356 - val_accuracy: 0.9876\n",
            "Epoch 3417/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0324 - val_accuracy: 0.9876\n",
            "Epoch 3418/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0115 - accuracy: 0.9954 - val_loss: 0.0317 - val_accuracy: 0.9876\n",
            "Epoch 3419/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0340 - val_accuracy: 0.9876\n",
            "Epoch 3420/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0307 - val_accuracy: 0.9876\n",
            "Epoch 3421/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0298 - val_accuracy: 0.9876\n",
            "Epoch 3422/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0315 - val_accuracy: 0.9876\n",
            "Epoch 3423/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.0334 - val_accuracy: 0.9876\n",
            "Epoch 3424/3500\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0320 - val_accuracy: 0.9876\n",
            "Epoch 3425/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0307 - val_accuracy: 0.9876\n",
            "Epoch 3426/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.0302 - val_accuracy: 0.9876\n",
            "Epoch 3427/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0299 - val_accuracy: 0.9876\n",
            "Epoch 3428/3500\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.0298 - val_accuracy: 0.9876\n",
            "Epoch 3429/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0307 - val_accuracy: 0.9876\n",
            "Epoch 3430/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0328 - val_accuracy: 0.9876\n",
            "Epoch 3431/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0379 - val_accuracy: 0.9845\n",
            "Epoch 3432/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0369 - val_accuracy: 0.9845\n",
            "Epoch 3433/3500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0321 - val_accuracy: 0.9876\n",
            "Epoch 3434/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0297 - val_accuracy: 0.9876\n",
            "Epoch 3435/3500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0301 - val_accuracy: 0.9876\n",
            "Epoch 3436/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0348 - val_accuracy: 0.9876\n",
            "Epoch 3437/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0354 - val_accuracy: 0.9845\n",
            "Epoch 3438/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0299 - val_accuracy: 0.9876\n",
            "Epoch 3439/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0312 - val_accuracy: 0.9876\n",
            "Epoch 3440/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.0359 - val_accuracy: 0.9845\n",
            "Epoch 3441/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0089 - accuracy: 0.9969 - val_loss: 0.0377 - val_accuracy: 0.9845\n",
            "Epoch 3442/3500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0296 - val_accuracy: 0.9907\n",
            "Epoch 3443/3500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0294 - val_accuracy: 0.9907\n",
            "Epoch 3444/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0331 - val_accuracy: 0.9876\n",
            "Epoch 3445/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0376 - val_accuracy: 0.9845\n",
            "Epoch 3446/3500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0342 - val_accuracy: 0.9876\n",
            "Epoch 3447/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0292 - val_accuracy: 0.9907\n",
            "Epoch 3448/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0076 - accuracy: 0.9985 - val_loss: 0.0294 - val_accuracy: 0.9876\n",
            "Epoch 3449/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0347 - val_accuracy: 0.9876\n",
            "Epoch 3450/3500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0349 - val_accuracy: 0.9876\n",
            "Epoch 3451/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0300 - val_accuracy: 0.9876\n",
            "Epoch 3452/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0299 - val_accuracy: 0.9876\n",
            "Epoch 3453/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0331 - val_accuracy: 0.9876\n",
            "Epoch 3454/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0365 - val_accuracy: 0.9845\n",
            "Epoch 3455/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0306 - val_accuracy: 0.9876\n",
            "Epoch 3456/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0282 - val_accuracy: 0.9876\n",
            "Epoch 3457/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0286 - val_accuracy: 0.9876\n",
            "Epoch 3458/3500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0305 - val_accuracy: 0.9876\n",
            "Epoch 3459/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.0307 - val_accuracy: 0.9876\n",
            "Epoch 3460/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0324 - val_accuracy: 0.9876\n",
            "Epoch 3461/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0348 - val_accuracy: 0.9876\n",
            "Epoch 3462/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0354 - val_accuracy: 0.9845\n",
            "Epoch 3463/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0328 - val_accuracy: 0.9876\n",
            "Epoch 3464/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0310 - val_accuracy: 0.9876\n",
            "Epoch 3465/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0301 - val_accuracy: 0.9876\n",
            "Epoch 3466/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0295 - val_accuracy: 0.9876\n",
            "Epoch 3467/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0294 - val_accuracy: 0.9876\n",
            "Epoch 3468/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0310 - val_accuracy: 0.9876\n",
            "Epoch 3469/3500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0324 - val_accuracy: 0.9876\n",
            "Epoch 3470/3500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0306 - val_accuracy: 0.9876\n",
            "Epoch 3471/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.0298 - val_accuracy: 0.9907\n",
            "Epoch 3472/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0319 - val_accuracy: 0.9876\n",
            "Epoch 3473/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0335 - val_accuracy: 0.9876\n",
            "Epoch 3474/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0316 - val_accuracy: 0.9876\n",
            "Epoch 3475/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0319 - val_accuracy: 0.9876\n",
            "Epoch 3476/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.0328 - val_accuracy: 0.9876\n",
            "Epoch 3477/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0316 - val_accuracy: 0.9876\n",
            "Epoch 3478/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0291 - val_accuracy: 0.9876\n",
            "Epoch 3479/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0311 - val_accuracy: 0.9876\n",
            "Epoch 3480/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0347 - val_accuracy: 0.9876\n",
            "Epoch 3481/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0308 - val_accuracy: 0.9876\n",
            "Epoch 3482/3500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0307 - val_accuracy: 0.9876\n",
            "Epoch 3483/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0344 - val_accuracy: 0.9876\n",
            "Epoch 3484/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.0374 - val_accuracy: 0.9845\n",
            "Epoch 3485/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0078 - accuracy: 0.9969 - val_loss: 0.0292 - val_accuracy: 0.9876\n",
            "Epoch 3486/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0079 - accuracy: 0.9969 - val_loss: 0.0302 - val_accuracy: 0.9876\n",
            "Epoch 3487/3500\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.0112 - accuracy: 0.9985 - val_loss: 0.0355 - val_accuracy: 0.9845\n",
            "Epoch 3488/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0363 - val_accuracy: 0.9845\n",
            "Epoch 3489/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0300 - val_accuracy: 0.9876\n",
            "Epoch 3490/3500\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0296 - val_accuracy: 0.9876\n",
            "Epoch 3491/3500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0347 - val_accuracy: 0.9876\n",
            "Epoch 3492/3500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0408 - val_accuracy: 0.9845\n",
            "Epoch 3493/3500\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0086 - accuracy: 0.9954 - val_loss: 0.0317 - val_accuracy: 0.9876\n",
            "Epoch 3494/3500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0286 - val_accuracy: 0.9876\n",
            "Epoch 3495/3500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0293 - val_accuracy: 0.9876\n",
            "Epoch 3496/3500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0343 - val_accuracy: 0.9876\n",
            "Epoch 3497/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0362 - val_accuracy: 0.9845\n",
            "Epoch 3498/3500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0345 - val_accuracy: 0.9876\n",
            "Epoch 3499/3500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.0330 - val_accuracy: 0.9876\n",
            "Epoch 3500/3500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0340 - val_accuracy: 0.9876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import  matplotlib.pyplot as plt \n",
        "y_vloss = history.history['val_loss']\n",
        "y_acc = history.history['accuracy']\n",
        "x_len = numpy.arange(len(y_acc))\n",
        "plt.plot(x_len, y_vloss, \"o\" , c='red', markersize=3)   # test셋 오차\n",
        "plt.plot(x_len, y_acc, \"o\", c='blue', markersize=3) # 학습셋 정확도\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "PHyhYWi39jVy",
        "outputId": "10f5d4b4-b5a7-4563-846c-d5c201323964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습의 자동중단 \n",
        "# 학습이 진행될수록 학습셋의 정확도는 올라가지만 과적합 때문에 테스트셋의 실험결과는 점점 나빠지게 됨\n",
        "# 테스트셋 오차가 줄지 않으면 학습을 멈추게하는 함수\n",
        "from keras.callbacks import EarlyStopping\n",
        "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience=100)\n",
        "model.fit(X,Y, validation_split=0.2, epochs=3500, batch_size=500, callbacks=[early_stopping_callback, checkpointer])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkkxYoGjunHj",
        "outputId": "0c745950-4ea3-4482-f609-b70bd6c66725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1.6167 - accuracy: 0.7680\n",
            "Epoch 00001: val_loss improved from inf to 1.12688, saving model to /content/model/01-1.1269.hdf5\n",
            "2/2 [==============================] - 1s 249ms/step - loss: 1.5881 - accuracy: 0.7603 - val_loss: 1.1269 - val_accuracy: 0.8103\n",
            "Epoch 2/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 1.2237 - accuracy: 0.7680\n",
            "Epoch 00002: val_loss improved from 1.12688 to 0.80641, saving model to /content/model/02-0.8064.hdf5\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 1.2044 - accuracy: 0.7603 - val_loss: 0.8064 - val_accuracy: 0.8103\n",
            "Epoch 3/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.7963 - accuracy: 0.7980\n",
            "Epoch 00003: val_loss improved from 0.80641 to 0.54107, saving model to /content/model/03-0.5411.hdf5\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.8477 - accuracy: 0.7603 - val_loss: 0.5411 - val_accuracy: 0.8103\n",
            "Epoch 4/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.6653 - accuracy: 0.7480\n",
            "Epoch 00004: val_loss did not improve from 0.54107\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.6088 - accuracy: 0.7564 - val_loss: 0.6415 - val_accuracy: 0.7077\n",
            "Epoch 5/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.6337 - accuracy: 0.6960\n",
            "Epoch 00005: val_loss did not improve from 0.54107\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.6826 - accuracy: 0.6372 - val_loss: 0.6061 - val_accuracy: 0.7128\n",
            "Epoch 6/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.6509 - accuracy: 0.6640\n",
            "Epoch 00006: val_loss improved from 0.54107 to 0.43954, saving model to /content/model/06-0.4395.hdf5\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.6061 - accuracy: 0.6949 - val_loss: 0.4395 - val_accuracy: 0.8000\n",
            "Epoch 7/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.5323 - accuracy: 0.7440\n",
            "Epoch 00007: val_loss improved from 0.43954 to 0.42897, saving model to /content/model/07-0.4290.hdf5\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.4872 - accuracy: 0.7551 - val_loss: 0.4290 - val_accuracy: 0.8103\n",
            "Epoch 8/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.5183 - accuracy: 0.7380\n",
            "Epoch 00008: val_loss did not improve from 0.42897\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.4886 - accuracy: 0.7590 - val_loss: 0.4418 - val_accuracy: 0.8103\n",
            "Epoch 9/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.4895 - accuracy: 0.7600\n",
            "Epoch 00009: val_loss did not improve from 0.42897\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.4972 - accuracy: 0.7590 - val_loss: 0.4360 - val_accuracy: 0.8103\n",
            "Epoch 10/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.5083 - accuracy: 0.7440\n",
            "Epoch 00010: val_loss improved from 0.42897 to 0.41056, saving model to /content/model/10-0.4106.hdf5\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.4832 - accuracy: 0.7590 - val_loss: 0.4106 - val_accuracy: 0.8103\n",
            "Epoch 11/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.4864 - accuracy: 0.7460\n",
            "Epoch 00011: val_loss improved from 0.41056 to 0.37657, saving model to /content/model/11-0.3766.hdf5\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.4493 - accuracy: 0.7628 - val_loss: 0.3766 - val_accuracy: 0.8103\n",
            "Epoch 12/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.4487 - accuracy: 0.7580\n",
            "Epoch 00012: val_loss improved from 0.37657 to 0.35110, saving model to /content/model/12-0.3511.hdf5\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.4091 - accuracy: 0.7744 - val_loss: 0.3511 - val_accuracy: 0.8410\n",
            "Epoch 13/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.4133 - accuracy: 0.7780\n",
            "Epoch 00013: val_loss improved from 0.35110 to 0.34793, saving model to /content/model/13-0.3479.hdf5\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.3805 - accuracy: 0.7923 - val_loss: 0.3479 - val_accuracy: 0.8410\n",
            "Epoch 14/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3661 - accuracy: 0.8200\n",
            "Epoch 00014: val_loss did not improve from 0.34793\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.3734 - accuracy: 0.8256 - val_loss: 0.3525 - val_accuracy: 0.8462\n",
            "Epoch 15/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3823 - accuracy: 0.8340\n",
            "Epoch 00015: val_loss improved from 0.34793 to 0.33764, saving model to /content/model/15-0.3376.hdf5\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.3690 - accuracy: 0.8462 - val_loss: 0.3376 - val_accuracy: 0.8667\n",
            "Epoch 16/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3472 - accuracy: 0.8660\n",
            "Epoch 00016: val_loss improved from 0.33764 to 0.31960, saving model to /content/model/16-0.3196.hdf5\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.3506 - accuracy: 0.8692 - val_loss: 0.3196 - val_accuracy: 0.8821\n",
            "Epoch 17/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3140 - accuracy: 0.8940\n",
            "Epoch 00017: val_loss improved from 0.31960 to 0.31086, saving model to /content/model/17-0.3109.hdf5\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.3320 - accuracy: 0.8795 - val_loss: 0.3109 - val_accuracy: 0.8974\n",
            "Epoch 18/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3170 - accuracy: 0.8820\n",
            "Epoch 00018: val_loss improved from 0.31086 to 0.30645, saving model to /content/model/18-0.3065.hdf5\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.3224 - accuracy: 0.8808 - val_loss: 0.3065 - val_accuracy: 0.9077\n",
            "Epoch 19/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3071 - accuracy: 0.8880\n",
            "Epoch 00019: val_loss improved from 0.30645 to 0.30130, saving model to /content/model/19-0.3013.hdf5\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.3156 - accuracy: 0.8923 - val_loss: 0.3013 - val_accuracy: 0.9128\n",
            "Epoch 20/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3208 - accuracy: 0.8920\n",
            "Epoch 00020: val_loss improved from 0.30130 to 0.29444, saving model to /content/model/20-0.2944.hdf5\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.3073 - accuracy: 0.9000 - val_loss: 0.2944 - val_accuracy: 0.9077\n",
            "Epoch 21/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.3005 - accuracy: 0.9040\n",
            "Epoch 00021: val_loss improved from 0.29444 to 0.28801, saving model to /content/model/21-0.2880.hdf5\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.2963 - accuracy: 0.9038 - val_loss: 0.2880 - val_accuracy: 0.9231\n",
            "Epoch 22/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2648 - accuracy: 0.9260\n",
            "Epoch 00022: val_loss improved from 0.28801 to 0.28310, saving model to /content/model/22-0.2831.hdf5\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.2866 - accuracy: 0.9115 - val_loss: 0.2831 - val_accuracy: 0.9282\n",
            "Epoch 23/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2818 - accuracy: 0.9140\n",
            "Epoch 00023: val_loss improved from 0.28310 to 0.27990, saving model to /content/model/23-0.2799.hdf5\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.2788 - accuracy: 0.9192 - val_loss: 0.2799 - val_accuracy: 0.9333\n",
            "Epoch 24/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2885 - accuracy: 0.9180\n",
            "Epoch 00024: val_loss improved from 0.27990 to 0.27658, saving model to /content/model/24-0.2766.hdf5\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.2738 - accuracy: 0.9231 - val_loss: 0.2766 - val_accuracy: 0.9231\n",
            "Epoch 25/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2240 - accuracy: 0.9400\n",
            "Epoch 00025: val_loss improved from 0.27658 to 0.27164, saving model to /content/model/25-0.2716.hdf5\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.2672 - accuracy: 0.9244 - val_loss: 0.2716 - val_accuracy: 0.9231\n",
            "Epoch 26/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2508 - accuracy: 0.9280\n",
            "Epoch 00026: val_loss improved from 0.27164 to 0.26661, saving model to /content/model/26-0.2666.hdf5\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.2593 - accuracy: 0.9256 - val_loss: 0.2666 - val_accuracy: 0.9231\n",
            "Epoch 27/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2678 - accuracy: 0.9200\n",
            "Epoch 00027: val_loss improved from 0.26661 to 0.26308, saving model to /content/model/27-0.2631.hdf5\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.2524 - accuracy: 0.9282 - val_loss: 0.2631 - val_accuracy: 0.9231\n",
            "Epoch 28/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2618 - accuracy: 0.9260\n",
            "Epoch 00028: val_loss improved from 0.26308 to 0.26153, saving model to /content/model/28-0.2615.hdf5\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.2454 - accuracy: 0.9282 - val_loss: 0.2615 - val_accuracy: 0.9282\n",
            "Epoch 29/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2362 - accuracy: 0.9380\n",
            "Epoch 00029: val_loss improved from 0.26153 to 0.26131, saving model to /content/model/29-0.2613.hdf5\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.2397 - accuracy: 0.9321 - val_loss: 0.2613 - val_accuracy: 0.9282\n",
            "Epoch 30/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2450 - accuracy: 0.9320\n",
            "Epoch 00030: val_loss did not improve from 0.26131\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.2345 - accuracy: 0.9397 - val_loss: 0.2645 - val_accuracy: 0.9231\n",
            "Epoch 31/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2277 - accuracy: 0.9460\n",
            "Epoch 00031: val_loss did not improve from 0.26131\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.2304 - accuracy: 0.9410 - val_loss: 0.2713 - val_accuracy: 0.9179\n",
            "Epoch 32/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2125 - accuracy: 0.9420\n",
            "Epoch 00032: val_loss did not improve from 0.26131\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.2297 - accuracy: 0.9346 - val_loss: 0.2774 - val_accuracy: 0.9179\n",
            "Epoch 33/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2122 - accuracy: 0.9440\n",
            "Epoch 00033: val_loss did not improve from 0.26131\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.2296 - accuracy: 0.9346 - val_loss: 0.2786 - val_accuracy: 0.9179\n",
            "Epoch 34/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2306 - accuracy: 0.9400\n",
            "Epoch 00034: val_loss did not improve from 0.26131\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.2286 - accuracy: 0.9359 - val_loss: 0.2756 - val_accuracy: 0.9179\n",
            "Epoch 35/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2028 - accuracy: 0.9500\n",
            "Epoch 00035: val_loss did not improve from 0.26131\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.2258 - accuracy: 0.9359 - val_loss: 0.2696 - val_accuracy: 0.9179\n",
            "Epoch 36/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2283 - accuracy: 0.9320\n",
            "Epoch 00036: val_loss did not improve from 0.26131\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.2225 - accuracy: 0.9359 - val_loss: 0.2622 - val_accuracy: 0.9231\n",
            "Epoch 37/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2054 - accuracy: 0.9340\n",
            "Epoch 00037: val_loss improved from 0.26131 to 0.25526, saving model to /content/model/37-0.2553.hdf5\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.2202 - accuracy: 0.9372 - val_loss: 0.2553 - val_accuracy: 0.9231\n",
            "Epoch 38/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2255 - accuracy: 0.9360\n",
            "Epoch 00038: val_loss improved from 0.25526 to 0.25023, saving model to /content/model/38-0.2502.hdf5\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.2183 - accuracy: 0.9410 - val_loss: 0.2502 - val_accuracy: 0.9231\n",
            "Epoch 39/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2585 - accuracy: 0.9240\n",
            "Epoch 00039: val_loss improved from 0.25023 to 0.24671, saving model to /content/model/39-0.2467.hdf5\n",
            "2/2 [==============================] - 0s 83ms/step - loss: 0.2171 - accuracy: 0.9410 - val_loss: 0.2467 - val_accuracy: 0.9231\n",
            "Epoch 40/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2426 - accuracy: 0.9280\n",
            "Epoch 00040: val_loss improved from 0.24671 to 0.24484, saving model to /content/model/40-0.2448.hdf5\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.2157 - accuracy: 0.9410 - val_loss: 0.2448 - val_accuracy: 0.9231\n",
            "Epoch 41/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2361 - accuracy: 0.9300\n",
            "Epoch 00041: val_loss improved from 0.24484 to 0.24441, saving model to /content/model/41-0.2444.hdf5\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.2142 - accuracy: 0.9397 - val_loss: 0.2444 - val_accuracy: 0.9231\n",
            "Epoch 42/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2232 - accuracy: 0.9320\n",
            "Epoch 00042: val_loss did not improve from 0.24441\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.2128 - accuracy: 0.9397 - val_loss: 0.2451 - val_accuracy: 0.9231\n",
            "Epoch 43/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2087 - accuracy: 0.9460\n",
            "Epoch 00043: val_loss did not improve from 0.24441\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.2112 - accuracy: 0.9423 - val_loss: 0.2463 - val_accuracy: 0.9231\n",
            "Epoch 44/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2159 - accuracy: 0.9380\n",
            "Epoch 00044: val_loss did not improve from 0.24441\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.2101 - accuracy: 0.9410 - val_loss: 0.2485 - val_accuracy: 0.9231\n",
            "Epoch 45/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2024 - accuracy: 0.9480\n",
            "Epoch 00045: val_loss did not improve from 0.24441\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.2087 - accuracy: 0.9410 - val_loss: 0.2492 - val_accuracy: 0.9231\n",
            "Epoch 46/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1859 - accuracy: 0.9460\n",
            "Epoch 00046: val_loss did not improve from 0.24441\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.2080 - accuracy: 0.9410 - val_loss: 0.2491 - val_accuracy: 0.9231\n",
            "Epoch 47/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2016 - accuracy: 0.9400\n",
            "Epoch 00047: val_loss did not improve from 0.24441\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.2068 - accuracy: 0.9410 - val_loss: 0.2483 - val_accuracy: 0.9231\n",
            "Epoch 48/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2073 - accuracy: 0.9400\n",
            "Epoch 00048: val_loss did not improve from 0.24441\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.2060 - accuracy: 0.9410 - val_loss: 0.2465 - val_accuracy: 0.9231\n",
            "Epoch 49/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2059 - accuracy: 0.9380\n",
            "Epoch 00049: val_loss improved from 0.24441 to 0.24355, saving model to /content/model/49-0.2436.hdf5\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.2049 - accuracy: 0.9410 - val_loss: 0.2436 - val_accuracy: 0.9231\n",
            "Epoch 50/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2111 - accuracy: 0.9400\n",
            "Epoch 00050: val_loss improved from 0.24355 to 0.24071, saving model to /content/model/50-0.2407.hdf5\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.2039 - accuracy: 0.9436 - val_loss: 0.2407 - val_accuracy: 0.9231\n",
            "Epoch 51/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2049 - accuracy: 0.9480\n",
            "Epoch 00051: val_loss improved from 0.24071 to 0.23790, saving model to /content/model/51-0.2379.hdf5\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.2031 - accuracy: 0.9423 - val_loss: 0.2379 - val_accuracy: 0.9231\n",
            "Epoch 52/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2340 - accuracy: 0.9340\n",
            "Epoch 00052: val_loss improved from 0.23790 to 0.23567, saving model to /content/model/52-0.2357.hdf5\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.2028 - accuracy: 0.9436 - val_loss: 0.2357 - val_accuracy: 0.9231\n",
            "Epoch 53/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2045 - accuracy: 0.9380\n",
            "Epoch 00053: val_loss improved from 0.23567 to 0.23455, saving model to /content/model/53-0.2345.hdf5\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.2023 - accuracy: 0.9423 - val_loss: 0.2345 - val_accuracy: 0.9231\n",
            "Epoch 54/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2003 - accuracy: 0.9380\n",
            "Epoch 00054: val_loss did not improve from 0.23455\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.2018 - accuracy: 0.9423 - val_loss: 0.2352 - val_accuracy: 0.9231\n",
            "Epoch 55/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2348 - accuracy: 0.9340\n",
            "Epoch 00055: val_loss did not improve from 0.23455\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.2009 - accuracy: 0.9436 - val_loss: 0.2366 - val_accuracy: 0.9231\n",
            "Epoch 56/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2143 - accuracy: 0.9380\n",
            "Epoch 00056: val_loss did not improve from 0.23455\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.2000 - accuracy: 0.9410 - val_loss: 0.2381 - val_accuracy: 0.9231\n",
            "Epoch 57/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1958 - accuracy: 0.9400\n",
            "Epoch 00057: val_loss did not improve from 0.23455\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.1995 - accuracy: 0.9410 - val_loss: 0.2398 - val_accuracy: 0.9231\n",
            "Epoch 58/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1813 - accuracy: 0.9500\n",
            "Epoch 00058: val_loss did not improve from 0.23455\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.1992 - accuracy: 0.9410 - val_loss: 0.2401 - val_accuracy: 0.9231\n",
            "Epoch 59/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1894 - accuracy: 0.9460\n",
            "Epoch 00059: val_loss did not improve from 0.23455\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1988 - accuracy: 0.9397 - val_loss: 0.2387 - val_accuracy: 0.9231\n",
            "Epoch 60/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2137 - accuracy: 0.9360\n",
            "Epoch 00060: val_loss did not improve from 0.23455\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.1982 - accuracy: 0.9410 - val_loss: 0.2365 - val_accuracy: 0.9231\n",
            "Epoch 61/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2088 - accuracy: 0.9420\n",
            "Epoch 00061: val_loss improved from 0.23455 to 0.23415, saving model to /content/model/61-0.2342.hdf5\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1975 - accuracy: 0.9410 - val_loss: 0.2342 - val_accuracy: 0.9231\n",
            "Epoch 62/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1981 - accuracy: 0.9440\n",
            "Epoch 00062: val_loss improved from 0.23415 to 0.23269, saving model to /content/model/62-0.2327.hdf5\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1970 - accuracy: 0.9423 - val_loss: 0.2327 - val_accuracy: 0.9231\n",
            "Epoch 63/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1957 - accuracy: 0.9420\n",
            "Epoch 00063: val_loss improved from 0.23269 to 0.23142, saving model to /content/model/63-0.2314.hdf5\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1968 - accuracy: 0.9436 - val_loss: 0.2314 - val_accuracy: 0.9231\n",
            "Epoch 64/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2011 - accuracy: 0.9380\n",
            "Epoch 00064: val_loss improved from 0.23142 to 0.23069, saving model to /content/model/64-0.2307.hdf5\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1964 - accuracy: 0.9436 - val_loss: 0.2307 - val_accuracy: 0.9231\n",
            "Epoch 65/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1826 - accuracy: 0.9480\n",
            "Epoch 00065: val_loss improved from 0.23069 to 0.23037, saving model to /content/model/65-0.2304.hdf5\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.1958 - accuracy: 0.9436 - val_loss: 0.2304 - val_accuracy: 0.9231\n",
            "Epoch 66/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2098 - accuracy: 0.9380\n",
            "Epoch 00066: val_loss did not improve from 0.23037\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1952 - accuracy: 0.9410 - val_loss: 0.2309 - val_accuracy: 0.9231\n",
            "Epoch 67/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2185 - accuracy: 0.9360\n",
            "Epoch 00067: val_loss did not improve from 0.23037\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.1948 - accuracy: 0.9410 - val_loss: 0.2312 - val_accuracy: 0.9231\n",
            "Epoch 68/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2013 - accuracy: 0.9360\n",
            "Epoch 00068: val_loss did not improve from 0.23037\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1943 - accuracy: 0.9410 - val_loss: 0.2310 - val_accuracy: 0.9231\n",
            "Epoch 69/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1969 - accuracy: 0.9460\n",
            "Epoch 00069: val_loss did not improve from 0.23037\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1938 - accuracy: 0.9410 - val_loss: 0.2304 - val_accuracy: 0.9231\n",
            "Epoch 70/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1769 - accuracy: 0.9520\n",
            "Epoch 00070: val_loss improved from 0.23037 to 0.22939, saving model to /content/model/70-0.2294.hdf5\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.1934 - accuracy: 0.9410 - val_loss: 0.2294 - val_accuracy: 0.9231\n",
            "Epoch 71/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2241 - accuracy: 0.9240\n",
            "Epoch 00071: val_loss improved from 0.22939 to 0.22858, saving model to /content/model/71-0.2286.hdf5\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.1928 - accuracy: 0.9410 - val_loss: 0.2286 - val_accuracy: 0.9231\n",
            "Epoch 72/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2006 - accuracy: 0.9360\n",
            "Epoch 00072: val_loss improved from 0.22858 to 0.22782, saving model to /content/model/72-0.2278.hdf5\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1924 - accuracy: 0.9410 - val_loss: 0.2278 - val_accuracy: 0.9231\n",
            "Epoch 73/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1934 - accuracy: 0.9460\n",
            "Epoch 00073: val_loss improved from 0.22782 to 0.22692, saving model to /content/model/73-0.2269.hdf5\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.1921 - accuracy: 0.9449 - val_loss: 0.2269 - val_accuracy: 0.9231\n",
            "Epoch 74/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2197 - accuracy: 0.9380\n",
            "Epoch 00074: val_loss improved from 0.22692 to 0.22689, saving model to /content/model/74-0.2269.hdf5\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1917 - accuracy: 0.9449 - val_loss: 0.2269 - val_accuracy: 0.9231\n",
            "Epoch 75/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2039 - accuracy: 0.9380\n",
            "Epoch 00075: val_loss did not improve from 0.22689\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1911 - accuracy: 0.9423 - val_loss: 0.2277 - val_accuracy: 0.9231\n",
            "Epoch 76/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1784 - accuracy: 0.9480\n",
            "Epoch 00076: val_loss did not improve from 0.22689\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1906 - accuracy: 0.9410 - val_loss: 0.2287 - val_accuracy: 0.9231\n",
            "Epoch 77/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1687 - accuracy: 0.9480\n",
            "Epoch 00077: val_loss did not improve from 0.22689\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1901 - accuracy: 0.9410 - val_loss: 0.2283 - val_accuracy: 0.9231\n",
            "Epoch 78/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1837 - accuracy: 0.9400\n",
            "Epoch 00078: val_loss did not improve from 0.22689\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.1898 - accuracy: 0.9410 - val_loss: 0.2271 - val_accuracy: 0.9231\n",
            "Epoch 79/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1825 - accuracy: 0.9460\n",
            "Epoch 00079: val_loss improved from 0.22689 to 0.22565, saving model to /content/model/79-0.2257.hdf5\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.1894 - accuracy: 0.9410 - val_loss: 0.2257 - val_accuracy: 0.9231\n",
            "Epoch 80/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1678 - accuracy: 0.9440\n",
            "Epoch 00080: val_loss improved from 0.22565 to 0.22448, saving model to /content/model/80-0.2245.hdf5\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.1890 - accuracy: 0.9410 - val_loss: 0.2245 - val_accuracy: 0.9231\n",
            "Epoch 81/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1810 - accuracy: 0.9440\n",
            "Epoch 00081: val_loss improved from 0.22448 to 0.22346, saving model to /content/model/81-0.2235.hdf5\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.1886 - accuracy: 0.9410 - val_loss: 0.2235 - val_accuracy: 0.9282\n",
            "Epoch 82/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2071 - accuracy: 0.9300\n",
            "Epoch 00082: val_loss improved from 0.22346 to 0.22311, saving model to /content/model/82-0.2231.hdf5\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.1882 - accuracy: 0.9410 - val_loss: 0.2231 - val_accuracy: 0.9282\n",
            "Epoch 83/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2045 - accuracy: 0.9340\n",
            "Epoch 00083: val_loss did not improve from 0.22311\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.1878 - accuracy: 0.9410 - val_loss: 0.2234 - val_accuracy: 0.9282\n",
            "Epoch 84/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1556 - accuracy: 0.9520\n",
            "Epoch 00084: val_loss did not improve from 0.22311\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.1876 - accuracy: 0.9410 - val_loss: 0.2242 - val_accuracy: 0.9282\n",
            "Epoch 85/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1801 - accuracy: 0.9420\n",
            "Epoch 00085: val_loss did not improve from 0.22311\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.1871 - accuracy: 0.9410 - val_loss: 0.2245 - val_accuracy: 0.9282\n",
            "Epoch 86/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1697 - accuracy: 0.9460\n",
            "Epoch 00086: val_loss did not improve from 0.22311\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1867 - accuracy: 0.9410 - val_loss: 0.2241 - val_accuracy: 0.9282\n",
            "Epoch 87/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1987 - accuracy: 0.9360\n",
            "Epoch 00087: val_loss did not improve from 0.22311\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.1865 - accuracy: 0.9423 - val_loss: 0.2233 - val_accuracy: 0.9282\n",
            "Epoch 88/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1642 - accuracy: 0.9500\n",
            "Epoch 00088: val_loss improved from 0.22311 to 0.22166, saving model to /content/model/88-0.2217.hdf5\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.1862 - accuracy: 0.9423 - val_loss: 0.2217 - val_accuracy: 0.9282\n",
            "Epoch 89/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1765 - accuracy: 0.9340\n",
            "Epoch 00089: val_loss improved from 0.22166 to 0.22010, saving model to /content/model/89-0.2201.hdf5\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.1858 - accuracy: 0.9423 - val_loss: 0.2201 - val_accuracy: 0.9282\n",
            "Epoch 90/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1703 - accuracy: 0.9440\n",
            "Epoch 00090: val_loss improved from 0.22010 to 0.21896, saving model to /content/model/90-0.2190.hdf5\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 0.1855 - accuracy: 0.9423 - val_loss: 0.2190 - val_accuracy: 0.9282\n",
            "Epoch 91/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1918 - accuracy: 0.9420\n",
            "Epoch 00091: val_loss improved from 0.21896 to 0.21821, saving model to /content/model/91-0.2182.hdf5\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1853 - accuracy: 0.9423 - val_loss: 0.2182 - val_accuracy: 0.9282\n",
            "Epoch 92/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2021 - accuracy: 0.9420\n",
            "Epoch 00092: val_loss improved from 0.21821 to 0.21820, saving model to /content/model/92-0.2182.hdf5\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.1848 - accuracy: 0.9410 - val_loss: 0.2182 - val_accuracy: 0.9282\n",
            "Epoch 93/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1948 - accuracy: 0.9380\n",
            "Epoch 00093: val_loss did not improve from 0.21820\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.1844 - accuracy: 0.9423 - val_loss: 0.2189 - val_accuracy: 0.9282\n",
            "Epoch 94/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1784 - accuracy: 0.9420\n",
            "Epoch 00094: val_loss did not improve from 0.21820\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.1840 - accuracy: 0.9423 - val_loss: 0.2201 - val_accuracy: 0.9282\n",
            "Epoch 95/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1592 - accuracy: 0.9520\n",
            "Epoch 00095: val_loss did not improve from 0.21820\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.1838 - accuracy: 0.9436 - val_loss: 0.2216 - val_accuracy: 0.9282\n",
            "Epoch 96/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1803 - accuracy: 0.9420\n",
            "Epoch 00096: val_loss did not improve from 0.21820\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.1835 - accuracy: 0.9423 - val_loss: 0.2223 - val_accuracy: 0.9282\n",
            "Epoch 97/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1736 - accuracy: 0.9460\n",
            "Epoch 00097: val_loss did not improve from 0.21820\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1832 - accuracy: 0.9423 - val_loss: 0.2213 - val_accuracy: 0.9282\n",
            "Epoch 98/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1766 - accuracy: 0.9460\n",
            "Epoch 00098: val_loss did not improve from 0.21820\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1829 - accuracy: 0.9423 - val_loss: 0.2193 - val_accuracy: 0.9282\n",
            "Epoch 99/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2082 - accuracy: 0.9340\n",
            "Epoch 00099: val_loss improved from 0.21820 to 0.21707, saving model to /content/model/99-0.2171.hdf5\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1825 - accuracy: 0.9423 - val_loss: 0.2171 - val_accuracy: 0.9282\n",
            "Epoch 100/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1690 - accuracy: 0.9440\n",
            "Epoch 00100: val_loss improved from 0.21707 to 0.21521, saving model to /content/model/100-0.2152.hdf5\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.1821 - accuracy: 0.9423 - val_loss: 0.2152 - val_accuracy: 0.9282\n",
            "Epoch 101/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1822 - accuracy: 0.9400\n",
            "Epoch 00101: val_loss improved from 0.21521 to 0.21329, saving model to /content/model/101-0.2133.hdf5\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.1817 - accuracy: 0.9423 - val_loss: 0.2133 - val_accuracy: 0.9282\n",
            "Epoch 102/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1821 - accuracy: 0.9440\n",
            "Epoch 00102: val_loss improved from 0.21329 to 0.21260, saving model to /content/model/102-0.2126.hdf5\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1814 - accuracy: 0.9423 - val_loss: 0.2126 - val_accuracy: 0.9282\n",
            "Epoch 103/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1904 - accuracy: 0.9360\n",
            "Epoch 00103: val_loss did not improve from 0.21260\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1811 - accuracy: 0.9423 - val_loss: 0.2128 - val_accuracy: 0.9282\n",
            "Epoch 104/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1835 - accuracy: 0.9440\n",
            "Epoch 00104: val_loss did not improve from 0.21260\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.1807 - accuracy: 0.9423 - val_loss: 0.2140 - val_accuracy: 0.9282\n",
            "Epoch 105/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1791 - accuracy: 0.9460\n",
            "Epoch 00105: val_loss did not improve from 0.21260\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1803 - accuracy: 0.9423 - val_loss: 0.2150 - val_accuracy: 0.9282\n",
            "Epoch 106/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1873 - accuracy: 0.9360\n",
            "Epoch 00106: val_loss did not improve from 0.21260\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1800 - accuracy: 0.9423 - val_loss: 0.2159 - val_accuracy: 0.9282\n",
            "Epoch 107/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1765 - accuracy: 0.9440\n",
            "Epoch 00107: val_loss did not improve from 0.21260\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.1798 - accuracy: 0.9436 - val_loss: 0.2148 - val_accuracy: 0.9282\n",
            "Epoch 108/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1825 - accuracy: 0.9420\n",
            "Epoch 00108: val_loss did not improve from 0.21260\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.1794 - accuracy: 0.9436 - val_loss: 0.2128 - val_accuracy: 0.9282\n",
            "Epoch 109/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1652 - accuracy: 0.9460\n",
            "Epoch 00109: val_loss improved from 0.21260 to 0.21086, saving model to /content/model/109-0.2109.hdf5\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.1791 - accuracy: 0.9423 - val_loss: 0.2109 - val_accuracy: 0.9282\n",
            "Epoch 110/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1514 - accuracy: 0.9560\n",
            "Epoch 00110: val_loss improved from 0.21086 to 0.20993, saving model to /content/model/110-0.2099.hdf5\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.1788 - accuracy: 0.9436 - val_loss: 0.2099 - val_accuracy: 0.9282\n",
            "Epoch 111/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1568 - accuracy: 0.9540\n",
            "Epoch 00111: val_loss improved from 0.20993 to 0.20848, saving model to /content/model/111-0.2085.hdf5\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.1784 - accuracy: 0.9436 - val_loss: 0.2085 - val_accuracy: 0.9282\n",
            "Epoch 112/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1684 - accuracy: 0.9440\n",
            "Epoch 00112: val_loss improved from 0.20848 to 0.20800, saving model to /content/model/112-0.2080.hdf5\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.1780 - accuracy: 0.9423 - val_loss: 0.2080 - val_accuracy: 0.9282\n",
            "Epoch 113/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1851 - accuracy: 0.9360\n",
            "Epoch 00113: val_loss did not improve from 0.20800\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1777 - accuracy: 0.9423 - val_loss: 0.2082 - val_accuracy: 0.9282\n",
            "Epoch 114/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1810 - accuracy: 0.9440\n",
            "Epoch 00114: val_loss did not improve from 0.20800\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.1773 - accuracy: 0.9436 - val_loss: 0.2090 - val_accuracy: 0.9333\n",
            "Epoch 115/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1635 - accuracy: 0.9500\n",
            "Epoch 00115: val_loss did not improve from 0.20800\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.1770 - accuracy: 0.9436 - val_loss: 0.2097 - val_accuracy: 0.9333\n",
            "Epoch 116/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1656 - accuracy: 0.9520\n",
            "Epoch 00116: val_loss did not improve from 0.20800\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.1767 - accuracy: 0.9436 - val_loss: 0.2093 - val_accuracy: 0.9333\n",
            "Epoch 117/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1757 - accuracy: 0.9460\n",
            "Epoch 00117: val_loss did not improve from 0.20800\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.1764 - accuracy: 0.9436 - val_loss: 0.2081 - val_accuracy: 0.9385\n",
            "Epoch 118/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1641 - accuracy: 0.9440\n",
            "Epoch 00118: val_loss improved from 0.20800 to 0.20716, saving model to /content/model/118-0.2072.hdf5\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.1760 - accuracy: 0.9449 - val_loss: 0.2072 - val_accuracy: 0.9333\n",
            "Epoch 119/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1944 - accuracy: 0.9380\n",
            "Epoch 00119: val_loss improved from 0.20716 to 0.20561, saving model to /content/model/119-0.2056.hdf5\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.1759 - accuracy: 0.9449 - val_loss: 0.2056 - val_accuracy: 0.9333\n",
            "Epoch 120/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1713 - accuracy: 0.9520\n",
            "Epoch 00120: val_loss did not improve from 0.20561\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.1754 - accuracy: 0.9436 - val_loss: 0.2057 - val_accuracy: 0.9282\n",
            "Epoch 121/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1729 - accuracy: 0.9480\n",
            "Epoch 00121: val_loss did not improve from 0.20561\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.1751 - accuracy: 0.9436 - val_loss: 0.2062 - val_accuracy: 0.9282\n",
            "Epoch 122/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1777 - accuracy: 0.9420\n",
            "Epoch 00122: val_loss did not improve from 0.20561\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1748 - accuracy: 0.9436 - val_loss: 0.2068 - val_accuracy: 0.9282\n",
            "Epoch 123/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1703 - accuracy: 0.9480\n",
            "Epoch 00123: val_loss did not improve from 0.20561\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1745 - accuracy: 0.9436 - val_loss: 0.2069 - val_accuracy: 0.9282\n",
            "Epoch 124/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1658 - accuracy: 0.9460\n",
            "Epoch 00124: val_loss did not improve from 0.20561\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1742 - accuracy: 0.9436 - val_loss: 0.2070 - val_accuracy: 0.9282\n",
            "Epoch 125/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1618 - accuracy: 0.9520\n",
            "Epoch 00125: val_loss did not improve from 0.20561\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1738 - accuracy: 0.9436 - val_loss: 0.2062 - val_accuracy: 0.9282\n",
            "Epoch 126/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1795 - accuracy: 0.9420\n",
            "Epoch 00126: val_loss improved from 0.20561 to 0.20455, saving model to /content/model/126-0.2046.hdf5\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1734 - accuracy: 0.9449 - val_loss: 0.2046 - val_accuracy: 0.9385\n",
            "Epoch 127/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.2007 - accuracy: 0.9300\n",
            "Epoch 00127: val_loss improved from 0.20455 to 0.20281, saving model to /content/model/127-0.2028.hdf5\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.1732 - accuracy: 0.9449 - val_loss: 0.2028 - val_accuracy: 0.9385\n",
            "Epoch 128/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1871 - accuracy: 0.9400\n",
            "Epoch 00128: val_loss improved from 0.20281 to 0.20206, saving model to /content/model/128-0.2021.hdf5\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1729 - accuracy: 0.9449 - val_loss: 0.2021 - val_accuracy: 0.9385\n",
            "Epoch 129/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1600 - accuracy: 0.9500\n",
            "Epoch 00129: val_loss improved from 0.20206 to 0.20150, saving model to /content/model/129-0.2015.hdf5\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1727 - accuracy: 0.9462 - val_loss: 0.2015 - val_accuracy: 0.9385\n",
            "Epoch 130/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1829 - accuracy: 0.9440\n",
            "Epoch 00130: val_loss did not improve from 0.20150\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.1723 - accuracy: 0.9462 - val_loss: 0.2022 - val_accuracy: 0.9385\n",
            "Epoch 131/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1841 - accuracy: 0.9460\n",
            "Epoch 00131: val_loss did not improve from 0.20150\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1719 - accuracy: 0.9462 - val_loss: 0.2033 - val_accuracy: 0.9385\n",
            "Epoch 132/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1652 - accuracy: 0.9500\n",
            "Epoch 00132: val_loss did not improve from 0.20150\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.1716 - accuracy: 0.9462 - val_loss: 0.2046 - val_accuracy: 0.9282\n",
            "Epoch 133/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1621 - accuracy: 0.9500\n",
            "Epoch 00133: val_loss did not improve from 0.20150\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.1714 - accuracy: 0.9436 - val_loss: 0.2044 - val_accuracy: 0.9282\n",
            "Epoch 134/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1830 - accuracy: 0.9340\n",
            "Epoch 00134: val_loss did not improve from 0.20150\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1711 - accuracy: 0.9436 - val_loss: 0.2030 - val_accuracy: 0.9333\n",
            "Epoch 135/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1760 - accuracy: 0.9420\n",
            "Epoch 00135: val_loss improved from 0.20150 to 0.20106, saving model to /content/model/135-0.2011.hdf5\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1707 - accuracy: 0.9462 - val_loss: 0.2011 - val_accuracy: 0.9385\n",
            "Epoch 136/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1643 - accuracy: 0.9420\n",
            "Epoch 00136: val_loss improved from 0.20106 to 0.20048, saving model to /content/model/136-0.2005.hdf5\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.1704 - accuracy: 0.9462 - val_loss: 0.2005 - val_accuracy: 0.9385\n",
            "Epoch 137/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1747 - accuracy: 0.9420\n",
            "Epoch 00137: val_loss improved from 0.20048 to 0.20036, saving model to /content/model/137-0.2004.hdf5\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.1702 - accuracy: 0.9462 - val_loss: 0.2004 - val_accuracy: 0.9385\n",
            "Epoch 138/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1697 - accuracy: 0.9460\n",
            "Epoch 00138: val_loss did not improve from 0.20036\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1698 - accuracy: 0.9462 - val_loss: 0.2008 - val_accuracy: 0.9385\n",
            "Epoch 139/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1736 - accuracy: 0.9420\n",
            "Epoch 00139: val_loss did not improve from 0.20036\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.1695 - accuracy: 0.9462 - val_loss: 0.2009 - val_accuracy: 0.9385\n",
            "Epoch 140/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1785 - accuracy: 0.9480\n",
            "Epoch 00140: val_loss improved from 0.20036 to 0.20017, saving model to /content/model/140-0.2002.hdf5\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1694 - accuracy: 0.9462 - val_loss: 0.2002 - val_accuracy: 0.9385\n",
            "Epoch 141/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1535 - accuracy: 0.9480\n",
            "Epoch 00141: val_loss improved from 0.20017 to 0.19989, saving model to /content/model/141-0.1999.hdf5\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1690 - accuracy: 0.9462 - val_loss: 0.1999 - val_accuracy: 0.9385\n",
            "Epoch 142/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1876 - accuracy: 0.9360\n",
            "Epoch 00142: val_loss improved from 0.19989 to 0.19947, saving model to /content/model/142-0.1995.hdf5\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 0.1686 - accuracy: 0.9462 - val_loss: 0.1995 - val_accuracy: 0.9385\n",
            "Epoch 143/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1497 - accuracy: 0.9540\n",
            "Epoch 00143: val_loss improved from 0.19947 to 0.19904, saving model to /content/model/143-0.1990.hdf5\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.1685 - accuracy: 0.9462 - val_loss: 0.1990 - val_accuracy: 0.9385\n",
            "Epoch 144/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1856 - accuracy: 0.9400\n",
            "Epoch 00144: val_loss improved from 0.19904 to 0.19884, saving model to /content/model/144-0.1988.hdf5\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1688 - accuracy: 0.9462 - val_loss: 0.1988 - val_accuracy: 0.9333\n",
            "Epoch 145/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1691 - accuracy: 0.9500\n",
            "Epoch 00145: val_loss improved from 0.19884 to 0.19773, saving model to /content/model/145-0.1977.hdf5\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.1679 - accuracy: 0.9462 - val_loss: 0.1977 - val_accuracy: 0.9385\n",
            "Epoch 146/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1427 - accuracy: 0.9540\n",
            "Epoch 00146: val_loss improved from 0.19773 to 0.19769, saving model to /content/model/146-0.1977.hdf5\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.1678 - accuracy: 0.9462 - val_loss: 0.1977 - val_accuracy: 0.9385\n",
            "Epoch 147/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1722 - accuracy: 0.9460\n",
            "Epoch 00147: val_loss did not improve from 0.19769\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1670 - accuracy: 0.9462 - val_loss: 0.1982 - val_accuracy: 0.9282\n",
            "Epoch 148/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1762 - accuracy: 0.9480\n",
            "Epoch 00148: val_loss did not improve from 0.19769\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1669 - accuracy: 0.9462 - val_loss: 0.1980 - val_accuracy: 0.9282\n",
            "Epoch 149/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1432 - accuracy: 0.9540\n",
            "Epoch 00149: val_loss improved from 0.19769 to 0.19746, saving model to /content/model/149-0.1975.hdf5\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1668 - accuracy: 0.9462 - val_loss: 0.1975 - val_accuracy: 0.9282\n",
            "Epoch 150/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1677 - accuracy: 0.9480\n",
            "Epoch 00150: val_loss improved from 0.19746 to 0.19676, saving model to /content/model/150-0.1968.hdf5\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.1663 - accuracy: 0.9462 - val_loss: 0.1968 - val_accuracy: 0.9333\n",
            "Epoch 151/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1652 - accuracy: 0.9500\n",
            "Epoch 00151: val_loss improved from 0.19676 to 0.19656, saving model to /content/model/151-0.1966.hdf5\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1659 - accuracy: 0.9462 - val_loss: 0.1966 - val_accuracy: 0.9333\n",
            "Epoch 152/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1607 - accuracy: 0.9540\n",
            "Epoch 00152: val_loss improved from 0.19656 to 0.19586, saving model to /content/model/152-0.1959.hdf5\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.1656 - accuracy: 0.9462 - val_loss: 0.1959 - val_accuracy: 0.9385\n",
            "Epoch 153/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1746 - accuracy: 0.9480\n",
            "Epoch 00153: val_loss improved from 0.19586 to 0.19546, saving model to /content/model/153-0.1955.hdf5\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1654 - accuracy: 0.9462 - val_loss: 0.1955 - val_accuracy: 0.9385\n",
            "Epoch 154/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1389 - accuracy: 0.9620\n",
            "Epoch 00154: val_loss did not improve from 0.19546\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.1652 - accuracy: 0.9462 - val_loss: 0.1965 - val_accuracy: 0.9333\n",
            "Epoch 155/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1664 - accuracy: 0.9480\n",
            "Epoch 00155: val_loss did not improve from 0.19546\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.1646 - accuracy: 0.9462 - val_loss: 0.1976 - val_accuracy: 0.9333\n",
            "Epoch 156/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1628 - accuracy: 0.9420\n",
            "Epoch 00156: val_loss did not improve from 0.19546\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.1647 - accuracy: 0.9449 - val_loss: 0.1986 - val_accuracy: 0.9231\n",
            "Epoch 157/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1739 - accuracy: 0.9340\n",
            "Epoch 00157: val_loss did not improve from 0.19546\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.1647 - accuracy: 0.9436 - val_loss: 0.1968 - val_accuracy: 0.9333\n",
            "Epoch 158/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1787 - accuracy: 0.9420\n",
            "Epoch 00158: val_loss improved from 0.19546 to 0.19403, saving model to /content/model/158-0.1940.hdf5\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.1642 - accuracy: 0.9436 - val_loss: 0.1940 - val_accuracy: 0.9333\n",
            "Epoch 159/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1781 - accuracy: 0.9460\n",
            "Epoch 00159: val_loss improved from 0.19403 to 0.19230, saving model to /content/model/159-0.1923.hdf5\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1638 - accuracy: 0.9462 - val_loss: 0.1923 - val_accuracy: 0.9333\n",
            "Epoch 160/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1709 - accuracy: 0.9380\n",
            "Epoch 00160: val_loss did not improve from 0.19230\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1634 - accuracy: 0.9462 - val_loss: 0.1923 - val_accuracy: 0.9385\n",
            "Epoch 161/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1568 - accuracy: 0.9480\n",
            "Epoch 00161: val_loss did not improve from 0.19230\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.1634 - accuracy: 0.9462 - val_loss: 0.1929 - val_accuracy: 0.9333\n",
            "Epoch 162/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1915 - accuracy: 0.9320\n",
            "Epoch 00162: val_loss did not improve from 0.19230\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.1629 - accuracy: 0.9462 - val_loss: 0.1941 - val_accuracy: 0.9333\n",
            "Epoch 163/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1689 - accuracy: 0.9440\n",
            "Epoch 00163: val_loss did not improve from 0.19230\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.1624 - accuracy: 0.9462 - val_loss: 0.1955 - val_accuracy: 0.9333\n",
            "Epoch 164/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1622 - accuracy: 0.9420\n",
            "Epoch 00164: val_loss did not improve from 0.19230\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.1622 - accuracy: 0.9449 - val_loss: 0.1953 - val_accuracy: 0.9333\n",
            "Epoch 165/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1460 - accuracy: 0.9580\n",
            "Epoch 00165: val_loss did not improve from 0.19230\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1620 - accuracy: 0.9449 - val_loss: 0.1935 - val_accuracy: 0.9333\n",
            "Epoch 166/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1791 - accuracy: 0.9380\n",
            "Epoch 00166: val_loss improved from 0.19230 to 0.19218, saving model to /content/model/166-0.1922.hdf5\n",
            "2/2 [==============================] - 0s 88ms/step - loss: 0.1615 - accuracy: 0.9462 - val_loss: 0.1922 - val_accuracy: 0.9333\n",
            "Epoch 167/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1640 - accuracy: 0.9460\n",
            "Epoch 00167: val_loss improved from 0.19218 to 0.19129, saving model to /content/model/167-0.1913.hdf5\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1611 - accuracy: 0.9462 - val_loss: 0.1913 - val_accuracy: 0.9333\n",
            "Epoch 168/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1829 - accuracy: 0.9380\n",
            "Epoch 00168: val_loss improved from 0.19129 to 0.19017, saving model to /content/model/168-0.1902.hdf5\n",
            "2/2 [==============================] - 0s 55ms/step - loss: 0.1609 - accuracy: 0.9462 - val_loss: 0.1902 - val_accuracy: 0.9333\n",
            "Epoch 169/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1656 - accuracy: 0.9500\n",
            "Epoch 00169: val_loss improved from 0.19017 to 0.18964, saving model to /content/model/169-0.1896.hdf5\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1610 - accuracy: 0.9462 - val_loss: 0.1896 - val_accuracy: 0.9333\n",
            "Epoch 170/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1476 - accuracy: 0.9500\n",
            "Epoch 00170: val_loss did not improve from 0.18964\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1605 - accuracy: 0.9462 - val_loss: 0.1904 - val_accuracy: 0.9333\n",
            "Epoch 171/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1246 - accuracy: 0.9560\n",
            "Epoch 00171: val_loss did not improve from 0.18964\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1600 - accuracy: 0.9462 - val_loss: 0.1921 - val_accuracy: 0.9333\n",
            "Epoch 172/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1324 - accuracy: 0.9600\n",
            "Epoch 00172: val_loss did not improve from 0.18964\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1597 - accuracy: 0.9449 - val_loss: 0.1928 - val_accuracy: 0.9231\n",
            "Epoch 173/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1641 - accuracy: 0.9500\n",
            "Epoch 00173: val_loss did not improve from 0.18964\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.1609 - accuracy: 0.9487 - val_loss: 0.1923 - val_accuracy: 0.9282\n",
            "Epoch 174/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1803 - accuracy: 0.9440\n",
            "Epoch 00174: val_loss did not improve from 0.18964\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.1607 - accuracy: 0.9474 - val_loss: 0.1903 - val_accuracy: 0.9333\n",
            "Epoch 175/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1403 - accuracy: 0.9520\n",
            "Epoch 00175: val_loss improved from 0.18964 to 0.18854, saving model to /content/model/175-0.1885.hdf5\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.1592 - accuracy: 0.9449 - val_loss: 0.1885 - val_accuracy: 0.9333\n",
            "Epoch 176/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1555 - accuracy: 0.9480\n",
            "Epoch 00176: val_loss improved from 0.18854 to 0.18795, saving model to /content/model/176-0.1880.hdf5\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.1591 - accuracy: 0.9462 - val_loss: 0.1880 - val_accuracy: 0.9487\n",
            "Epoch 177/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1665 - accuracy: 0.9460\n",
            "Epoch 00177: val_loss improved from 0.18795 to 0.18777, saving model to /content/model/177-0.1878.hdf5\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1593 - accuracy: 0.9474 - val_loss: 0.1878 - val_accuracy: 0.9487\n",
            "Epoch 178/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1774 - accuracy: 0.9420\n",
            "Epoch 00178: val_loss did not improve from 0.18777\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.1590 - accuracy: 0.9474 - val_loss: 0.1878 - val_accuracy: 0.9333\n",
            "Epoch 179/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1760 - accuracy: 0.9420\n",
            "Epoch 00179: val_loss did not improve from 0.18777\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1583 - accuracy: 0.9462 - val_loss: 0.1879 - val_accuracy: 0.9333\n",
            "Epoch 180/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1681 - accuracy: 0.9400\n",
            "Epoch 00180: val_loss did not improve from 0.18777\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1577 - accuracy: 0.9449 - val_loss: 0.1881 - val_accuracy: 0.9333\n",
            "Epoch 181/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1472 - accuracy: 0.9360\n",
            "Epoch 00181: val_loss improved from 0.18777 to 0.18721, saving model to /content/model/181-0.1872.hdf5\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.1573 - accuracy: 0.9449 - val_loss: 0.1872 - val_accuracy: 0.9333\n",
            "Epoch 182/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1602 - accuracy: 0.9400\n",
            "Epoch 00182: val_loss improved from 0.18721 to 0.18501, saving model to /content/model/182-0.1850.hdf5\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1570 - accuracy: 0.9449 - val_loss: 0.1850 - val_accuracy: 0.9333\n",
            "Epoch 183/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1571 - accuracy: 0.9500\n",
            "Epoch 00183: val_loss improved from 0.18501 to 0.18384, saving model to /content/model/183-0.1838.hdf5\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1567 - accuracy: 0.9462 - val_loss: 0.1838 - val_accuracy: 0.9333\n",
            "Epoch 184/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1553 - accuracy: 0.9480\n",
            "Epoch 00184: val_loss improved from 0.18384 to 0.18376, saving model to /content/model/184-0.1838.hdf5\n",
            "2/2 [==============================] - 0s 69ms/step - loss: 0.1565 - accuracy: 0.9462 - val_loss: 0.1838 - val_accuracy: 0.9333\n",
            "Epoch 185/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1644 - accuracy: 0.9420\n",
            "Epoch 00185: val_loss did not improve from 0.18376\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.1564 - accuracy: 0.9449 - val_loss: 0.1845 - val_accuracy: 0.9333\n",
            "Epoch 186/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1449 - accuracy: 0.9520\n",
            "Epoch 00186: val_loss did not improve from 0.18376\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.1560 - accuracy: 0.9449 - val_loss: 0.1853 - val_accuracy: 0.9333\n",
            "Epoch 187/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1599 - accuracy: 0.9480\n",
            "Epoch 00187: val_loss did not improve from 0.18376\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1556 - accuracy: 0.9449 - val_loss: 0.1857 - val_accuracy: 0.9333\n",
            "Epoch 188/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1572 - accuracy: 0.9440\n",
            "Epoch 00188: val_loss did not improve from 0.18376\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1554 - accuracy: 0.9462 - val_loss: 0.1859 - val_accuracy: 0.9333\n",
            "Epoch 189/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1552 - accuracy: 0.9460\n",
            "Epoch 00189: val_loss did not improve from 0.18376\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.1551 - accuracy: 0.9462 - val_loss: 0.1847 - val_accuracy: 0.9333\n",
            "Epoch 190/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1523 - accuracy: 0.9420\n",
            "Epoch 00190: val_loss improved from 0.18376 to 0.18348, saving model to /content/model/190-0.1835.hdf5\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.1548 - accuracy: 0.9449 - val_loss: 0.1835 - val_accuracy: 0.9333\n",
            "Epoch 191/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1410 - accuracy: 0.9520\n",
            "Epoch 00191: val_loss improved from 0.18348 to 0.18289, saving model to /content/model/191-0.1829.hdf5\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1544 - accuracy: 0.9462 - val_loss: 0.1829 - val_accuracy: 0.9333\n",
            "Epoch 192/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1521 - accuracy: 0.9500\n",
            "Epoch 00192: val_loss improved from 0.18289 to 0.18244, saving model to /content/model/192-0.1824.hdf5\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.1541 - accuracy: 0.9462 - val_loss: 0.1824 - val_accuracy: 0.9333\n",
            "Epoch 193/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1439 - accuracy: 0.9460\n",
            "Epoch 00193: val_loss improved from 0.18244 to 0.18242, saving model to /content/model/193-0.1824.hdf5\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.1539 - accuracy: 0.9474 - val_loss: 0.1824 - val_accuracy: 0.9333\n",
            "Epoch 194/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1757 - accuracy: 0.9360\n",
            "Epoch 00194: val_loss improved from 0.18242 to 0.18224, saving model to /content/model/194-0.1822.hdf5\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.1543 - accuracy: 0.9474 - val_loss: 0.1822 - val_accuracy: 0.9333\n",
            "Epoch 195/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1490 - accuracy: 0.9580\n",
            "Epoch 00195: val_loss improved from 0.18224 to 0.18069, saving model to /content/model/195-0.1807.hdf5\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1545 - accuracy: 0.9474 - val_loss: 0.1807 - val_accuracy: 0.9385\n",
            "Epoch 196/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1639 - accuracy: 0.9420\n",
            "Epoch 00196: val_loss improved from 0.18069 to 0.18025, saving model to /content/model/196-0.1802.hdf5\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1532 - accuracy: 0.9474 - val_loss: 0.1802 - val_accuracy: 0.9385\n",
            "Epoch 197/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1727 - accuracy: 0.9400\n",
            "Epoch 00197: val_loss improved from 0.18025 to 0.17978, saving model to /content/model/197-0.1798.hdf5\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.1529 - accuracy: 0.9474 - val_loss: 0.1798 - val_accuracy: 0.9385\n",
            "Epoch 198/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1562 - accuracy: 0.9480\n",
            "Epoch 00198: val_loss improved from 0.17978 to 0.17938, saving model to /content/model/198-0.1794.hdf5\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.1525 - accuracy: 0.9487 - val_loss: 0.1794 - val_accuracy: 0.9385\n",
            "Epoch 199/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1534 - accuracy: 0.9460\n",
            "Epoch 00199: val_loss improved from 0.17938 to 0.17896, saving model to /content/model/199-0.1790.hdf5\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 0.1523 - accuracy: 0.9487 - val_loss: 0.1790 - val_accuracy: 0.9385\n",
            "Epoch 200/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1587 - accuracy: 0.9440\n",
            "Epoch 00200: val_loss improved from 0.17896 to 0.17849, saving model to /content/model/200-0.1785.hdf5\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1520 - accuracy: 0.9487 - val_loss: 0.1785 - val_accuracy: 0.9385\n",
            "Epoch 201/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1636 - accuracy: 0.9480\n",
            "Epoch 00201: val_loss did not improve from 0.17849\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1517 - accuracy: 0.9487 - val_loss: 0.1785 - val_accuracy: 0.9385\n",
            "Epoch 202/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1412 - accuracy: 0.9500\n",
            "Epoch 00202: val_loss did not improve from 0.17849\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1516 - accuracy: 0.9487 - val_loss: 0.1786 - val_accuracy: 0.9385\n",
            "Epoch 203/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1439 - accuracy: 0.9520\n",
            "Epoch 00203: val_loss did not improve from 0.17849\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1513 - accuracy: 0.9500 - val_loss: 0.1791 - val_accuracy: 0.9436\n",
            "Epoch 204/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1599 - accuracy: 0.9480\n",
            "Epoch 00204: val_loss did not improve from 0.17849\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1511 - accuracy: 0.9500 - val_loss: 0.1789 - val_accuracy: 0.9436\n",
            "Epoch 205/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1536 - accuracy: 0.9500\n",
            "Epoch 00205: val_loss did not improve from 0.17849\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1507 - accuracy: 0.9513 - val_loss: 0.1789 - val_accuracy: 0.9436\n",
            "Epoch 206/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1574 - accuracy: 0.9500\n",
            "Epoch 00206: val_loss improved from 0.17849 to 0.17811, saving model to /content/model/206-0.1781.hdf5\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1505 - accuracy: 0.9500 - val_loss: 0.1781 - val_accuracy: 0.9436\n",
            "Epoch 207/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1568 - accuracy: 0.9460\n",
            "Epoch 00207: val_loss improved from 0.17811 to 0.17690, saving model to /content/model/207-0.1769.hdf5\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.1501 - accuracy: 0.9513 - val_loss: 0.1769 - val_accuracy: 0.9436\n",
            "Epoch 208/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1615 - accuracy: 0.9460\n",
            "Epoch 00208: val_loss improved from 0.17690 to 0.17594, saving model to /content/model/208-0.1759.hdf5\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1500 - accuracy: 0.9500 - val_loss: 0.1759 - val_accuracy: 0.9436\n",
            "Epoch 209/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1398 - accuracy: 0.9460\n",
            "Epoch 00209: val_loss improved from 0.17594 to 0.17578, saving model to /content/model/209-0.1758.hdf5\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.1499 - accuracy: 0.9487 - val_loss: 0.1758 - val_accuracy: 0.9436\n",
            "Epoch 210/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1459 - accuracy: 0.9500\n",
            "Epoch 00210: val_loss did not improve from 0.17578\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.1494 - accuracy: 0.9487 - val_loss: 0.1758 - val_accuracy: 0.9436\n",
            "Epoch 211/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1511 - accuracy: 0.9480\n",
            "Epoch 00211: val_loss did not improve from 0.17578\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.1490 - accuracy: 0.9513 - val_loss: 0.1764 - val_accuracy: 0.9436\n",
            "Epoch 212/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1675 - accuracy: 0.9480\n",
            "Epoch 00212: val_loss did not improve from 0.17578\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.1490 - accuracy: 0.9526 - val_loss: 0.1766 - val_accuracy: 0.9385\n",
            "Epoch 213/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1498 - accuracy: 0.9480\n",
            "Epoch 00213: val_loss did not improve from 0.17578\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1490 - accuracy: 0.9513 - val_loss: 0.1764 - val_accuracy: 0.9436\n",
            "Epoch 214/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1696 - accuracy: 0.9440\n",
            "Epoch 00214: val_loss improved from 0.17578 to 0.17497, saving model to /content/model/214-0.1750.hdf5\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.1484 - accuracy: 0.9526 - val_loss: 0.1750 - val_accuracy: 0.9436\n",
            "Epoch 215/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1359 - accuracy: 0.9540\n",
            "Epoch 00215: val_loss improved from 0.17497 to 0.17437, saving model to /content/model/215-0.1744.hdf5\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.1479 - accuracy: 0.9526 - val_loss: 0.1744 - val_accuracy: 0.9487\n",
            "Epoch 216/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1518 - accuracy: 0.9500\n",
            "Epoch 00216: val_loss improved from 0.17437 to 0.17409, saving model to /content/model/216-0.1741.hdf5\n",
            "2/2 [==============================] - 0s 76ms/step - loss: 0.1479 - accuracy: 0.9500 - val_loss: 0.1741 - val_accuracy: 0.9487\n",
            "Epoch 217/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1635 - accuracy: 0.9460\n",
            "Epoch 00217: val_loss did not improve from 0.17409\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.1476 - accuracy: 0.9526 - val_loss: 0.1742 - val_accuracy: 0.9436\n",
            "Epoch 218/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1470 - accuracy: 0.9540\n",
            "Epoch 00218: val_loss did not improve from 0.17409\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1472 - accuracy: 0.9526 - val_loss: 0.1746 - val_accuracy: 0.9436\n",
            "Epoch 219/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1547 - accuracy: 0.9460\n",
            "Epoch 00219: val_loss did not improve from 0.17409\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.1470 - accuracy: 0.9526 - val_loss: 0.1753 - val_accuracy: 0.9436\n",
            "Epoch 220/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1340 - accuracy: 0.9560\n",
            "Epoch 00220: val_loss did not improve from 0.17409\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1469 - accuracy: 0.9526 - val_loss: 0.1747 - val_accuracy: 0.9436\n",
            "Epoch 221/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1525 - accuracy: 0.9540\n",
            "Epoch 00221: val_loss improved from 0.17409 to 0.17287, saving model to /content/model/221-0.1729.hdf5\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1464 - accuracy: 0.9526 - val_loss: 0.1729 - val_accuracy: 0.9436\n",
            "Epoch 222/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1510 - accuracy: 0.9540\n",
            "Epoch 00222: val_loss improved from 0.17287 to 0.17166, saving model to /content/model/222-0.1717.hdf5\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.1462 - accuracy: 0.9526 - val_loss: 0.1717 - val_accuracy: 0.9487\n",
            "Epoch 223/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1639 - accuracy: 0.9420\n",
            "Epoch 00223: val_loss improved from 0.17166 to 0.17114, saving model to /content/model/223-0.1711.hdf5\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1461 - accuracy: 0.9513 - val_loss: 0.1711 - val_accuracy: 0.9487\n",
            "Epoch 224/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1353 - accuracy: 0.9580\n",
            "Epoch 00224: val_loss did not improve from 0.17114\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1458 - accuracy: 0.9526 - val_loss: 0.1713 - val_accuracy: 0.9487\n",
            "Epoch 225/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1538 - accuracy: 0.9520\n",
            "Epoch 00225: val_loss did not improve from 0.17114\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1456 - accuracy: 0.9526 - val_loss: 0.1713 - val_accuracy: 0.9436\n",
            "Epoch 226/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1377 - accuracy: 0.9540\n",
            "Epoch 00226: val_loss improved from 0.17114 to 0.17071, saving model to /content/model/226-0.1707.hdf5\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.1451 - accuracy: 0.9526 - val_loss: 0.1707 - val_accuracy: 0.9436\n",
            "Epoch 227/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1264 - accuracy: 0.9620\n",
            "Epoch 00227: val_loss improved from 0.17071 to 0.17000, saving model to /content/model/227-0.1700.hdf5\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.1448 - accuracy: 0.9526 - val_loss: 0.1700 - val_accuracy: 0.9436\n",
            "Epoch 228/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1517 - accuracy: 0.9500\n",
            "Epoch 00228: val_loss improved from 0.17000 to 0.16882, saving model to /content/model/228-0.1688.hdf5\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1448 - accuracy: 0.9526 - val_loss: 0.1688 - val_accuracy: 0.9487\n",
            "Epoch 229/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1397 - accuracy: 0.9540\n",
            "Epoch 00229: val_loss improved from 0.16882 to 0.16823, saving model to /content/model/229-0.1682.hdf5\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1443 - accuracy: 0.9513 - val_loss: 0.1682 - val_accuracy: 0.9487\n",
            "Epoch 230/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1197 - accuracy: 0.9580\n",
            "Epoch 00230: val_loss improved from 0.16823 to 0.16792, saving model to /content/model/230-0.1679.hdf5\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1440 - accuracy: 0.9513 - val_loss: 0.1679 - val_accuracy: 0.9436\n",
            "Epoch 231/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1189 - accuracy: 0.9660\n",
            "Epoch 00231: val_loss improved from 0.16792 to 0.16747, saving model to /content/model/231-0.1675.hdf5\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.1437 - accuracy: 0.9526 - val_loss: 0.1675 - val_accuracy: 0.9436\n",
            "Epoch 232/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1288 - accuracy: 0.9600\n",
            "Epoch 00232: val_loss improved from 0.16747 to 0.16651, saving model to /content/model/232-0.1665.hdf5\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1437 - accuracy: 0.9526 - val_loss: 0.1665 - val_accuracy: 0.9436\n",
            "Epoch 233/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1369 - accuracy: 0.9620\n",
            "Epoch 00233: val_loss improved from 0.16651 to 0.16608, saving model to /content/model/233-0.1661.hdf5\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1435 - accuracy: 0.9526 - val_loss: 0.1661 - val_accuracy: 0.9487\n",
            "Epoch 234/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1745 - accuracy: 0.9380\n",
            "Epoch 00234: val_loss did not improve from 0.16608\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1430 - accuracy: 0.9526 - val_loss: 0.1668 - val_accuracy: 0.9487\n",
            "Epoch 235/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1570 - accuracy: 0.9400\n",
            "Epoch 00235: val_loss did not improve from 0.16608\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.1425 - accuracy: 0.9513 - val_loss: 0.1680 - val_accuracy: 0.9538\n",
            "Epoch 236/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1503 - accuracy: 0.9480\n",
            "Epoch 00236: val_loss did not improve from 0.16608\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.1424 - accuracy: 0.9513 - val_loss: 0.1686 - val_accuracy: 0.9538\n",
            "Epoch 237/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1274 - accuracy: 0.9540\n",
            "Epoch 00237: val_loss did not improve from 0.16608\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1423 - accuracy: 0.9513 - val_loss: 0.1689 - val_accuracy: 0.9487\n",
            "Epoch 238/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1453 - accuracy: 0.9560\n",
            "Epoch 00238: val_loss did not improve from 0.16608\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.1424 - accuracy: 0.9526 - val_loss: 0.1688 - val_accuracy: 0.9487\n",
            "Epoch 239/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1507 - accuracy: 0.9460\n",
            "Epoch 00239: val_loss did not improve from 0.16608\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1417 - accuracy: 0.9526 - val_loss: 0.1674 - val_accuracy: 0.9436\n",
            "Epoch 240/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1387 - accuracy: 0.9540\n",
            "Epoch 00240: val_loss improved from 0.16608 to 0.16550, saving model to /content/model/240-0.1655.hdf5\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1415 - accuracy: 0.9513 - val_loss: 0.1655 - val_accuracy: 0.9436\n",
            "Epoch 241/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1449 - accuracy: 0.9540\n",
            "Epoch 00241: val_loss improved from 0.16550 to 0.16462, saving model to /content/model/241-0.1646.hdf5\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.1408 - accuracy: 0.9513 - val_loss: 0.1646 - val_accuracy: 0.9487\n",
            "Epoch 242/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1353 - accuracy: 0.9520\n",
            "Epoch 00242: val_loss improved from 0.16462 to 0.16454, saving model to /content/model/242-0.1645.hdf5\n",
            "2/2 [==============================] - 0s 86ms/step - loss: 0.1405 - accuracy: 0.9513 - val_loss: 0.1645 - val_accuracy: 0.9487\n",
            "Epoch 243/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1210 - accuracy: 0.9580\n",
            "Epoch 00243: val_loss did not improve from 0.16454\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.1405 - accuracy: 0.9513 - val_loss: 0.1652 - val_accuracy: 0.9436\n",
            "Epoch 244/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1365 - accuracy: 0.9580\n",
            "Epoch 00244: val_loss did not improve from 0.16454\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.1398 - accuracy: 0.9513 - val_loss: 0.1651 - val_accuracy: 0.9436\n",
            "Epoch 245/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1172 - accuracy: 0.9600\n",
            "Epoch 00245: val_loss did not improve from 0.16454\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1396 - accuracy: 0.9526 - val_loss: 0.1650 - val_accuracy: 0.9436\n",
            "Epoch 246/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1385 - accuracy: 0.9560\n",
            "Epoch 00246: val_loss improved from 0.16454 to 0.16395, saving model to /content/model/246-0.1639.hdf5\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.1395 - accuracy: 0.9526 - val_loss: 0.1639 - val_accuracy: 0.9436\n",
            "Epoch 247/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1378 - accuracy: 0.9500\n",
            "Epoch 00247: val_loss improved from 0.16395 to 0.16317, saving model to /content/model/247-0.1632.hdf5\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1392 - accuracy: 0.9526 - val_loss: 0.1632 - val_accuracy: 0.9436\n",
            "Epoch 248/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1180 - accuracy: 0.9660\n",
            "Epoch 00248: val_loss improved from 0.16317 to 0.16229, saving model to /content/model/248-0.1623.hdf5\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1388 - accuracy: 0.9526 - val_loss: 0.1623 - val_accuracy: 0.9487\n",
            "Epoch 249/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1646 - accuracy: 0.9380\n",
            "Epoch 00249: val_loss improved from 0.16229 to 0.16203, saving model to /content/model/249-0.1620.hdf5\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.1384 - accuracy: 0.9513 - val_loss: 0.1620 - val_accuracy: 0.9487\n",
            "Epoch 250/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1462 - accuracy: 0.9460\n",
            "Epoch 00250: val_loss did not improve from 0.16203\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.1380 - accuracy: 0.9513 - val_loss: 0.1624 - val_accuracy: 0.9487\n",
            "Epoch 251/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1486 - accuracy: 0.9480\n",
            "Epoch 00251: val_loss did not improve from 0.16203\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.1376 - accuracy: 0.9513 - val_loss: 0.1625 - val_accuracy: 0.9487\n",
            "Epoch 252/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1379 - accuracy: 0.9520\n",
            "Epoch 00252: val_loss did not improve from 0.16203\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1373 - accuracy: 0.9526 - val_loss: 0.1629 - val_accuracy: 0.9436\n",
            "Epoch 253/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1256 - accuracy: 0.9600\n",
            "Epoch 00253: val_loss did not improve from 0.16203\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.1372 - accuracy: 0.9526 - val_loss: 0.1624 - val_accuracy: 0.9436\n",
            "Epoch 254/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1278 - accuracy: 0.9580\n",
            "Epoch 00254: val_loss improved from 0.16203 to 0.16117, saving model to /content/model/254-0.1612.hdf5\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1368 - accuracy: 0.9526 - val_loss: 0.1612 - val_accuracy: 0.9436\n",
            "Epoch 255/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1460 - accuracy: 0.9500\n",
            "Epoch 00255: val_loss improved from 0.16117 to 0.15924, saving model to /content/model/255-0.1592.hdf5\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.1366 - accuracy: 0.9526 - val_loss: 0.1592 - val_accuracy: 0.9487\n",
            "Epoch 256/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1222 - accuracy: 0.9580\n",
            "Epoch 00256: val_loss improved from 0.15924 to 0.15793, saving model to /content/model/256-0.1579.hdf5\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.1363 - accuracy: 0.9526 - val_loss: 0.1579 - val_accuracy: 0.9538\n",
            "Epoch 257/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1482 - accuracy: 0.9500\n",
            "Epoch 00257: val_loss improved from 0.15793 to 0.15747, saving model to /content/model/257-0.1575.hdf5\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1358 - accuracy: 0.9526 - val_loss: 0.1575 - val_accuracy: 0.9538\n",
            "Epoch 258/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1483 - accuracy: 0.9440\n",
            "Epoch 00258: val_loss did not improve from 0.15747\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.1356 - accuracy: 0.9538 - val_loss: 0.1580 - val_accuracy: 0.9538\n",
            "Epoch 259/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1449 - accuracy: 0.9480\n",
            "Epoch 00259: val_loss did not improve from 0.15747\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1353 - accuracy: 0.9526 - val_loss: 0.1585 - val_accuracy: 0.9538\n",
            "Epoch 260/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1277 - accuracy: 0.9580\n",
            "Epoch 00260: val_loss did not improve from 0.15747\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1352 - accuracy: 0.9551 - val_loss: 0.1585 - val_accuracy: 0.9590\n",
            "Epoch 261/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1387 - accuracy: 0.9520\n",
            "Epoch 00261: val_loss did not improve from 0.15747\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1346 - accuracy: 0.9526 - val_loss: 0.1580 - val_accuracy: 0.9487\n",
            "Epoch 262/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1213 - accuracy: 0.9600\n",
            "Epoch 00262: val_loss did not improve from 0.15747\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1341 - accuracy: 0.9526 - val_loss: 0.1578 - val_accuracy: 0.9538\n",
            "Epoch 263/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1322 - accuracy: 0.9560\n",
            "Epoch 00263: val_loss improved from 0.15747 to 0.15722, saving model to /content/model/263-0.1572.hdf5\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1342 - accuracy: 0.9526 - val_loss: 0.1572 - val_accuracy: 0.9538\n",
            "Epoch 264/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1210 - accuracy: 0.9560\n",
            "Epoch 00264: val_loss improved from 0.15722 to 0.15692, saving model to /content/model/264-0.1569.hdf5\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 0.1337 - accuracy: 0.9538 - val_loss: 0.1569 - val_accuracy: 0.9538\n",
            "Epoch 265/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1384 - accuracy: 0.9560\n",
            "Epoch 00265: val_loss improved from 0.15692 to 0.15631, saving model to /content/model/265-0.1563.hdf5\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1332 - accuracy: 0.9538 - val_loss: 0.1563 - val_accuracy: 0.9590\n",
            "Epoch 266/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1140 - accuracy: 0.9640\n",
            "Epoch 00266: val_loss improved from 0.15631 to 0.15572, saving model to /content/model/266-0.1557.hdf5\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1333 - accuracy: 0.9551 - val_loss: 0.1557 - val_accuracy: 0.9590\n",
            "Epoch 267/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1386 - accuracy: 0.9520\n",
            "Epoch 00267: val_loss improved from 0.15572 to 0.15490, saving model to /content/model/267-0.1549.hdf5\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.1326 - accuracy: 0.9551 - val_loss: 0.1549 - val_accuracy: 0.9590\n",
            "Epoch 268/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1280 - accuracy: 0.9580\n",
            "Epoch 00268: val_loss improved from 0.15490 to 0.15456, saving model to /content/model/268-0.1546.hdf5\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1325 - accuracy: 0.9526 - val_loss: 0.1546 - val_accuracy: 0.9538\n",
            "Epoch 269/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1403 - accuracy: 0.9500\n",
            "Epoch 00269: val_loss did not improve from 0.15456\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1329 - accuracy: 0.9526 - val_loss: 0.1546 - val_accuracy: 0.9538\n",
            "Epoch 270/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1251 - accuracy: 0.9520\n",
            "Epoch 00270: val_loss did not improve from 0.15456\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.1331 - accuracy: 0.9526 - val_loss: 0.1557 - val_accuracy: 0.9590\n",
            "Epoch 271/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1159 - accuracy: 0.9600\n",
            "Epoch 00271: val_loss did not improve from 0.15456\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1320 - accuracy: 0.9551 - val_loss: 0.1555 - val_accuracy: 0.9590\n",
            "Epoch 272/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1273 - accuracy: 0.9580\n",
            "Epoch 00272: val_loss did not improve from 0.15456\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1318 - accuracy: 0.9551 - val_loss: 0.1546 - val_accuracy: 0.9590\n",
            "Epoch 273/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1434 - accuracy: 0.9520\n",
            "Epoch 00273: val_loss improved from 0.15456 to 0.15362, saving model to /content/model/273-0.1536.hdf5\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.1317 - accuracy: 0.9526 - val_loss: 0.1536 - val_accuracy: 0.9590\n",
            "Epoch 274/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1320 - accuracy: 0.9520\n",
            "Epoch 00274: val_loss improved from 0.15362 to 0.15327, saving model to /content/model/274-0.1533.hdf5\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1304 - accuracy: 0.9538 - val_loss: 0.1533 - val_accuracy: 0.9590\n",
            "Epoch 275/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1280 - accuracy: 0.9600\n",
            "Epoch 00275: val_loss did not improve from 0.15327\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1307 - accuracy: 0.9551 - val_loss: 0.1538 - val_accuracy: 0.9590\n",
            "Epoch 276/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1337 - accuracy: 0.9540\n",
            "Epoch 00276: val_loss did not improve from 0.15327\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.1312 - accuracy: 0.9538 - val_loss: 0.1533 - val_accuracy: 0.9590\n",
            "Epoch 277/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1323 - accuracy: 0.9480\n",
            "Epoch 00277: val_loss improved from 0.15327 to 0.15313, saving model to /content/model/277-0.1531.hdf5\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1306 - accuracy: 0.9551 - val_loss: 0.1531 - val_accuracy: 0.9641\n",
            "Epoch 278/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1430 - accuracy: 0.9460\n",
            "Epoch 00278: val_loss improved from 0.15313 to 0.15231, saving model to /content/model/278-0.1523.hdf5\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.1296 - accuracy: 0.9538 - val_loss: 0.1523 - val_accuracy: 0.9590\n",
            "Epoch 279/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1219 - accuracy: 0.9560\n",
            "Epoch 00279: val_loss improved from 0.15231 to 0.15164, saving model to /content/model/279-0.1516.hdf5\n",
            "2/2 [==============================] - 0s 77ms/step - loss: 0.1295 - accuracy: 0.9538 - val_loss: 0.1516 - val_accuracy: 0.9590\n",
            "Epoch 280/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1329 - accuracy: 0.9500\n",
            "Epoch 00280: val_loss improved from 0.15164 to 0.15053, saving model to /content/model/280-0.1505.hdf5\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1295 - accuracy: 0.9538 - val_loss: 0.1505 - val_accuracy: 0.9590\n",
            "Epoch 281/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1185 - accuracy: 0.9580\n",
            "Epoch 00281: val_loss improved from 0.15053 to 0.15028, saving model to /content/model/281-0.1503.hdf5\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1288 - accuracy: 0.9551 - val_loss: 0.1503 - val_accuracy: 0.9590\n",
            "Epoch 282/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1230 - accuracy: 0.9580\n",
            "Epoch 00282: val_loss did not improve from 0.15028\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.1290 - accuracy: 0.9551 - val_loss: 0.1507 - val_accuracy: 0.9590\n",
            "Epoch 283/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1286 - accuracy: 0.9620\n",
            "Epoch 00283: val_loss did not improve from 0.15028\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.1283 - accuracy: 0.9551 - val_loss: 0.1504 - val_accuracy: 0.9641\n",
            "Epoch 284/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1283 - accuracy: 0.9480\n",
            "Epoch 00284: val_loss did not improve from 0.15028\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.1283 - accuracy: 0.9564 - val_loss: 0.1515 - val_accuracy: 0.9538\n",
            "Epoch 285/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1369 - accuracy: 0.9460\n",
            "Epoch 00285: val_loss did not improve from 0.15028\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1283 - accuracy: 0.9551 - val_loss: 0.1505 - val_accuracy: 0.9590\n",
            "Epoch 286/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1212 - accuracy: 0.9600\n",
            "Epoch 00286: val_loss improved from 0.15028 to 0.14940, saving model to /content/model/286-0.1494.hdf5\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1272 - accuracy: 0.9538 - val_loss: 0.1494 - val_accuracy: 0.9641\n",
            "Epoch 287/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1300 - accuracy: 0.9540\n",
            "Epoch 00287: val_loss improved from 0.14940 to 0.14929, saving model to /content/model/287-0.1493.hdf5\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.1276 - accuracy: 0.9551 - val_loss: 0.1493 - val_accuracy: 0.9590\n",
            "Epoch 288/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1242 - accuracy: 0.9580\n",
            "Epoch 00288: val_loss improved from 0.14929 to 0.14886, saving model to /content/model/288-0.1489.hdf5\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1276 - accuracy: 0.9564 - val_loss: 0.1489 - val_accuracy: 0.9641\n",
            "Epoch 289/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1269 - accuracy: 0.9520\n",
            "Epoch 00289: val_loss did not improve from 0.14886\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1268 - accuracy: 0.9538 - val_loss: 0.1493 - val_accuracy: 0.9590\n",
            "Epoch 290/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1248 - accuracy: 0.9580\n",
            "Epoch 00290: val_loss did not improve from 0.14886\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1272 - accuracy: 0.9538 - val_loss: 0.1489 - val_accuracy: 0.9590\n",
            "Epoch 291/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1375 - accuracy: 0.9480\n",
            "Epoch 00291: val_loss improved from 0.14886 to 0.14800, saving model to /content/model/291-0.1480.hdf5\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1264 - accuracy: 0.9538 - val_loss: 0.1480 - val_accuracy: 0.9641\n",
            "Epoch 292/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1137 - accuracy: 0.9600\n",
            "Epoch 00292: val_loss improved from 0.14800 to 0.14785, saving model to /content/model/292-0.1479.hdf5\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.1258 - accuracy: 0.9564 - val_loss: 0.1479 - val_accuracy: 0.9590\n",
            "Epoch 293/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1297 - accuracy: 0.9520\n",
            "Epoch 00293: val_loss improved from 0.14785 to 0.14669, saving model to /content/model/293-0.1467.hdf5\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1260 - accuracy: 0.9564 - val_loss: 0.1467 - val_accuracy: 0.9641\n",
            "Epoch 294/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1391 - accuracy: 0.9480\n",
            "Epoch 00294: val_loss improved from 0.14669 to 0.14599, saving model to /content/model/294-0.1460.hdf5\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.1255 - accuracy: 0.9564 - val_loss: 0.1460 - val_accuracy: 0.9641\n",
            "Epoch 295/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1054 - accuracy: 0.9720\n",
            "Epoch 00295: val_loss did not improve from 0.14599\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.1249 - accuracy: 0.9564 - val_loss: 0.1470 - val_accuracy: 0.9590\n",
            "Epoch 296/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1236 - accuracy: 0.9540\n",
            "Epoch 00296: val_loss did not improve from 0.14599\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1247 - accuracy: 0.9551 - val_loss: 0.1473 - val_accuracy: 0.9590\n",
            "Epoch 297/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1272 - accuracy: 0.9520\n",
            "Epoch 00297: val_loss did not improve from 0.14599\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1247 - accuracy: 0.9551 - val_loss: 0.1475 - val_accuracy: 0.9641\n",
            "Epoch 298/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1399 - accuracy: 0.9520\n",
            "Epoch 00298: val_loss did not improve from 0.14599\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1240 - accuracy: 0.9564 - val_loss: 0.1470 - val_accuracy: 0.9641\n",
            "Epoch 299/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1040 - accuracy: 0.9640\n",
            "Epoch 00299: val_loss did not improve from 0.14599\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.1245 - accuracy: 0.9564 - val_loss: 0.1468 - val_accuracy: 0.9641\n",
            "Epoch 300/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1107 - accuracy: 0.9580\n",
            "Epoch 00300: val_loss improved from 0.14599 to 0.14438, saving model to /content/model/300-0.1444.hdf5\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1238 - accuracy: 0.9564 - val_loss: 0.1444 - val_accuracy: 0.9641\n",
            "Epoch 301/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1183 - accuracy: 0.9580\n",
            "Epoch 00301: val_loss improved from 0.14438 to 0.14361, saving model to /content/model/301-0.1436.hdf5\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1229 - accuracy: 0.9564 - val_loss: 0.1436 - val_accuracy: 0.9590\n",
            "Epoch 302/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1201 - accuracy: 0.9580\n",
            "Epoch 00302: val_loss did not improve from 0.14361\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1243 - accuracy: 0.9564 - val_loss: 0.1440 - val_accuracy: 0.9590\n",
            "Epoch 303/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1224 - accuracy: 0.9560\n",
            "Epoch 00303: val_loss did not improve from 0.14361\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.1239 - accuracy: 0.9551 - val_loss: 0.1442 - val_accuracy: 0.9641\n",
            "Epoch 304/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1261 - accuracy: 0.9520\n",
            "Epoch 00304: val_loss did not improve from 0.14361\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.1219 - accuracy: 0.9577 - val_loss: 0.1479 - val_accuracy: 0.9590\n",
            "Epoch 305/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1252 - accuracy: 0.9520\n",
            "Epoch 00305: val_loss did not improve from 0.14361\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1246 - accuracy: 0.9564 - val_loss: 0.1496 - val_accuracy: 0.9590\n",
            "Epoch 306/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1353 - accuracy: 0.9500\n",
            "Epoch 00306: val_loss did not improve from 0.14361\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.1241 - accuracy: 0.9564 - val_loss: 0.1460 - val_accuracy: 0.9641\n",
            "Epoch 307/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1229 - accuracy: 0.9540\n",
            "Epoch 00307: val_loss did not improve from 0.14361\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1219 - accuracy: 0.9564 - val_loss: 0.1444 - val_accuracy: 0.9590\n",
            "Epoch 308/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1091 - accuracy: 0.9640\n",
            "Epoch 00308: val_loss improved from 0.14361 to 0.14355, saving model to /content/model/308-0.1436.hdf5\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.1229 - accuracy: 0.9564 - val_loss: 0.1436 - val_accuracy: 0.9590\n",
            "Epoch 309/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1283 - accuracy: 0.9540\n",
            "Epoch 00309: val_loss improved from 0.14355 to 0.14285, saving model to /content/model/309-0.1428.hdf5\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1218 - accuracy: 0.9551 - val_loss: 0.1428 - val_accuracy: 0.9641\n",
            "Epoch 310/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1092 - accuracy: 0.9660\n",
            "Epoch 00310: val_loss did not improve from 0.14285\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.1216 - accuracy: 0.9577 - val_loss: 0.1448 - val_accuracy: 0.9641\n",
            "Epoch 311/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1002 - accuracy: 0.9620\n",
            "Epoch 00311: val_loss did not improve from 0.14285\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.1221 - accuracy: 0.9564 - val_loss: 0.1437 - val_accuracy: 0.9641\n",
            "Epoch 312/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0978 - accuracy: 0.9640\n",
            "Epoch 00312: val_loss improved from 0.14285 to 0.14140, saving model to /content/model/312-0.1414.hdf5\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1205 - accuracy: 0.9577 - val_loss: 0.1414 - val_accuracy: 0.9641\n",
            "Epoch 313/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1056 - accuracy: 0.9660\n",
            "Epoch 00313: val_loss improved from 0.14140 to 0.14116, saving model to /content/model/313-0.1412.hdf5\n",
            "2/2 [==============================] - 0s 64ms/step - loss: 0.1203 - accuracy: 0.9577 - val_loss: 0.1412 - val_accuracy: 0.9590\n",
            "Epoch 314/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1129 - accuracy: 0.9620\n",
            "Epoch 00314: val_loss improved from 0.14116 to 0.14061, saving model to /content/model/314-0.1406.hdf5\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 0.1213 - accuracy: 0.9564 - val_loss: 0.1406 - val_accuracy: 0.9641\n",
            "Epoch 315/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1191 - accuracy: 0.9600\n",
            "Epoch 00315: val_loss did not improve from 0.14061\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.1201 - accuracy: 0.9577 - val_loss: 0.1422 - val_accuracy: 0.9641\n",
            "Epoch 316/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1075 - accuracy: 0.9660\n",
            "Epoch 00316: val_loss did not improve from 0.14061\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1202 - accuracy: 0.9577 - val_loss: 0.1437 - val_accuracy: 0.9641\n",
            "Epoch 317/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1413 - accuracy: 0.9500\n",
            "Epoch 00317: val_loss did not improve from 0.14061\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.1199 - accuracy: 0.9564 - val_loss: 0.1424 - val_accuracy: 0.9641\n",
            "Epoch 318/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1038 - accuracy: 0.9680\n",
            "Epoch 00318: val_loss did not improve from 0.14061\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1189 - accuracy: 0.9590 - val_loss: 0.1416 - val_accuracy: 0.9641\n",
            "Epoch 319/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1185 - accuracy: 0.9620\n",
            "Epoch 00319: val_loss improved from 0.14061 to 0.14033, saving model to /content/model/319-0.1403.hdf5\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 0.1192 - accuracy: 0.9590 - val_loss: 0.1403 - val_accuracy: 0.9590\n",
            "Epoch 320/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1063 - accuracy: 0.9660\n",
            "Epoch 00320: val_loss improved from 0.14033 to 0.13918, saving model to /content/model/320-0.1392.hdf5\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.1191 - accuracy: 0.9590 - val_loss: 0.1392 - val_accuracy: 0.9641\n",
            "Epoch 321/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1105 - accuracy: 0.9620\n",
            "Epoch 00321: val_loss did not improve from 0.13918\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1189 - accuracy: 0.9577 - val_loss: 0.1392 - val_accuracy: 0.9641\n",
            "Epoch 322/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1150 - accuracy: 0.9580\n",
            "Epoch 00322: val_loss improved from 0.13918 to 0.13809, saving model to /content/model/322-0.1381.hdf5\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1182 - accuracy: 0.9577 - val_loss: 0.1381 - val_accuracy: 0.9641\n",
            "Epoch 323/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1155 - accuracy: 0.9580\n",
            "Epoch 00323: val_loss improved from 0.13809 to 0.13742, saving model to /content/model/323-0.1374.hdf5\n",
            "2/2 [==============================] - 0s 85ms/step - loss: 0.1178 - accuracy: 0.9590 - val_loss: 0.1374 - val_accuracy: 0.9641\n",
            "Epoch 324/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1286 - accuracy: 0.9500\n",
            "Epoch 00324: val_loss did not improve from 0.13742\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.1185 - accuracy: 0.9577 - val_loss: 0.1380 - val_accuracy: 0.9641\n",
            "Epoch 325/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1206 - accuracy: 0.9560\n",
            "Epoch 00325: val_loss did not improve from 0.13742\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1172 - accuracy: 0.9577 - val_loss: 0.1405 - val_accuracy: 0.9641\n",
            "Epoch 326/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1095 - accuracy: 0.9600\n",
            "Epoch 00326: val_loss did not improve from 0.13742\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1172 - accuracy: 0.9590 - val_loss: 0.1424 - val_accuracy: 0.9641\n",
            "Epoch 327/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1037 - accuracy: 0.9640\n",
            "Epoch 00327: val_loss did not improve from 0.13742\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1175 - accuracy: 0.9590 - val_loss: 0.1411 - val_accuracy: 0.9641\n",
            "Epoch 328/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1326 - accuracy: 0.9500\n",
            "Epoch 00328: val_loss did not improve from 0.13742\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1175 - accuracy: 0.9577 - val_loss: 0.1385 - val_accuracy: 0.9641\n",
            "Epoch 329/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1059 - accuracy: 0.9620\n",
            "Epoch 00329: val_loss improved from 0.13742 to 0.13701, saving model to /content/model/329-0.1370.hdf5\n",
            "2/2 [==============================] - 0s 85ms/step - loss: 0.1170 - accuracy: 0.9577 - val_loss: 0.1370 - val_accuracy: 0.9641\n",
            "Epoch 330/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1076 - accuracy: 0.9600\n",
            "Epoch 00330: val_loss improved from 0.13701 to 0.13587, saving model to /content/model/330-0.1359.hdf5\n",
            "2/2 [==============================] - 0s 65ms/step - loss: 0.1160 - accuracy: 0.9577 - val_loss: 0.1359 - val_accuracy: 0.9641\n",
            "Epoch 331/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1092 - accuracy: 0.9620\n",
            "Epoch 00331: val_loss improved from 0.13587 to 0.13522, saving model to /content/model/331-0.1352.hdf5\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1164 - accuracy: 0.9577 - val_loss: 0.1352 - val_accuracy: 0.9641\n",
            "Epoch 332/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1029 - accuracy: 0.9600\n",
            "Epoch 00332: val_loss did not improve from 0.13522\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1158 - accuracy: 0.9577 - val_loss: 0.1354 - val_accuracy: 0.9641\n",
            "Epoch 333/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1173 - accuracy: 0.9580\n",
            "Epoch 00333: val_loss did not improve from 0.13522\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.1155 - accuracy: 0.9577 - val_loss: 0.1362 - val_accuracy: 0.9641\n",
            "Epoch 334/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1220 - accuracy: 0.9520\n",
            "Epoch 00334: val_loss did not improve from 0.13522\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.1149 - accuracy: 0.9590 - val_loss: 0.1380 - val_accuracy: 0.9641\n",
            "Epoch 335/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1176 - accuracy: 0.9600\n",
            "Epoch 00335: val_loss did not improve from 0.13522\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1148 - accuracy: 0.9577 - val_loss: 0.1396 - val_accuracy: 0.9641\n",
            "Epoch 336/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1294 - accuracy: 0.9520\n",
            "Epoch 00336: val_loss did not improve from 0.13522\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.1150 - accuracy: 0.9577 - val_loss: 0.1394 - val_accuracy: 0.9641\n",
            "Epoch 337/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1223 - accuracy: 0.9540\n",
            "Epoch 00337: val_loss did not improve from 0.13522\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.1147 - accuracy: 0.9577 - val_loss: 0.1383 - val_accuracy: 0.9641\n",
            "Epoch 338/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1084 - accuracy: 0.9600\n",
            "Epoch 00338: val_loss did not improve from 0.13522\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1142 - accuracy: 0.9577 - val_loss: 0.1368 - val_accuracy: 0.9641\n",
            "Epoch 339/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1145 - accuracy: 0.9560\n",
            "Epoch 00339: val_loss improved from 0.13522 to 0.13483, saving model to /content/model/339-0.1348.hdf5\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.1139 - accuracy: 0.9577 - val_loss: 0.1348 - val_accuracy: 0.9641\n",
            "Epoch 340/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1109 - accuracy: 0.9580\n",
            "Epoch 00340: val_loss improved from 0.13483 to 0.13363, saving model to /content/model/340-0.1336.hdf5\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.1139 - accuracy: 0.9577 - val_loss: 0.1336 - val_accuracy: 0.9641\n",
            "Epoch 341/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1197 - accuracy: 0.9540\n",
            "Epoch 00341: val_loss improved from 0.13363 to 0.13346, saving model to /content/model/341-0.1335.hdf5\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.1136 - accuracy: 0.9577 - val_loss: 0.1335 - val_accuracy: 0.9641\n",
            "Epoch 342/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1082 - accuracy: 0.9660\n",
            "Epoch 00342: val_loss did not improve from 0.13346\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1134 - accuracy: 0.9590 - val_loss: 0.1343 - val_accuracy: 0.9641\n",
            "Epoch 343/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1184 - accuracy: 0.9600\n",
            "Epoch 00343: val_loss did not improve from 0.13346\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.1131 - accuracy: 0.9590 - val_loss: 0.1354 - val_accuracy: 0.9641\n",
            "Epoch 344/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0975 - accuracy: 0.9700\n",
            "Epoch 00344: val_loss did not improve from 0.13346\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1126 - accuracy: 0.9590 - val_loss: 0.1354 - val_accuracy: 0.9641\n",
            "Epoch 345/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1084 - accuracy: 0.9620\n",
            "Epoch 00345: val_loss did not improve from 0.13346\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.1126 - accuracy: 0.9590 - val_loss: 0.1352 - val_accuracy: 0.9641\n",
            "Epoch 346/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1212 - accuracy: 0.9540\n",
            "Epoch 00346: val_loss did not improve from 0.13346\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.1123 - accuracy: 0.9590 - val_loss: 0.1359 - val_accuracy: 0.9641\n",
            "Epoch 347/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1348 - accuracy: 0.9480\n",
            "Epoch 00347: val_loss did not improve from 0.13346\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1120 - accuracy: 0.9577 - val_loss: 0.1365 - val_accuracy: 0.9641\n",
            "Epoch 348/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0996 - accuracy: 0.9640\n",
            "Epoch 00348: val_loss did not improve from 0.13346\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1127 - accuracy: 0.9590 - val_loss: 0.1362 - val_accuracy: 0.9641\n",
            "Epoch 349/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1097 - accuracy: 0.9620\n",
            "Epoch 00349: val_loss improved from 0.13346 to 0.13320, saving model to /content/model/349-0.1332.hdf5\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.1117 - accuracy: 0.9577 - val_loss: 0.1332 - val_accuracy: 0.9641\n",
            "Epoch 350/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1064 - accuracy: 0.9620\n",
            "Epoch 00350: val_loss improved from 0.13320 to 0.13235, saving model to /content/model/350-0.1324.hdf5\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.1113 - accuracy: 0.9603 - val_loss: 0.1324 - val_accuracy: 0.9641\n",
            "Epoch 351/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1213 - accuracy: 0.9560\n",
            "Epoch 00351: val_loss improved from 0.13235 to 0.13193, saving model to /content/model/351-0.1319.hdf5\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1121 - accuracy: 0.9603 - val_loss: 0.1319 - val_accuracy: 0.9641\n",
            "Epoch 352/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1212 - accuracy: 0.9540\n",
            "Epoch 00352: val_loss did not improve from 0.13193\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1110 - accuracy: 0.9590 - val_loss: 0.1340 - val_accuracy: 0.9641\n",
            "Epoch 353/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1182 - accuracy: 0.9580\n",
            "Epoch 00353: val_loss did not improve from 0.13193\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1106 - accuracy: 0.9577 - val_loss: 0.1367 - val_accuracy: 0.9641\n",
            "Epoch 354/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1199 - accuracy: 0.9540\n",
            "Epoch 00354: val_loss did not improve from 0.13193\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1115 - accuracy: 0.9590 - val_loss: 0.1374 - val_accuracy: 0.9641\n",
            "Epoch 355/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0873 - accuracy: 0.9680\n",
            "Epoch 00355: val_loss did not improve from 0.13193\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1113 - accuracy: 0.9577 - val_loss: 0.1345 - val_accuracy: 0.9641\n",
            "Epoch 356/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1088 - accuracy: 0.9620\n",
            "Epoch 00356: val_loss improved from 0.13193 to 0.13101, saving model to /content/model/356-0.1310.hdf5\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 0.1097 - accuracy: 0.9590 - val_loss: 0.1310 - val_accuracy: 0.9641\n",
            "Epoch 357/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1130 - accuracy: 0.9580\n",
            "Epoch 00357: val_loss improved from 0.13101 to 0.13020, saving model to /content/model/357-0.1302.hdf5\n",
            "2/2 [==============================] - 0s 63ms/step - loss: 0.1106 - accuracy: 0.9603 - val_loss: 0.1302 - val_accuracy: 0.9590\n",
            "Epoch 358/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1121 - accuracy: 0.9560\n",
            "Epoch 00358: val_loss did not improve from 0.13020\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1107 - accuracy: 0.9603 - val_loss: 0.1306 - val_accuracy: 0.9641\n",
            "Epoch 359/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1037 - accuracy: 0.9620\n",
            "Epoch 00359: val_loss did not improve from 0.13020\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.1111 - accuracy: 0.9590 - val_loss: 0.1345 - val_accuracy: 0.9641\n",
            "Epoch 360/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1094 - accuracy: 0.9560\n",
            "Epoch 00360: val_loss did not improve from 0.13020\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1103 - accuracy: 0.9590 - val_loss: 0.1318 - val_accuracy: 0.9641\n",
            "Epoch 361/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1139 - accuracy: 0.9560\n",
            "Epoch 00361: val_loss improved from 0.13020 to 0.12949, saving model to /content/model/361-0.1295.hdf5\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.1094 - accuracy: 0.9577 - val_loss: 0.1295 - val_accuracy: 0.9641\n",
            "Epoch 362/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1122 - accuracy: 0.9560\n",
            "Epoch 00362: val_loss did not improve from 0.12949\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1095 - accuracy: 0.9603 - val_loss: 0.1299 - val_accuracy: 0.9641\n",
            "Epoch 363/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1027 - accuracy: 0.9620\n",
            "Epoch 00363: val_loss did not improve from 0.12949\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.1085 - accuracy: 0.9603 - val_loss: 0.1328 - val_accuracy: 0.9641\n",
            "Epoch 364/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1207 - accuracy: 0.9560\n",
            "Epoch 00364: val_loss did not improve from 0.12949\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.1081 - accuracy: 0.9590 - val_loss: 0.1361 - val_accuracy: 0.9641\n",
            "Epoch 365/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1165 - accuracy: 0.9560\n",
            "Epoch 00365: val_loss did not improve from 0.12949\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1093 - accuracy: 0.9577 - val_loss: 0.1353 - val_accuracy: 0.9641\n",
            "Epoch 366/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1173 - accuracy: 0.9540\n",
            "Epoch 00366: val_loss did not improve from 0.12949\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1083 - accuracy: 0.9577 - val_loss: 0.1319 - val_accuracy: 0.9641\n",
            "Epoch 367/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1205 - accuracy: 0.9620\n",
            "Epoch 00367: val_loss did not improve from 0.12949\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.1084 - accuracy: 0.9603 - val_loss: 0.1300 - val_accuracy: 0.9590\n",
            "Epoch 368/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1155 - accuracy: 0.9580\n",
            "Epoch 00368: val_loss did not improve from 0.12949\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.1089 - accuracy: 0.9615 - val_loss: 0.1301 - val_accuracy: 0.9641\n",
            "Epoch 369/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1171 - accuracy: 0.9560\n",
            "Epoch 00369: val_loss did not improve from 0.12949\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1074 - accuracy: 0.9590 - val_loss: 0.1335 - val_accuracy: 0.9641\n",
            "Epoch 370/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0920 - accuracy: 0.9640\n",
            "Epoch 00370: val_loss did not improve from 0.12949\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.1094 - accuracy: 0.9590 - val_loss: 0.1331 - val_accuracy: 0.9641\n",
            "Epoch 371/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1187 - accuracy: 0.9540\n",
            "Epoch 00371: val_loss improved from 0.12949 to 0.12766, saving model to /content/model/371-0.1277.hdf5\n",
            "2/2 [==============================] - 0s 76ms/step - loss: 0.1079 - accuracy: 0.9590 - val_loss: 0.1277 - val_accuracy: 0.9641\n",
            "Epoch 372/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0971 - accuracy: 0.9620\n",
            "Epoch 00372: val_loss improved from 0.12766 to 0.12740, saving model to /content/model/372-0.1274.hdf5\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.1069 - accuracy: 0.9603 - val_loss: 0.1274 - val_accuracy: 0.9590\n",
            "Epoch 373/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1146 - accuracy: 0.9540\n",
            "Epoch 00373: val_loss did not improve from 0.12740\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.1076 - accuracy: 0.9615 - val_loss: 0.1276 - val_accuracy: 0.9641\n",
            "Epoch 374/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0972 - accuracy: 0.9640\n",
            "Epoch 00374: val_loss did not improve from 0.12740\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1069 - accuracy: 0.9590 - val_loss: 0.1301 - val_accuracy: 0.9641\n",
            "Epoch 375/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1164 - accuracy: 0.9500\n",
            "Epoch 00375: val_loss did not improve from 0.12740\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.1061 - accuracy: 0.9590 - val_loss: 0.1307 - val_accuracy: 0.9641\n",
            "Epoch 376/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1016 - accuracy: 0.9580\n",
            "Epoch 00376: val_loss did not improve from 0.12740\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.1062 - accuracy: 0.9590 - val_loss: 0.1297 - val_accuracy: 0.9641\n",
            "Epoch 377/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1111 - accuracy: 0.9580\n",
            "Epoch 00377: val_loss did not improve from 0.12740\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1065 - accuracy: 0.9603 - val_loss: 0.1281 - val_accuracy: 0.9641\n",
            "Epoch 378/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1305 - accuracy: 0.9500\n",
            "Epoch 00378: val_loss did not improve from 0.12740\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.1052 - accuracy: 0.9603 - val_loss: 0.1291 - val_accuracy: 0.9641\n",
            "Epoch 379/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1059 - accuracy: 0.9580\n",
            "Epoch 00379: val_loss did not improve from 0.12740\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.1052 - accuracy: 0.9590 - val_loss: 0.1301 - val_accuracy: 0.9641\n",
            "Epoch 380/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1035 - accuracy: 0.9620\n",
            "Epoch 00380: val_loss did not improve from 0.12740\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.1054 - accuracy: 0.9577 - val_loss: 0.1286 - val_accuracy: 0.9641\n",
            "Epoch 381/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1065 - accuracy: 0.9560\n",
            "Epoch 00381: val_loss improved from 0.12740 to 0.12645, saving model to /content/model/381-0.1265.hdf5\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.1047 - accuracy: 0.9590 - val_loss: 0.1265 - val_accuracy: 0.9641\n",
            "Epoch 382/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1032 - accuracy: 0.9640\n",
            "Epoch 00382: val_loss improved from 0.12645 to 0.12626, saving model to /content/model/382-0.1263.hdf5\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 0.1048 - accuracy: 0.9603 - val_loss: 0.1263 - val_accuracy: 0.9641\n",
            "Epoch 383/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0934 - accuracy: 0.9680\n",
            "Epoch 00383: val_loss did not improve from 0.12626\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1048 - accuracy: 0.9603 - val_loss: 0.1270 - val_accuracy: 0.9641\n",
            "Epoch 384/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1072 - accuracy: 0.9580\n",
            "Epoch 00384: val_loss did not improve from 0.12626\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.1042 - accuracy: 0.9603 - val_loss: 0.1278 - val_accuracy: 0.9641\n",
            "Epoch 385/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1045 - accuracy: 0.9640\n",
            "Epoch 00385: val_loss did not improve from 0.12626\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.1037 - accuracy: 0.9603 - val_loss: 0.1297 - val_accuracy: 0.9641\n",
            "Epoch 386/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1122 - accuracy: 0.9560\n",
            "Epoch 00386: val_loss did not improve from 0.12626\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.1040 - accuracy: 0.9590 - val_loss: 0.1308 - val_accuracy: 0.9641\n",
            "Epoch 387/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1074 - accuracy: 0.9500\n",
            "Epoch 00387: val_loss did not improve from 0.12626\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.1040 - accuracy: 0.9590 - val_loss: 0.1280 - val_accuracy: 0.9641\n",
            "Epoch 388/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0986 - accuracy: 0.9600\n",
            "Epoch 00388: val_loss improved from 0.12626 to 0.12544, saving model to /content/model/388-0.1254.hdf5\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1031 - accuracy: 0.9603 - val_loss: 0.1254 - val_accuracy: 0.9590\n",
            "Epoch 389/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1135 - accuracy: 0.9600\n",
            "Epoch 00389: val_loss improved from 0.12544 to 0.12529, saving model to /content/model/389-0.1253.hdf5\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 0.1044 - accuracy: 0.9641 - val_loss: 0.1253 - val_accuracy: 0.9641\n",
            "Epoch 390/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1074 - accuracy: 0.9600\n",
            "Epoch 00390: val_loss did not improve from 0.12529\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1029 - accuracy: 0.9603 - val_loss: 0.1284 - val_accuracy: 0.9641\n",
            "Epoch 391/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1013 - accuracy: 0.9600\n",
            "Epoch 00391: val_loss did not improve from 0.12529\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.1033 - accuracy: 0.9590 - val_loss: 0.1299 - val_accuracy: 0.9641\n",
            "Epoch 392/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0943 - accuracy: 0.9620\n",
            "Epoch 00392: val_loss did not improve from 0.12529\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.1033 - accuracy: 0.9577 - val_loss: 0.1262 - val_accuracy: 0.9641\n",
            "Epoch 393/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0858 - accuracy: 0.9720\n",
            "Epoch 00393: val_loss improved from 0.12529 to 0.12398, saving model to /content/model/393-0.1240.hdf5\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.1019 - accuracy: 0.9603 - val_loss: 0.1240 - val_accuracy: 0.9641\n",
            "Epoch 394/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1025 - accuracy: 0.9560\n",
            "Epoch 00394: val_loss improved from 0.12398 to 0.12391, saving model to /content/model/394-0.1239.hdf5\n",
            "2/2 [==============================] - 0s 98ms/step - loss: 0.1030 - accuracy: 0.9615 - val_loss: 0.1239 - val_accuracy: 0.9641\n",
            "Epoch 395/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0983 - accuracy: 0.9680\n",
            "Epoch 00395: val_loss did not improve from 0.12391\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.1025 - accuracy: 0.9628 - val_loss: 0.1260 - val_accuracy: 0.9641\n",
            "Epoch 396/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0971 - accuracy: 0.9600\n",
            "Epoch 00396: val_loss did not improve from 0.12391\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.1019 - accuracy: 0.9590 - val_loss: 0.1287 - val_accuracy: 0.9641\n",
            "Epoch 397/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1011 - accuracy: 0.9620\n",
            "Epoch 00397: val_loss did not improve from 0.12391\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.1022 - accuracy: 0.9577 - val_loss: 0.1263 - val_accuracy: 0.9641\n",
            "Epoch 398/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0912 - accuracy: 0.9660\n",
            "Epoch 00398: val_loss improved from 0.12391 to 0.12371, saving model to /content/model/398-0.1237.hdf5\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 0.1012 - accuracy: 0.9603 - val_loss: 0.1237 - val_accuracy: 0.9641\n",
            "Epoch 399/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1031 - accuracy: 0.9580\n",
            "Epoch 00399: val_loss improved from 0.12371 to 0.12330, saving model to /content/model/399-0.1233.hdf5\n",
            "2/2 [==============================] - 0s 67ms/step - loss: 0.1024 - accuracy: 0.9615 - val_loss: 0.1233 - val_accuracy: 0.9641\n",
            "Epoch 400/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1021 - accuracy: 0.9600\n",
            "Epoch 00400: val_loss did not improve from 0.12330\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.1019 - accuracy: 0.9615 - val_loss: 0.1263 - val_accuracy: 0.9641\n",
            "Epoch 401/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0857 - accuracy: 0.9660\n",
            "Epoch 00401: val_loss did not improve from 0.12330\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.1011 - accuracy: 0.9603 - val_loss: 0.1272 - val_accuracy: 0.9641\n",
            "Epoch 402/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1050 - accuracy: 0.9620\n",
            "Epoch 00402: val_loss did not improve from 0.12330\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.1013 - accuracy: 0.9603 - val_loss: 0.1251 - val_accuracy: 0.9641\n",
            "Epoch 403/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1063 - accuracy: 0.9560\n",
            "Epoch 00403: val_loss did not improve from 0.12330\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.1001 - accuracy: 0.9603 - val_loss: 0.1248 - val_accuracy: 0.9641\n",
            "Epoch 404/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1096 - accuracy: 0.9520\n",
            "Epoch 00404: val_loss did not improve from 0.12330\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.1000 - accuracy: 0.9603 - val_loss: 0.1246 - val_accuracy: 0.9641\n",
            "Epoch 405/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0894 - accuracy: 0.9600\n",
            "Epoch 00405: val_loss did not improve from 0.12330\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0998 - accuracy: 0.9603 - val_loss: 0.1245 - val_accuracy: 0.9641\n",
            "Epoch 406/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0931 - accuracy: 0.9620\n",
            "Epoch 00406: val_loss did not improve from 0.12330\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0997 - accuracy: 0.9603 - val_loss: 0.1235 - val_accuracy: 0.9641\n",
            "Epoch 407/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0653 - accuracy: 0.9760\n",
            "Epoch 00407: val_loss did not improve from 0.12330\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0995 - accuracy: 0.9603 - val_loss: 0.1233 - val_accuracy: 0.9641\n",
            "Epoch 408/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1076 - accuracy: 0.9540\n",
            "Epoch 00408: val_loss improved from 0.12330 to 0.12279, saving model to /content/model/408-0.1228.hdf5\n",
            "2/2 [==============================] - 0s 79ms/step - loss: 0.0994 - accuracy: 0.9603 - val_loss: 0.1228 - val_accuracy: 0.9641\n",
            "Epoch 409/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1018 - accuracy: 0.9560\n",
            "Epoch 00409: val_loss did not improve from 0.12279\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0993 - accuracy: 0.9603 - val_loss: 0.1235 - val_accuracy: 0.9641\n",
            "Epoch 410/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1026 - accuracy: 0.9560\n",
            "Epoch 00410: val_loss improved from 0.12279 to 0.12268, saving model to /content/model/410-0.1227.hdf5\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.0989 - accuracy: 0.9603 - val_loss: 0.1227 - val_accuracy: 0.9641\n",
            "Epoch 411/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1083 - accuracy: 0.9540\n",
            "Epoch 00411: val_loss improved from 0.12268 to 0.12262, saving model to /content/model/411-0.1226.hdf5\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.0987 - accuracy: 0.9603 - val_loss: 0.1226 - val_accuracy: 0.9641\n",
            "Epoch 412/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1056 - accuracy: 0.9580\n",
            "Epoch 00412: val_loss did not improve from 0.12262\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0983 - accuracy: 0.9603 - val_loss: 0.1239 - val_accuracy: 0.9641\n",
            "Epoch 413/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1082 - accuracy: 0.9560\n",
            "Epoch 00413: val_loss did not improve from 0.12262\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0982 - accuracy: 0.9603 - val_loss: 0.1243 - val_accuracy: 0.9641\n",
            "Epoch 414/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0916 - accuracy: 0.9600\n",
            "Epoch 00414: val_loss did not improve from 0.12262\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0983 - accuracy: 0.9603 - val_loss: 0.1238 - val_accuracy: 0.9641\n",
            "Epoch 415/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0910 - accuracy: 0.9660\n",
            "Epoch 00415: val_loss improved from 0.12262 to 0.12236, saving model to /content/model/415-0.1224.hdf5\n",
            "2/2 [==============================] - 0s 76ms/step - loss: 0.0981 - accuracy: 0.9603 - val_loss: 0.1224 - val_accuracy: 0.9641\n",
            "Epoch 416/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1041 - accuracy: 0.9600\n",
            "Epoch 00416: val_loss improved from 0.12236 to 0.12119, saving model to /content/model/416-0.1212.hdf5\n",
            "2/2 [==============================] - 0s 73ms/step - loss: 0.0981 - accuracy: 0.9641 - val_loss: 0.1212 - val_accuracy: 0.9641\n",
            "Epoch 417/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1010 - accuracy: 0.9620\n",
            "Epoch 00417: val_loss did not improve from 0.12119\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0976 - accuracy: 0.9628 - val_loss: 0.1227 - val_accuracy: 0.9641\n",
            "Epoch 418/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0908 - accuracy: 0.9600\n",
            "Epoch 00418: val_loss did not improve from 0.12119\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0975 - accuracy: 0.9603 - val_loss: 0.1244 - val_accuracy: 0.9641\n",
            "Epoch 419/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1087 - accuracy: 0.9500\n",
            "Epoch 00419: val_loss did not improve from 0.12119\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0975 - accuracy: 0.9590 - val_loss: 0.1230 - val_accuracy: 0.9641\n",
            "Epoch 420/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0875 - accuracy: 0.9680\n",
            "Epoch 00420: val_loss did not improve from 0.12119\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0970 - accuracy: 0.9615 - val_loss: 0.1225 - val_accuracy: 0.9641\n",
            "Epoch 421/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1058 - accuracy: 0.9540\n",
            "Epoch 00421: val_loss did not improve from 0.12119\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0966 - accuracy: 0.9615 - val_loss: 0.1226 - val_accuracy: 0.9641\n",
            "Epoch 422/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0998 - accuracy: 0.9620\n",
            "Epoch 00422: val_loss did not improve from 0.12119\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0966 - accuracy: 0.9603 - val_loss: 0.1227 - val_accuracy: 0.9641\n",
            "Epoch 423/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1049 - accuracy: 0.9540\n",
            "Epoch 00423: val_loss did not improve from 0.12119\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0966 - accuracy: 0.9603 - val_loss: 0.1218 - val_accuracy: 0.9641\n",
            "Epoch 424/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0926 - accuracy: 0.9620\n",
            "Epoch 00424: val_loss did not improve from 0.12119\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0961 - accuracy: 0.9628 - val_loss: 0.1226 - val_accuracy: 0.9641\n",
            "Epoch 425/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1064 - accuracy: 0.9560\n",
            "Epoch 00425: val_loss did not improve from 0.12119\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0961 - accuracy: 0.9603 - val_loss: 0.1221 - val_accuracy: 0.9641\n",
            "Epoch 426/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0963 - accuracy: 0.9560\n",
            "Epoch 00426: val_loss did not improve from 0.12119\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0957 - accuracy: 0.9603 - val_loss: 0.1213 - val_accuracy: 0.9641\n",
            "Epoch 427/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0836 - accuracy: 0.9740\n",
            "Epoch 00427: val_loss improved from 0.12119 to 0.12074, saving model to /content/model/427-0.1207.hdf5\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.0959 - accuracy: 0.9654 - val_loss: 0.1207 - val_accuracy: 0.9641\n",
            "Epoch 428/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0764 - accuracy: 0.9800\n",
            "Epoch 00428: val_loss improved from 0.12074 to 0.12011, saving model to /content/model/428-0.1201.hdf5\n",
            "2/2 [==============================] - 0s 80ms/step - loss: 0.0956 - accuracy: 0.9654 - val_loss: 0.1201 - val_accuracy: 0.9641\n",
            "Epoch 429/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1064 - accuracy: 0.9620\n",
            "Epoch 00429: val_loss improved from 0.12011 to 0.11993, saving model to /content/model/429-0.1199.hdf5\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 0.0958 - accuracy: 0.9654 - val_loss: 0.1199 - val_accuracy: 0.9641\n",
            "Epoch 430/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1033 - accuracy: 0.9620\n",
            "Epoch 00430: val_loss did not improve from 0.11993\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0954 - accuracy: 0.9654 - val_loss: 0.1215 - val_accuracy: 0.9641\n",
            "Epoch 431/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1100 - accuracy: 0.9620\n",
            "Epoch 00431: val_loss did not improve from 0.11993\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0947 - accuracy: 0.9654 - val_loss: 0.1246 - val_accuracy: 0.9641\n",
            "Epoch 432/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0862 - accuracy: 0.9640\n",
            "Epoch 00432: val_loss did not improve from 0.11993\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0953 - accuracy: 0.9590 - val_loss: 0.1249 - val_accuracy: 0.9641\n",
            "Epoch 433/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0710 - accuracy: 0.9700\n",
            "Epoch 00433: val_loss did not improve from 0.11993\n",
            "2/2 [==============================] - 0s 68ms/step - loss: 0.0950 - accuracy: 0.9603 - val_loss: 0.1211 - val_accuracy: 0.9641\n",
            "Epoch 434/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0894 - accuracy: 0.9700\n",
            "Epoch 00434: val_loss improved from 0.11993 to 0.11917, saving model to /content/model/434-0.1192.hdf5\n",
            "2/2 [==============================] - 0s 76ms/step - loss: 0.0948 - accuracy: 0.9654 - val_loss: 0.1192 - val_accuracy: 0.9590\n",
            "Epoch 435/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1088 - accuracy: 0.9560\n",
            "Epoch 00435: val_loss did not improve from 0.11917\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0955 - accuracy: 0.9654 - val_loss: 0.1196 - val_accuracy: 0.9641\n",
            "Epoch 436/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0762 - accuracy: 0.9740\n",
            "Epoch 00436: val_loss did not improve from 0.11917\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0964 - accuracy: 0.9628 - val_loss: 0.1223 - val_accuracy: 0.9641\n",
            "Epoch 437/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0925 - accuracy: 0.9680\n",
            "Epoch 00437: val_loss improved from 0.11917 to 0.11915, saving model to /content/model/437-0.1192.hdf5\n",
            "2/2 [==============================] - 0s 76ms/step - loss: 0.0941 - accuracy: 0.9628 - val_loss: 0.1192 - val_accuracy: 0.9641\n",
            "Epoch 438/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0925 - accuracy: 0.9620\n",
            "Epoch 00438: val_loss improved from 0.11915 to 0.11883, saving model to /content/model/438-0.1188.hdf5\n",
            "2/2 [==============================] - 0s 96ms/step - loss: 0.0939 - accuracy: 0.9654 - val_loss: 0.1188 - val_accuracy: 0.9641\n",
            "Epoch 439/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1064 - accuracy: 0.9580\n",
            "Epoch 00439: val_loss did not improve from 0.11883\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0936 - accuracy: 0.9654 - val_loss: 0.1205 - val_accuracy: 0.9641\n",
            "Epoch 440/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0696 - accuracy: 0.9740\n",
            "Epoch 00440: val_loss did not improve from 0.11883\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0946 - accuracy: 0.9615 - val_loss: 0.1220 - val_accuracy: 0.9641\n",
            "Epoch 441/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0912 - accuracy: 0.9660\n",
            "Epoch 00441: val_loss did not improve from 0.11883\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0928 - accuracy: 0.9654 - val_loss: 0.1189 - val_accuracy: 0.9641\n",
            "Epoch 442/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0969 - accuracy: 0.9640\n",
            "Epoch 00442: val_loss improved from 0.11883 to 0.11836, saving model to /content/model/442-0.1184.hdf5\n",
            "2/2 [==============================] - 0s 89ms/step - loss: 0.0937 - accuracy: 0.9667 - val_loss: 0.1184 - val_accuracy: 0.9590\n",
            "Epoch 443/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0894 - accuracy: 0.9680\n",
            "Epoch 00443: val_loss did not improve from 0.11836\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0930 - accuracy: 0.9654 - val_loss: 0.1211 - val_accuracy: 0.9641\n",
            "Epoch 444/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0884 - accuracy: 0.9700\n",
            "Epoch 00444: val_loss did not improve from 0.11836\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0925 - accuracy: 0.9654 - val_loss: 0.1238 - val_accuracy: 0.9641\n",
            "Epoch 445/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1043 - accuracy: 0.9520\n",
            "Epoch 00445: val_loss did not improve from 0.11836\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0932 - accuracy: 0.9590 - val_loss: 0.1215 - val_accuracy: 0.9641\n",
            "Epoch 446/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1071 - accuracy: 0.9540\n",
            "Epoch 00446: val_loss did not improve from 0.11836\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0928 - accuracy: 0.9628 - val_loss: 0.1192 - val_accuracy: 0.9641\n",
            "Epoch 447/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0782 - accuracy: 0.9720\n",
            "Epoch 00447: val_loss did not improve from 0.11836\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0920 - accuracy: 0.9654 - val_loss: 0.1193 - val_accuracy: 0.9641\n",
            "Epoch 448/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0884 - accuracy: 0.9700\n",
            "Epoch 00448: val_loss did not improve from 0.11836\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0918 - accuracy: 0.9654 - val_loss: 0.1194 - val_accuracy: 0.9641\n",
            "Epoch 449/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0811 - accuracy: 0.9640\n",
            "Epoch 00449: val_loss did not improve from 0.11836\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0917 - accuracy: 0.9654 - val_loss: 0.1205 - val_accuracy: 0.9641\n",
            "Epoch 450/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0809 - accuracy: 0.9720\n",
            "Epoch 00450: val_loss did not improve from 0.11836\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0913 - accuracy: 0.9654 - val_loss: 0.1195 - val_accuracy: 0.9641\n",
            "Epoch 451/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1024 - accuracy: 0.9600\n",
            "Epoch 00451: val_loss did not improve from 0.11836\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0913 - accuracy: 0.9654 - val_loss: 0.1186 - val_accuracy: 0.9641\n",
            "Epoch 452/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1090 - accuracy: 0.9600\n",
            "Epoch 00452: val_loss did not improve from 0.11836\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0912 - accuracy: 0.9654 - val_loss: 0.1190 - val_accuracy: 0.9641\n",
            "Epoch 453/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0801 - accuracy: 0.9740\n",
            "Epoch 00453: val_loss did not improve from 0.11836\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0921 - accuracy: 0.9628 - val_loss: 0.1200 - val_accuracy: 0.9641\n",
            "Epoch 454/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0886 - accuracy: 0.9640\n",
            "Epoch 00454: val_loss improved from 0.11836 to 0.11794, saving model to /content/model/454-0.1179.hdf5\n",
            "2/2 [==============================] - 0s 81ms/step - loss: 0.0904 - accuracy: 0.9654 - val_loss: 0.1179 - val_accuracy: 0.9590\n",
            "Epoch 455/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0972 - accuracy: 0.9640\n",
            "Epoch 00455: val_loss did not improve from 0.11794\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0916 - accuracy: 0.9679 - val_loss: 0.1180 - val_accuracy: 0.9590\n",
            "Epoch 456/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0844 - accuracy: 0.9700\n",
            "Epoch 00456: val_loss did not improve from 0.11794\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0907 - accuracy: 0.9679 - val_loss: 0.1222 - val_accuracy: 0.9641\n",
            "Epoch 457/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0822 - accuracy: 0.9660\n",
            "Epoch 00457: val_loss did not improve from 0.11794\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0917 - accuracy: 0.9641 - val_loss: 0.1266 - val_accuracy: 0.9590\n",
            "Epoch 458/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0893 - accuracy: 0.9580\n",
            "Epoch 00458: val_loss did not improve from 0.11794\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0925 - accuracy: 0.9603 - val_loss: 0.1190 - val_accuracy: 0.9641\n",
            "Epoch 459/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0937 - accuracy: 0.9620\n",
            "Epoch 00459: val_loss improved from 0.11794 to 0.11700, saving model to /content/model/459-0.1170.hdf5\n",
            "2/2 [==============================] - 0s 81ms/step - loss: 0.0900 - accuracy: 0.9641 - val_loss: 0.1170 - val_accuracy: 0.9641\n",
            "Epoch 460/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0956 - accuracy: 0.9640\n",
            "Epoch 00460: val_loss improved from 0.11700 to 0.11687, saving model to /content/model/460-0.1169.hdf5\n",
            "2/2 [==============================] - 0s 82ms/step - loss: 0.0914 - accuracy: 0.9679 - val_loss: 0.1169 - val_accuracy: 0.9692\n",
            "Epoch 461/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0766 - accuracy: 0.9720\n",
            "Epoch 00461: val_loss did not improve from 0.11687\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0909 - accuracy: 0.9654 - val_loss: 0.1206 - val_accuracy: 0.9641\n",
            "Epoch 462/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0840 - accuracy: 0.9620\n",
            "Epoch 00462: val_loss did not improve from 0.11687\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0898 - accuracy: 0.9628 - val_loss: 0.1197 - val_accuracy: 0.9641\n",
            "Epoch 463/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0790 - accuracy: 0.9700\n",
            "Epoch 00463: val_loss did not improve from 0.11687\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0892 - accuracy: 0.9654 - val_loss: 0.1174 - val_accuracy: 0.9590\n",
            "Epoch 464/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0915 - accuracy: 0.9660\n",
            "Epoch 00464: val_loss did not improve from 0.11687\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0898 - accuracy: 0.9679 - val_loss: 0.1171 - val_accuracy: 0.9590\n",
            "Epoch 465/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0887 - accuracy: 0.9680\n",
            "Epoch 00465: val_loss did not improve from 0.11687\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0888 - accuracy: 0.9654 - val_loss: 0.1198 - val_accuracy: 0.9641\n",
            "Epoch 466/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1056 - accuracy: 0.9560\n",
            "Epoch 00466: val_loss did not improve from 0.11687\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0889 - accuracy: 0.9641 - val_loss: 0.1210 - val_accuracy: 0.9590\n",
            "Epoch 467/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0961 - accuracy: 0.9540\n",
            "Epoch 00467: val_loss did not improve from 0.11687\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.0891 - accuracy: 0.9615 - val_loss: 0.1187 - val_accuracy: 0.9641\n",
            "Epoch 468/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0810 - accuracy: 0.9660\n",
            "Epoch 00468: val_loss did not improve from 0.11687\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0882 - accuracy: 0.9654 - val_loss: 0.1175 - val_accuracy: 0.9590\n",
            "Epoch 469/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0930 - accuracy: 0.9640\n",
            "Epoch 00469: val_loss did not improve from 0.11687\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0893 - accuracy: 0.9654 - val_loss: 0.1178 - val_accuracy: 0.9590\n",
            "Epoch 470/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0871 - accuracy: 0.9680\n",
            "Epoch 00470: val_loss did not improve from 0.11687\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0876 - accuracy: 0.9654 - val_loss: 0.1226 - val_accuracy: 0.9590\n",
            "Epoch 471/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1009 - accuracy: 0.9600\n",
            "Epoch 00471: val_loss did not improve from 0.11687\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0886 - accuracy: 0.9641 - val_loss: 0.1242 - val_accuracy: 0.9590\n",
            "Epoch 472/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0932 - accuracy: 0.9580\n",
            "Epoch 00472: val_loss did not improve from 0.11687\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.0892 - accuracy: 0.9615 - val_loss: 0.1209 - val_accuracy: 0.9590\n",
            "Epoch 473/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1048 - accuracy: 0.9580\n",
            "Epoch 00473: val_loss did not improve from 0.11687\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0886 - accuracy: 0.9654 - val_loss: 0.1173 - val_accuracy: 0.9641\n",
            "Epoch 474/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0684 - accuracy: 0.9740\n",
            "Epoch 00474: val_loss did not improve from 0.11687\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0879 - accuracy: 0.9667 - val_loss: 0.1174 - val_accuracy: 0.9641\n",
            "Epoch 475/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0995 - accuracy: 0.9580\n",
            "Epoch 00475: val_loss did not improve from 0.11687\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0874 - accuracy: 0.9667 - val_loss: 0.1171 - val_accuracy: 0.9692\n",
            "Epoch 476/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0904 - accuracy: 0.9580\n",
            "Epoch 00476: val_loss did not improve from 0.11687\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0869 - accuracy: 0.9654 - val_loss: 0.1192 - val_accuracy: 0.9590\n",
            "Epoch 477/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1031 - accuracy: 0.9540\n",
            "Epoch 00477: val_loss did not improve from 0.11687\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0869 - accuracy: 0.9641 - val_loss: 0.1218 - val_accuracy: 0.9590\n",
            "Epoch 478/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0883 - accuracy: 0.9620\n",
            "Epoch 00478: val_loss did not improve from 0.11687\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0877 - accuracy: 0.9615 - val_loss: 0.1196 - val_accuracy: 0.9590\n",
            "Epoch 479/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0725 - accuracy: 0.9660\n",
            "Epoch 00479: val_loss improved from 0.11687 to 0.11660, saving model to /content/model/479-0.1166.hdf5\n",
            "2/2 [==============================] - 0s 78ms/step - loss: 0.0864 - accuracy: 0.9641 - val_loss: 0.1166 - val_accuracy: 0.9641\n",
            "Epoch 480/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0853 - accuracy: 0.9680\n",
            "Epoch 00480: val_loss improved from 0.11660 to 0.11645, saving model to /content/model/480-0.1164.hdf5\n",
            "2/2 [==============================] - 0s 77ms/step - loss: 0.0871 - accuracy: 0.9679 - val_loss: 0.1164 - val_accuracy: 0.9590\n",
            "Epoch 481/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0978 - accuracy: 0.9640\n",
            "Epoch 00481: val_loss did not improve from 0.11645\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0870 - accuracy: 0.9692 - val_loss: 0.1183 - val_accuracy: 0.9641\n",
            "Epoch 482/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1052 - accuracy: 0.9560\n",
            "Epoch 00482: val_loss did not improve from 0.11645\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0857 - accuracy: 0.9654 - val_loss: 0.1226 - val_accuracy: 0.9590\n",
            "Epoch 483/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0938 - accuracy: 0.9600\n",
            "Epoch 00483: val_loss did not improve from 0.11645\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0872 - accuracy: 0.9628 - val_loss: 0.1209 - val_accuracy: 0.9590\n",
            "Epoch 484/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0942 - accuracy: 0.9600\n",
            "Epoch 00484: val_loss did not improve from 0.11645\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.0861 - accuracy: 0.9654 - val_loss: 0.1166 - val_accuracy: 0.9641\n",
            "Epoch 485/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0855 - accuracy: 0.9640\n",
            "Epoch 00485: val_loss improved from 0.11645 to 0.11607, saving model to /content/model/485-0.1161.hdf5\n",
            "2/2 [==============================] - 0s 72ms/step - loss: 0.0857 - accuracy: 0.9679 - val_loss: 0.1161 - val_accuracy: 0.9641\n",
            "Epoch 486/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0832 - accuracy: 0.9700\n",
            "Epoch 00486: val_loss did not improve from 0.11607\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0864 - accuracy: 0.9692 - val_loss: 0.1172 - val_accuracy: 0.9641\n",
            "Epoch 487/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0848 - accuracy: 0.9680\n",
            "Epoch 00487: val_loss did not improve from 0.11607\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0857 - accuracy: 0.9667 - val_loss: 0.1196 - val_accuracy: 0.9641\n",
            "Epoch 488/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.1000 - accuracy: 0.9580\n",
            "Epoch 00488: val_loss did not improve from 0.11607\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0854 - accuracy: 0.9654 - val_loss: 0.1177 - val_accuracy: 0.9641\n",
            "Epoch 489/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0703 - accuracy: 0.9680\n",
            "Epoch 00489: val_loss did not improve from 0.11607\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0848 - accuracy: 0.9654 - val_loss: 0.1167 - val_accuracy: 0.9641\n",
            "Epoch 490/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0880 - accuracy: 0.9660\n",
            "Epoch 00490: val_loss did not improve from 0.11607\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0852 - accuracy: 0.9667 - val_loss: 0.1162 - val_accuracy: 0.9641\n",
            "Epoch 491/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0781 - accuracy: 0.9640\n",
            "Epoch 00491: val_loss did not improve from 0.11607\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0851 - accuracy: 0.9654 - val_loss: 0.1183 - val_accuracy: 0.9641\n",
            "Epoch 492/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0851 - accuracy: 0.9640\n",
            "Epoch 00492: val_loss did not improve from 0.11607\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0845 - accuracy: 0.9654 - val_loss: 0.1172 - val_accuracy: 0.9692\n",
            "Epoch 493/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0928 - accuracy: 0.9620\n",
            "Epoch 00493: val_loss did not improve from 0.11607\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0845 - accuracy: 0.9667 - val_loss: 0.1162 - val_accuracy: 0.9641\n",
            "Epoch 494/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0907 - accuracy: 0.9660\n",
            "Epoch 00494: val_loss did not improve from 0.11607\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0842 - accuracy: 0.9692 - val_loss: 0.1173 - val_accuracy: 0.9641\n",
            "Epoch 495/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0727 - accuracy: 0.9700\n",
            "Epoch 00495: val_loss did not improve from 0.11607\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.0844 - accuracy: 0.9654 - val_loss: 0.1180 - val_accuracy: 0.9641\n",
            "Epoch 496/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0923 - accuracy: 0.9620\n",
            "Epoch 00496: val_loss did not improve from 0.11607\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0841 - accuracy: 0.9654 - val_loss: 0.1162 - val_accuracy: 0.9641\n",
            "Epoch 497/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0815 - accuracy: 0.9660\n",
            "Epoch 00497: val_loss did not improve from 0.11607\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0838 - accuracy: 0.9654 - val_loss: 0.1167 - val_accuracy: 0.9692\n",
            "Epoch 498/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0899 - accuracy: 0.9580\n",
            "Epoch 00498: val_loss did not improve from 0.11607\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0836 - accuracy: 0.9654 - val_loss: 0.1182 - val_accuracy: 0.9641\n",
            "Epoch 499/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0840 - accuracy: 0.9640\n",
            "Epoch 00499: val_loss did not improve from 0.11607\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0836 - accuracy: 0.9641 - val_loss: 0.1167 - val_accuracy: 0.9692\n",
            "Epoch 500/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0738 - accuracy: 0.9660\n",
            "Epoch 00500: val_loss improved from 0.11607 to 0.11554, saving model to /content/model/500-0.1155.hdf5\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.0835 - accuracy: 0.9654 - val_loss: 0.1155 - val_accuracy: 0.9590\n",
            "Epoch 501/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0886 - accuracy: 0.9700\n",
            "Epoch 00501: val_loss did not improve from 0.11554\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0833 - accuracy: 0.9692 - val_loss: 0.1166 - val_accuracy: 0.9641\n",
            "Epoch 502/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0773 - accuracy: 0.9680\n",
            "Epoch 00502: val_loss did not improve from 0.11554\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0835 - accuracy: 0.9654 - val_loss: 0.1179 - val_accuracy: 0.9641\n",
            "Epoch 503/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0816 - accuracy: 0.9620\n",
            "Epoch 00503: val_loss did not improve from 0.11554\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0830 - accuracy: 0.9679 - val_loss: 0.1161 - val_accuracy: 0.9641\n",
            "Epoch 504/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0892 - accuracy: 0.9660\n",
            "Epoch 00504: val_loss did not improve from 0.11554\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0825 - accuracy: 0.9679 - val_loss: 0.1162 - val_accuracy: 0.9641\n",
            "Epoch 505/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0775 - accuracy: 0.9700\n",
            "Epoch 00505: val_loss did not improve from 0.11554\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0823 - accuracy: 0.9679 - val_loss: 0.1167 - val_accuracy: 0.9590\n",
            "Epoch 506/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0799 - accuracy: 0.9640\n",
            "Epoch 00506: val_loss did not improve from 0.11554\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0823 - accuracy: 0.9654 - val_loss: 0.1163 - val_accuracy: 0.9641\n",
            "Epoch 507/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0933 - accuracy: 0.9600\n",
            "Epoch 00507: val_loss did not improve from 0.11554\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0833 - accuracy: 0.9654 - val_loss: 0.1159 - val_accuracy: 0.9590\n",
            "Epoch 508/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0802 - accuracy: 0.9680\n",
            "Epoch 00508: val_loss did not improve from 0.11554\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0818 - accuracy: 0.9667 - val_loss: 0.1185 - val_accuracy: 0.9641\n",
            "Epoch 509/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0847 - accuracy: 0.9620\n",
            "Epoch 00509: val_loss did not improve from 0.11554\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0822 - accuracy: 0.9641 - val_loss: 0.1171 - val_accuracy: 0.9641\n",
            "Epoch 510/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0750 - accuracy: 0.9660\n",
            "Epoch 00510: val_loss improved from 0.11554 to 0.11530, saving model to /content/model/510-0.1153.hdf5\n",
            "2/2 [==============================] - 0s 81ms/step - loss: 0.0816 - accuracy: 0.9641 - val_loss: 0.1153 - val_accuracy: 0.9590\n",
            "Epoch 511/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0741 - accuracy: 0.9720\n",
            "Epoch 00511: val_loss did not improve from 0.11530\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0815 - accuracy: 0.9692 - val_loss: 0.1153 - val_accuracy: 0.9590\n",
            "Epoch 512/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0776 - accuracy: 0.9660\n",
            "Epoch 00512: val_loss did not improve from 0.11530\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0813 - accuracy: 0.9679 - val_loss: 0.1165 - val_accuracy: 0.9641\n",
            "Epoch 513/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0729 - accuracy: 0.9720\n",
            "Epoch 00513: val_loss did not improve from 0.11530\n",
            "2/2 [==============================] - 0s 50ms/step - loss: 0.0818 - accuracy: 0.9654 - val_loss: 0.1165 - val_accuracy: 0.9641\n",
            "Epoch 514/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0838 - accuracy: 0.9680\n",
            "Epoch 00514: val_loss improved from 0.11530 to 0.11530, saving model to /content/model/514-0.1153.hdf5\n",
            "2/2 [==============================] - 0s 70ms/step - loss: 0.0812 - accuracy: 0.9667 - val_loss: 0.1153 - val_accuracy: 0.9590\n",
            "Epoch 515/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0818 - accuracy: 0.9680\n",
            "Epoch 00515: val_loss did not improve from 0.11530\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0813 - accuracy: 0.9679 - val_loss: 0.1159 - val_accuracy: 0.9538\n",
            "Epoch 516/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0870 - accuracy: 0.9680\n",
            "Epoch 00516: val_loss did not improve from 0.11530\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0806 - accuracy: 0.9679 - val_loss: 0.1164 - val_accuracy: 0.9590\n",
            "Epoch 517/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0950 - accuracy: 0.9600\n",
            "Epoch 00517: val_loss did not improve from 0.11530\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0803 - accuracy: 0.9654 - val_loss: 0.1175 - val_accuracy: 0.9641\n",
            "Epoch 518/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0838 - accuracy: 0.9600\n",
            "Epoch 00518: val_loss did not improve from 0.11530\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.0804 - accuracy: 0.9654 - val_loss: 0.1171 - val_accuracy: 0.9590\n",
            "Epoch 519/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0743 - accuracy: 0.9680\n",
            "Epoch 00519: val_loss did not improve from 0.11530\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0801 - accuracy: 0.9654 - val_loss: 0.1161 - val_accuracy: 0.9538\n",
            "Epoch 520/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0746 - accuracy: 0.9720\n",
            "Epoch 00520: val_loss did not improve from 0.11530\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0798 - accuracy: 0.9667 - val_loss: 0.1158 - val_accuracy: 0.9538\n",
            "Epoch 521/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0732 - accuracy: 0.9760\n",
            "Epoch 00521: val_loss did not improve from 0.11530\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0798 - accuracy: 0.9692 - val_loss: 0.1157 - val_accuracy: 0.9538\n",
            "Epoch 522/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0995 - accuracy: 0.9580\n",
            "Epoch 00522: val_loss did not improve from 0.11530\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0800 - accuracy: 0.9705 - val_loss: 0.1155 - val_accuracy: 0.9590\n",
            "Epoch 523/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0800 - accuracy: 0.9700\n",
            "Epoch 00523: val_loss did not improve from 0.11530\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0792 - accuracy: 0.9692 - val_loss: 0.1174 - val_accuracy: 0.9641\n",
            "Epoch 524/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0733 - accuracy: 0.9740\n",
            "Epoch 00524: val_loss did not improve from 0.11530\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0796 - accuracy: 0.9641 - val_loss: 0.1173 - val_accuracy: 0.9641\n",
            "Epoch 525/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0810 - accuracy: 0.9660\n",
            "Epoch 00525: val_loss improved from 0.11530 to 0.11517, saving model to /content/model/525-0.1152.hdf5\n",
            "2/2 [==============================] - 0s 76ms/step - loss: 0.0794 - accuracy: 0.9667 - val_loss: 0.1152 - val_accuracy: 0.9538\n",
            "Epoch 526/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0895 - accuracy: 0.9640\n",
            "Epoch 00526: val_loss did not improve from 0.11517\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0791 - accuracy: 0.9705 - val_loss: 0.1152 - val_accuracy: 0.9538\n",
            "Epoch 527/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0843 - accuracy: 0.9680\n",
            "Epoch 00527: val_loss did not improve from 0.11517\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0788 - accuracy: 0.9705 - val_loss: 0.1174 - val_accuracy: 0.9641\n",
            "Epoch 528/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0732 - accuracy: 0.9700\n",
            "Epoch 00528: val_loss did not improve from 0.11517\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0802 - accuracy: 0.9679 - val_loss: 0.1192 - val_accuracy: 0.9641\n",
            "Epoch 529/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0821 - accuracy: 0.9660\n",
            "Epoch 00529: val_loss improved from 0.11517 to 0.11481, saving model to /content/model/529-0.1148.hdf5\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 0.0798 - accuracy: 0.9679 - val_loss: 0.1148 - val_accuracy: 0.9590\n",
            "Epoch 530/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0743 - accuracy: 0.9740\n",
            "Epoch 00530: val_loss improved from 0.11481 to 0.11464, saving model to /content/model/530-0.1146.hdf5\n",
            "2/2 [==============================] - 0s 75ms/step - loss: 0.0789 - accuracy: 0.9705 - val_loss: 0.1146 - val_accuracy: 0.9590\n",
            "Epoch 531/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0783 - accuracy: 0.9720\n",
            "Epoch 00531: val_loss did not improve from 0.11464\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0788 - accuracy: 0.9718 - val_loss: 0.1148 - val_accuracy: 0.9538\n",
            "Epoch 532/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0740 - accuracy: 0.9720\n",
            "Epoch 00532: val_loss did not improve from 0.11464\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0780 - accuracy: 0.9705 - val_loss: 0.1179 - val_accuracy: 0.9641\n",
            "Epoch 533/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0713 - accuracy: 0.9740\n",
            "Epoch 00533: val_loss did not improve from 0.11464\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0791 - accuracy: 0.9654 - val_loss: 0.1163 - val_accuracy: 0.9641\n",
            "Epoch 534/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0907 - accuracy: 0.9600\n",
            "Epoch 00534: val_loss improved from 0.11464 to 0.11449, saving model to /content/model/534-0.1145.hdf5\n",
            "2/2 [==============================] - 0s 74ms/step - loss: 0.0810 - accuracy: 0.9667 - val_loss: 0.1145 - val_accuracy: 0.9590\n",
            "Epoch 535/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0838 - accuracy: 0.9700\n",
            "Epoch 00535: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.0778 - accuracy: 0.9705 - val_loss: 0.1180 - val_accuracy: 0.9641\n",
            "Epoch 536/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0709 - accuracy: 0.9660\n",
            "Epoch 00536: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0785 - accuracy: 0.9667 - val_loss: 0.1181 - val_accuracy: 0.9641\n",
            "Epoch 537/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0697 - accuracy: 0.9720\n",
            "Epoch 00537: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0774 - accuracy: 0.9705 - val_loss: 0.1147 - val_accuracy: 0.9590\n",
            "Epoch 538/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0784 - accuracy: 0.9740\n",
            "Epoch 00538: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0775 - accuracy: 0.9718 - val_loss: 0.1148 - val_accuracy: 0.9590\n",
            "Epoch 539/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0795 - accuracy: 0.9720\n",
            "Epoch 00539: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0776 - accuracy: 0.9718 - val_loss: 0.1156 - val_accuracy: 0.9538\n",
            "Epoch 540/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0745 - accuracy: 0.9740\n",
            "Epoch 00540: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0766 - accuracy: 0.9718 - val_loss: 0.1170 - val_accuracy: 0.9641\n",
            "Epoch 541/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0826 - accuracy: 0.9680\n",
            "Epoch 00541: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0768 - accuracy: 0.9679 - val_loss: 0.1166 - val_accuracy: 0.9590\n",
            "Epoch 542/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0666 - accuracy: 0.9760\n",
            "Epoch 00542: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0763 - accuracy: 0.9705 - val_loss: 0.1148 - val_accuracy: 0.9538\n",
            "Epoch 543/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0701 - accuracy: 0.9720\n",
            "Epoch 00543: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0765 - accuracy: 0.9718 - val_loss: 0.1147 - val_accuracy: 0.9538\n",
            "Epoch 544/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0798 - accuracy: 0.9680\n",
            "Epoch 00544: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 54ms/step - loss: 0.0763 - accuracy: 0.9718 - val_loss: 0.1165 - val_accuracy: 0.9590\n",
            "Epoch 545/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0741 - accuracy: 0.9660\n",
            "Epoch 00545: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0760 - accuracy: 0.9692 - val_loss: 0.1196 - val_accuracy: 0.9641\n",
            "Epoch 546/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0643 - accuracy: 0.9760\n",
            "Epoch 00546: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0772 - accuracy: 0.9692 - val_loss: 0.1159 - val_accuracy: 0.9538\n",
            "Epoch 547/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0709 - accuracy: 0.9760\n",
            "Epoch 00547: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0773 - accuracy: 0.9705 - val_loss: 0.1161 - val_accuracy: 0.9590\n",
            "Epoch 548/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0760 - accuracy: 0.9720\n",
            "Epoch 00548: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0778 - accuracy: 0.9718 - val_loss: 0.1158 - val_accuracy: 0.9590\n",
            "Epoch 549/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0834 - accuracy: 0.9640\n",
            "Epoch 00549: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0753 - accuracy: 0.9705 - val_loss: 0.1172 - val_accuracy: 0.9692\n",
            "Epoch 550/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0725 - accuracy: 0.9700\n",
            "Epoch 00550: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0758 - accuracy: 0.9705 - val_loss: 0.1169 - val_accuracy: 0.9641\n",
            "Epoch 551/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0773 - accuracy: 0.9760\n",
            "Epoch 00551: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0754 - accuracy: 0.9705 - val_loss: 0.1151 - val_accuracy: 0.9538\n",
            "Epoch 552/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0907 - accuracy: 0.9620\n",
            "Epoch 00552: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0756 - accuracy: 0.9718 - val_loss: 0.1152 - val_accuracy: 0.9538\n",
            "Epoch 553/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0789 - accuracy: 0.9760\n",
            "Epoch 00553: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0745 - accuracy: 0.9731 - val_loss: 0.1182 - val_accuracy: 0.9641\n",
            "Epoch 554/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0748 - accuracy: 0.9720\n",
            "Epoch 00554: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0755 - accuracy: 0.9692 - val_loss: 0.1178 - val_accuracy: 0.9590\n",
            "Epoch 555/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0696 - accuracy: 0.9760\n",
            "Epoch 00555: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0747 - accuracy: 0.9705 - val_loss: 0.1156 - val_accuracy: 0.9590\n",
            "Epoch 556/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0831 - accuracy: 0.9660\n",
            "Epoch 00556: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 56ms/step - loss: 0.0759 - accuracy: 0.9718 - val_loss: 0.1154 - val_accuracy: 0.9538\n",
            "Epoch 557/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0772 - accuracy: 0.9700\n",
            "Epoch 00557: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0751 - accuracy: 0.9718 - val_loss: 0.1191 - val_accuracy: 0.9641\n",
            "Epoch 558/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0834 - accuracy: 0.9640\n",
            "Epoch 00558: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0750 - accuracy: 0.9692 - val_loss: 0.1172 - val_accuracy: 0.9538\n",
            "Epoch 559/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0582 - accuracy: 0.9800\n",
            "Epoch 00559: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0740 - accuracy: 0.9718 - val_loss: 0.1153 - val_accuracy: 0.9538\n",
            "Epoch 560/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0742 - accuracy: 0.9720\n",
            "Epoch 00560: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0748 - accuracy: 0.9705 - val_loss: 0.1152 - val_accuracy: 0.9538\n",
            "Epoch 561/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0837 - accuracy: 0.9640\n",
            "Epoch 00561: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0736 - accuracy: 0.9718 - val_loss: 0.1181 - val_accuracy: 0.9590\n",
            "Epoch 562/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0775 - accuracy: 0.9700\n",
            "Epoch 00562: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0751 - accuracy: 0.9718 - val_loss: 0.1197 - val_accuracy: 0.9641\n",
            "Epoch 563/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0932 - accuracy: 0.9540\n",
            "Epoch 00563: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0765 - accuracy: 0.9667 - val_loss: 0.1151 - val_accuracy: 0.9538\n",
            "Epoch 564/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0713 - accuracy: 0.9760\n",
            "Epoch 00564: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0734 - accuracy: 0.9718 - val_loss: 0.1157 - val_accuracy: 0.9538\n",
            "Epoch 565/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0831 - accuracy: 0.9680\n",
            "Epoch 00565: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0731 - accuracy: 0.9731 - val_loss: 0.1172 - val_accuracy: 0.9538\n",
            "Epoch 566/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0748 - accuracy: 0.9720\n",
            "Epoch 00566: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0731 - accuracy: 0.9731 - val_loss: 0.1163 - val_accuracy: 0.9538\n",
            "Epoch 567/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0690 - accuracy: 0.9740\n",
            "Epoch 00567: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0727 - accuracy: 0.9731 - val_loss: 0.1155 - val_accuracy: 0.9538\n",
            "Epoch 568/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0533 - accuracy: 0.9800\n",
            "Epoch 00568: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0729 - accuracy: 0.9731 - val_loss: 0.1155 - val_accuracy: 0.9538\n",
            "Epoch 569/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0723 - accuracy: 0.9640\n",
            "Epoch 00569: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0731 - accuracy: 0.9718 - val_loss: 0.1161 - val_accuracy: 0.9538\n",
            "Epoch 570/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0651 - accuracy: 0.9740\n",
            "Epoch 00570: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.0742 - accuracy: 0.9705 - val_loss: 0.1184 - val_accuracy: 0.9590\n",
            "Epoch 571/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0776 - accuracy: 0.9700\n",
            "Epoch 00571: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0727 - accuracy: 0.9731 - val_loss: 0.1155 - val_accuracy: 0.9538\n",
            "Epoch 572/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0720 - accuracy: 0.9720\n",
            "Epoch 00572: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.1156 - val_accuracy: 0.9538\n",
            "Epoch 573/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0644 - accuracy: 0.9760\n",
            "Epoch 00573: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0725 - accuracy: 0.9731 - val_loss: 0.1178 - val_accuracy: 0.9538\n",
            "Epoch 574/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0743 - accuracy: 0.9700\n",
            "Epoch 00574: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0721 - accuracy: 0.9731 - val_loss: 0.1168 - val_accuracy: 0.9538\n",
            "Epoch 575/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0703 - accuracy: 0.9740\n",
            "Epoch 00575: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0715 - accuracy: 0.9731 - val_loss: 0.1161 - val_accuracy: 0.9538\n",
            "Epoch 576/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0724 - accuracy: 0.9760\n",
            "Epoch 00576: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0716 - accuracy: 0.9731 - val_loss: 0.1158 - val_accuracy: 0.9538\n",
            "Epoch 577/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0847 - accuracy: 0.9680\n",
            "Epoch 00577: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0712 - accuracy: 0.9731 - val_loss: 0.1171 - val_accuracy: 0.9538\n",
            "Epoch 578/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0769 - accuracy: 0.9720\n",
            "Epoch 00578: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0715 - accuracy: 0.9731 - val_loss: 0.1183 - val_accuracy: 0.9590\n",
            "Epoch 579/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0702 - accuracy: 0.9700\n",
            "Epoch 00579: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 62ms/step - loss: 0.0718 - accuracy: 0.9718 - val_loss: 0.1179 - val_accuracy: 0.9590\n",
            "Epoch 580/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0882 - accuracy: 0.9700\n",
            "Epoch 00580: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.0728 - accuracy: 0.9731 - val_loss: 0.1156 - val_accuracy: 0.9538\n",
            "Epoch 581/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0628 - accuracy: 0.9740\n",
            "Epoch 00581: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0716 - accuracy: 0.9731 - val_loss: 0.1162 - val_accuracy: 0.9538\n",
            "Epoch 582/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0605 - accuracy: 0.9780\n",
            "Epoch 00582: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0706 - accuracy: 0.9731 - val_loss: 0.1160 - val_accuracy: 0.9538\n",
            "Epoch 583/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0692 - accuracy: 0.9700\n",
            "Epoch 00583: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0706 - accuracy: 0.9731 - val_loss: 0.1161 - val_accuracy: 0.9538\n",
            "Epoch 584/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0562 - accuracy: 0.9760\n",
            "Epoch 00584: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0711 - accuracy: 0.9731 - val_loss: 0.1164 - val_accuracy: 0.9538\n",
            "Epoch 585/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0541 - accuracy: 0.9800\n",
            "Epoch 00585: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0704 - accuracy: 0.9731 - val_loss: 0.1162 - val_accuracy: 0.9538\n",
            "Epoch 586/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0786 - accuracy: 0.9680\n",
            "Epoch 00586: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0707 - accuracy: 0.9718 - val_loss: 0.1161 - val_accuracy: 0.9538\n",
            "Epoch 587/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0756 - accuracy: 0.9700\n",
            "Epoch 00587: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0699 - accuracy: 0.9731 - val_loss: 0.1183 - val_accuracy: 0.9538\n",
            "Epoch 588/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0723 - accuracy: 0.9720\n",
            "Epoch 00588: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0706 - accuracy: 0.9731 - val_loss: 0.1182 - val_accuracy: 0.9538\n",
            "Epoch 589/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0783 - accuracy: 0.9700\n",
            "Epoch 00589: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0708 - accuracy: 0.9744 - val_loss: 0.1163 - val_accuracy: 0.9538\n",
            "Epoch 590/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0787 - accuracy: 0.9680\n",
            "Epoch 00590: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0700 - accuracy: 0.9744 - val_loss: 0.1165 - val_accuracy: 0.9538\n",
            "Epoch 591/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0580 - accuracy: 0.9760\n",
            "Epoch 00591: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0694 - accuracy: 0.9731 - val_loss: 0.1169 - val_accuracy: 0.9538\n",
            "Epoch 592/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0697 - accuracy: 0.9760\n",
            "Epoch 00592: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0692 - accuracy: 0.9731 - val_loss: 0.1168 - val_accuracy: 0.9538\n",
            "Epoch 593/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0647 - accuracy: 0.9740\n",
            "Epoch 00593: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0695 - accuracy: 0.9731 - val_loss: 0.1168 - val_accuracy: 0.9538\n",
            "Epoch 594/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0714 - accuracy: 0.9740\n",
            "Epoch 00594: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.0692 - accuracy: 0.9731 - val_loss: 0.1166 - val_accuracy: 0.9538\n",
            "Epoch 595/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0644 - accuracy: 0.9720\n",
            "Epoch 00595: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0690 - accuracy: 0.9731 - val_loss: 0.1172 - val_accuracy: 0.9538\n",
            "Epoch 596/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0637 - accuracy: 0.9740\n",
            "Epoch 00596: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0689 - accuracy: 0.9731 - val_loss: 0.1181 - val_accuracy: 0.9590\n",
            "Epoch 597/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0714 - accuracy: 0.9720\n",
            "Epoch 00597: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0690 - accuracy: 0.9744 - val_loss: 0.1179 - val_accuracy: 0.9590\n",
            "Epoch 598/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0577 - accuracy: 0.9820\n",
            "Epoch 00598: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0687 - accuracy: 0.9744 - val_loss: 0.1164 - val_accuracy: 0.9538\n",
            "Epoch 599/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0641 - accuracy: 0.9800\n",
            "Epoch 00599: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 71ms/step - loss: 0.0689 - accuracy: 0.9744 - val_loss: 0.1167 - val_accuracy: 0.9538\n",
            "Epoch 600/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0661 - accuracy: 0.9760\n",
            "Epoch 00600: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0686 - accuracy: 0.9744 - val_loss: 0.1177 - val_accuracy: 0.9590\n",
            "Epoch 601/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0717 - accuracy: 0.9720\n",
            "Epoch 00601: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0686 - accuracy: 0.9744 - val_loss: 0.1183 - val_accuracy: 0.9590\n",
            "Epoch 602/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0895 - accuracy: 0.9620\n",
            "Epoch 00602: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0689 - accuracy: 0.9744 - val_loss: 0.1168 - val_accuracy: 0.9538\n",
            "Epoch 603/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0741 - accuracy: 0.9740\n",
            "Epoch 00603: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0682 - accuracy: 0.9744 - val_loss: 0.1173 - val_accuracy: 0.9538\n",
            "Epoch 604/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0651 - accuracy: 0.9780\n",
            "Epoch 00604: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.0679 - accuracy: 0.9731 - val_loss: 0.1189 - val_accuracy: 0.9590\n",
            "Epoch 605/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0748 - accuracy: 0.9680\n",
            "Epoch 00605: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0685 - accuracy: 0.9731 - val_loss: 0.1174 - val_accuracy: 0.9538\n",
            "Epoch 606/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0761 - accuracy: 0.9680\n",
            "Epoch 00606: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0678 - accuracy: 0.9731 - val_loss: 0.1172 - val_accuracy: 0.9538\n",
            "Epoch 607/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0699 - accuracy: 0.9740\n",
            "Epoch 00607: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0673 - accuracy: 0.9731 - val_loss: 0.1179 - val_accuracy: 0.9590\n",
            "Epoch 608/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0768 - accuracy: 0.9680\n",
            "Epoch 00608: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0677 - accuracy: 0.9744 - val_loss: 0.1173 - val_accuracy: 0.9538\n",
            "Epoch 609/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0757 - accuracy: 0.9720\n",
            "Epoch 00609: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.0672 - accuracy: 0.9744 - val_loss: 0.1173 - val_accuracy: 0.9538\n",
            "Epoch 610/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0673 - accuracy: 0.9780\n",
            "Epoch 00610: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0671 - accuracy: 0.9744 - val_loss: 0.1171 - val_accuracy: 0.9538\n",
            "Epoch 611/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0721 - accuracy: 0.9700\n",
            "Epoch 00611: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0668 - accuracy: 0.9731 - val_loss: 0.1168 - val_accuracy: 0.9538\n",
            "Epoch 612/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0592 - accuracy: 0.9780\n",
            "Epoch 00612: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0671 - accuracy: 0.9744 - val_loss: 0.1170 - val_accuracy: 0.9538\n",
            "Epoch 613/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0685 - accuracy: 0.9720\n",
            "Epoch 00613: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0668 - accuracy: 0.9744 - val_loss: 0.1171 - val_accuracy: 0.9538\n",
            "Epoch 614/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0653 - accuracy: 0.9720\n",
            "Epoch 00614: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0666 - accuracy: 0.9744 - val_loss: 0.1175 - val_accuracy: 0.9538\n",
            "Epoch 615/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0665 - accuracy: 0.9740\n",
            "Epoch 00615: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0665 - accuracy: 0.9744 - val_loss: 0.1180 - val_accuracy: 0.9538\n",
            "Epoch 616/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0695 - accuracy: 0.9720\n",
            "Epoch 00616: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0663 - accuracy: 0.9744 - val_loss: 0.1170 - val_accuracy: 0.9538\n",
            "Epoch 617/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0587 - accuracy: 0.9740\n",
            "Epoch 00617: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.0662 - accuracy: 0.9744 - val_loss: 0.1169 - val_accuracy: 0.9538\n",
            "Epoch 618/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0495 - accuracy: 0.9840\n",
            "Epoch 00618: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 0.0663 - accuracy: 0.9744 - val_loss: 0.1171 - val_accuracy: 0.9538\n",
            "Epoch 619/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0691 - accuracy: 0.9700\n",
            "Epoch 00619: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 48ms/step - loss: 0.0660 - accuracy: 0.9744 - val_loss: 0.1172 - val_accuracy: 0.9538\n",
            "Epoch 620/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0601 - accuracy: 0.9740\n",
            "Epoch 00620: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 0.0659 - accuracy: 0.9756 - val_loss: 0.1170 - val_accuracy: 0.9538\n",
            "Epoch 621/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0789 - accuracy: 0.9680\n",
            "Epoch 00621: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0664 - accuracy: 0.9756 - val_loss: 0.1178 - val_accuracy: 0.9590\n",
            "Epoch 622/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0802 - accuracy: 0.9680\n",
            "Epoch 00622: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0655 - accuracy: 0.9744 - val_loss: 0.1206 - val_accuracy: 0.9590\n",
            "Epoch 623/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0679 - accuracy: 0.9760\n",
            "Epoch 00623: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0666 - accuracy: 0.9744 - val_loss: 0.1182 - val_accuracy: 0.9590\n",
            "Epoch 624/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0688 - accuracy: 0.9780\n",
            "Epoch 00624: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0661 - accuracy: 0.9744 - val_loss: 0.1173 - val_accuracy: 0.9538\n",
            "Epoch 625/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0663 - accuracy: 0.9740\n",
            "Epoch 00625: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0659 - accuracy: 0.9718 - val_loss: 0.1179 - val_accuracy: 0.9590\n",
            "Epoch 626/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0670 - accuracy: 0.9760\n",
            "Epoch 00626: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.0670 - accuracy: 0.9744 - val_loss: 0.1199 - val_accuracy: 0.9590\n",
            "Epoch 627/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0679 - accuracy: 0.9720\n",
            "Epoch 00627: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0656 - accuracy: 0.9756 - val_loss: 0.1176 - val_accuracy: 0.9487\n",
            "Epoch 628/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0671 - accuracy: 0.9720\n",
            "Epoch 00628: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.0668 - accuracy: 0.9731 - val_loss: 0.1173 - val_accuracy: 0.9538\n",
            "Epoch 629/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0627 - accuracy: 0.9740\n",
            "Epoch 00629: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.0657 - accuracy: 0.9718 - val_loss: 0.1215 - val_accuracy: 0.9590\n",
            "Epoch 630/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0434 - accuracy: 0.9900\n",
            "Epoch 00630: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.0674 - accuracy: 0.9769 - val_loss: 0.1179 - val_accuracy: 0.9590\n",
            "Epoch 631/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0689 - accuracy: 0.9720\n",
            "Epoch 00631: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.0652 - accuracy: 0.9744 - val_loss: 0.1232 - val_accuracy: 0.9436\n",
            "Epoch 632/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0751 - accuracy: 0.9740\n",
            "Epoch 00632: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.0677 - accuracy: 0.9756 - val_loss: 0.1189 - val_accuracy: 0.9590\n",
            "Epoch 633/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0587 - accuracy: 0.9720\n",
            "Epoch 00633: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 0.0665 - accuracy: 0.9769 - val_loss: 0.1273 - val_accuracy: 0.9692\n",
            "Epoch 634/3500\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.0524 - accuracy: 0.9800\n",
            "Epoch 00634: val_loss did not improve from 0.11449\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.0682 - accuracy: 0.9769 - val_loss: 0.1177 - val_accuracy: 0.9538\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f61521be990>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "o9cbkw_ZANgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15장 선형회귀 적용하기 "
      ],
      "metadata": {
        "id": "wEOgIsejAV-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"dataset/housing.csv\", delim_whitespace=True, header=None)  # 공백을 delimiter로 사용함\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhiSkzPmunMx",
        "outputId": "ee0a6a34-192c-4311-88b4-9fbe6dcb4810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 506 entries, 0 to 505\n",
            "Data columns (total 14 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       506 non-null    float64\n",
            " 1   1       506 non-null    float64\n",
            " 2   2       506 non-null    float64\n",
            " 3   3       506 non-null    int64  \n",
            " 4   4       506 non-null    float64\n",
            " 5   5       506 non-null    float64\n",
            " 6   6       506 non-null    float64\n",
            " 7   7       506 non-null    float64\n",
            " 8   8       506 non-null    int64  \n",
            " 9   9       506 non-null    float64\n",
            " 10  10      506 non-null    float64\n",
            " 11  11      506 non-null    float64\n",
            " 12  12      506 non-null    float64\n",
            " 13  13      506 non-null    float64\n",
            "dtypes: float64(12), int64(2)\n",
            "memory usage: 55.5 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dCZXIa6vwlY3",
        "outputId": "6767d143-1e3d-461c-ee3c-032e36bef52a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-90a5e032-e934-4bed-a222-03b3d4d6ebc7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90a5e032-e934-4bed-a222-03b3d4d6ebc7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-90a5e032-e934-4bed-a222-03b3d4d6ebc7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-90a5e032-e934-4bed-a222-03b3d4d6ebc7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        0     1     2   3      4      5   ...  8      9     10      11    12    13\n",
              "0  0.00632  18.0  2.31   0  0.538  6.575  ...   1  296.0  15.3  396.90  4.98  24.0\n",
              "1  0.02731   0.0  7.07   0  0.469  6.421  ...   2  242.0  17.8  396.90  9.14  21.6\n",
              "2  0.02729   0.0  7.07   0  0.469  7.185  ...   2  242.0  17.8  392.83  4.03  34.7\n",
              "3  0.03237   0.0  2.18   0  0.458  6.998  ...   3  222.0  18.7  394.63  2.94  33.4\n",
              "4  0.06905   0.0  2.18   0  0.458  7.147  ...   3  222.0  18.7  396.90  5.33  36.2\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers  import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "1tubaEJjxfh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = df.values\n",
        "X=dataset[:,0:13]\n",
        "Y=dataset[:,13]\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3 ,  random_state=3)\n"
      ],
      "metadata": {
        "id": "QPHxUthJxLlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 선형회귀모델 생성\n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_dim=13, activation='relu'))\n",
        "model.add(Dense(6, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(X_train, Y_train, epochs=200, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-19s9JWw3hA",
        "outputId": "0d3a99f4-e038-4cbd-dd63-e56d1048301e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "36/36 [==============================] - 1s 2ms/step - loss: 777.5534\n",
            "Epoch 2/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 192.9821\n",
            "Epoch 3/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 115.6900\n",
            "Epoch 4/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 98.7644\n",
            "Epoch 5/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 84.7658\n",
            "Epoch 6/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 73.2857\n",
            "Epoch 7/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 68.1778\n",
            "Epoch 8/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 69.7440\n",
            "Epoch 9/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 57.7037\n",
            "Epoch 10/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 55.5743\n",
            "Epoch 11/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 53.2379\n",
            "Epoch 12/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 50.5965\n",
            "Epoch 13/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 50.3975\n",
            "Epoch 14/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 47.7964\n",
            "Epoch 15/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 45.3481\n",
            "Epoch 16/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 45.7798\n",
            "Epoch 17/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 49.3722\n",
            "Epoch 18/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 45.3236\n",
            "Epoch 19/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 47.0377\n",
            "Epoch 20/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 45.2941\n",
            "Epoch 21/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 41.6138\n",
            "Epoch 22/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 39.3895\n",
            "Epoch 23/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 38.8615\n",
            "Epoch 24/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 42.0159\n",
            "Epoch 25/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 39.0255\n",
            "Epoch 26/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 39.2212\n",
            "Epoch 27/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 42.6666\n",
            "Epoch 28/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 41.2345\n",
            "Epoch 29/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 37.2274\n",
            "Epoch 30/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 40.2713\n",
            "Epoch 31/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 35.8698\n",
            "Epoch 32/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 37.8791\n",
            "Epoch 33/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 38.9771\n",
            "Epoch 34/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 35.1634\n",
            "Epoch 35/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 37.8832\n",
            "Epoch 36/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 36.2770\n",
            "Epoch 37/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 35.4499\n",
            "Epoch 38/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 44.3532\n",
            "Epoch 39/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 36.5265\n",
            "Epoch 40/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 39.9133\n",
            "Epoch 41/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 40.6527\n",
            "Epoch 42/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 36.0335\n",
            "Epoch 43/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 35.7402\n",
            "Epoch 44/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.0184\n",
            "Epoch 45/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 37.1122\n",
            "Epoch 46/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 34.6586\n",
            "Epoch 47/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 36.6194\n",
            "Epoch 48/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 37.0700\n",
            "Epoch 49/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.9223\n",
            "Epoch 50/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 36.7447\n",
            "Epoch 51/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 41.3574\n",
            "Epoch 52/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.6396\n",
            "Epoch 53/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 35.6372\n",
            "Epoch 54/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 32.7543\n",
            "Epoch 55/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 32.4514\n",
            "Epoch 56/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 37.8477\n",
            "Epoch 57/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 34.9767\n",
            "Epoch 58/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.9677\n",
            "Epoch 59/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.1466\n",
            "Epoch 60/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 36.0972\n",
            "Epoch 61/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.4488\n",
            "Epoch 62/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 36.6729\n",
            "Epoch 63/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.4787\n",
            "Epoch 64/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 34.8750\n",
            "Epoch 65/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 32.4154\n",
            "Epoch 66/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.8190\n",
            "Epoch 67/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 38.6020\n",
            "Epoch 68/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.7550\n",
            "Epoch 69/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 34.2876\n",
            "Epoch 70/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.6246\n",
            "Epoch 71/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.1905\n",
            "Epoch 72/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.8690\n",
            "Epoch 73/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.5227\n",
            "Epoch 74/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 35.2946\n",
            "Epoch 75/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 39.5839\n",
            "Epoch 76/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.2986\n",
            "Epoch 77/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.6867\n",
            "Epoch 78/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.0433\n",
            "Epoch 79/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.8179\n",
            "Epoch 80/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 39.4063\n",
            "Epoch 81/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 32.4550\n",
            "Epoch 82/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.3854\n",
            "Epoch 83/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.5729\n",
            "Epoch 84/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.9300\n",
            "Epoch 85/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 36.0982\n",
            "Epoch 86/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 32.0461\n",
            "Epoch 87/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.6775\n",
            "Epoch 88/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.2159\n",
            "Epoch 89/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 45.2089\n",
            "Epoch 90/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.5272\n",
            "Epoch 91/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.7882\n",
            "Epoch 92/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.4659\n",
            "Epoch 93/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 32.2677\n",
            "Epoch 94/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.5546\n",
            "Epoch 95/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.9255\n",
            "Epoch 96/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 32.8074\n",
            "Epoch 97/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.1044\n",
            "Epoch 98/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.5522\n",
            "Epoch 99/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 35.0543\n",
            "Epoch 100/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 36.4504\n",
            "Epoch 101/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 42.3188\n",
            "Epoch 102/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.4303\n",
            "Epoch 103/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.9643\n",
            "Epoch 104/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.7326\n",
            "Epoch 105/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.1274\n",
            "Epoch 106/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 34.6038\n",
            "Epoch 107/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.4861\n",
            "Epoch 108/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.9892\n",
            "Epoch 109/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.3437\n",
            "Epoch 110/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.5272\n",
            "Epoch 111/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.9376\n",
            "Epoch 112/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.8982\n",
            "Epoch 113/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 35.1351\n",
            "Epoch 114/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.6893\n",
            "Epoch 115/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.3265\n",
            "Epoch 116/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.6057\n",
            "Epoch 117/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.9598\n",
            "Epoch 118/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.4881\n",
            "Epoch 119/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.1696\n",
            "Epoch 120/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.8907\n",
            "Epoch 121/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.8625\n",
            "Epoch 122/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.8408\n",
            "Epoch 123/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 35.0741\n",
            "Epoch 124/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 32.7909\n",
            "Epoch 125/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.2708\n",
            "Epoch 126/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.2564\n",
            "Epoch 127/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.3251\n",
            "Epoch 128/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.6284\n",
            "Epoch 129/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.1023\n",
            "Epoch 130/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.1641\n",
            "Epoch 131/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.9110\n",
            "Epoch 132/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.5460\n",
            "Epoch 133/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.8633\n",
            "Epoch 134/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.2082\n",
            "Epoch 135/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.1048\n",
            "Epoch 136/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.6432\n",
            "Epoch 137/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.7079\n",
            "Epoch 138/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.6182\n",
            "Epoch 139/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.8903\n",
            "Epoch 140/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.2315\n",
            "Epoch 141/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.1577\n",
            "Epoch 142/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.5775\n",
            "Epoch 143/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.4550\n",
            "Epoch 144/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.8329\n",
            "Epoch 145/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.6064\n",
            "Epoch 146/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.4101\n",
            "Epoch 147/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 32.8893\n",
            "Epoch 148/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.9105\n",
            "Epoch 149/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.7783\n",
            "Epoch 150/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.9242\n",
            "Epoch 151/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.8257\n",
            "Epoch 152/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.6457\n",
            "Epoch 153/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.4409\n",
            "Epoch 154/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.4602\n",
            "Epoch 155/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.7832\n",
            "Epoch 156/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.8109\n",
            "Epoch 157/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.8516\n",
            "Epoch 158/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.0234\n",
            "Epoch 159/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.4636\n",
            "Epoch 160/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.0097\n",
            "Epoch 161/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.2368\n",
            "Epoch 162/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.8666\n",
            "Epoch 163/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.4033\n",
            "Epoch 164/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.4282\n",
            "Epoch 165/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.7263\n",
            "Epoch 166/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.9539\n",
            "Epoch 167/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.3927\n",
            "Epoch 168/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.2052\n",
            "Epoch 169/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.6260\n",
            "Epoch 170/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.5877\n",
            "Epoch 171/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.1892\n",
            "Epoch 172/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.8360\n",
            "Epoch 173/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.9904\n",
            "Epoch 174/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 24.7846\n",
            "Epoch 175/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.8692\n",
            "Epoch 176/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.8871\n",
            "Epoch 177/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.9944\n",
            "Epoch 178/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.6419\n",
            "Epoch 179/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.7487\n",
            "Epoch 180/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.1466\n",
            "Epoch 181/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.2300\n",
            "Epoch 182/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.2467\n",
            "Epoch 183/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.7393\n",
            "Epoch 184/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.9314\n",
            "Epoch 185/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 27.3940\n",
            "Epoch 186/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.0361\n",
            "Epoch 187/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 29.5634\n",
            "Epoch 188/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.5169\n",
            "Epoch 189/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.8898\n",
            "Epoch 190/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.8991\n",
            "Epoch 191/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.9551\n",
            "Epoch 192/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.0204\n",
            "Epoch 193/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.3904\n",
            "Epoch 194/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.4786\n",
            "Epoch 195/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.0852\n",
            "Epoch 196/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.7197\n",
            "Epoch 197/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 24.4081\n",
            "Epoch 198/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.9558\n",
            "Epoch 199/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 24.1403\n",
            "Epoch 200/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.3022\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f614936ffd0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_prediction = model.predict(X_test).flatten()\n",
        "for i in range(10):\n",
        "  label = Y_test[i]\n",
        "  prediction = Y_prediction[i]\n",
        "  print(\"실제가격: {:.3f}, 예상가격: {:.3f}\".format(label, prediction))\n",
        "  # 실제가격과 예상가격이 비례해서 변화하는것을 확인 가능"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiK3uZdFxDVV",
        "outputId": "48f4b682-4799-4e01-a593-22fd8a4f7749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "실제가격: 44.800, 예상가격: 31.549\n",
            "실제가격: 17.100, 예상가격: 18.202\n",
            "실제가격: 17.800, 예상가격: 20.822\n",
            "실제가격: 33.100, 예상가격: 26.798\n",
            "실제가격: 21.900, 예상가격: 20.434\n",
            "실제가격: 21.000, 예상가격: 21.442\n",
            "실제가격: 18.400, 예상가격: 18.804\n",
            "실제가격: 10.400, 예상가격: 7.641\n",
            "실제가격: 23.100, 예상가격: 21.782\n",
            "실제가격: 20.000, 예상가격: 15.705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PBb4eQyLy5ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16장 이미지 인식의 꽃 CNN익히기 "
      ],
      "metadata": {
        "id": "tsUTHMrYy67C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist"
      ],
      "metadata": {
        "id": "1mhS9EOBxtSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, Y_class_train), (X_test, Y_class_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "QYaB0EPRzQU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"학습셋 이미지수 : %d 개\"% (X_train.shape[0]))\n",
        "print(\"test셋 이미지수 : %d 개\"% (X_test.shape[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGCBUgC4zi6V",
        "outputId": "e540037c-acf3-45c1-ba98-d6128852d83e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습셋 이미지수 : 60000 개\n",
            "test셋 이미지수 : 10000 개\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 한개만 불러와보기\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(X_train[0], cmap='Greys')\n",
        "plt.show()  # 784=28x28 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "DR_9rZ9gxIfa",
        "outputId": "63f41750-d840-48d5-9e4e-34c34e7d4cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "for x in X_train[0]: # X_train의 한줄씩\n",
        "  for i in x:  # X_train의 한칸씩\n",
        "    sys.stdout.write('%d\\t' % i)\n",
        "  sys.stdout.write('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8qX9TaXwlqf",
        "outputId": "d078ccdb-387c-41e7-8a6d-2190962d89ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t3\t18\t18\t18\t126\t136\t175\t26\t166\t255\t247\t127\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t30\t36\t94\t154\t170\t253\t253\t253\t253\t253\t225\t172\t253\t242\t195\t64\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t49\t238\t253\t253\t253\t253\t253\t253\t253\t253\t251\t93\t82\t82\t56\t39\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t18\t219\t253\t253\t253\t253\t253\t198\t182\t247\t241\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t80\t156\t107\t253\t253\t205\t11\t0\t43\t154\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t14\t1\t154\t253\t90\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t139\t253\t190\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t11\t190\t253\t70\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t35\t241\t225\t160\t108\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t81\t240\t253\t253\t119\t25\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t45\t186\t253\t253\t150\t27\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t16\t93\t252\t253\t187\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t249\t253\t249\t64\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t46\t130\t183\t253\t253\t207\t2\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t39\t148\t229\t253\t253\t253\t250\t182\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t24\t114\t221\t253\t253\t253\t253\t201\t78\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t23\t66\t213\t253\t253\t253\t253\t198\t81\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t18\t171\t219\t253\t253\t253\t253\t195\t80\t9\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t55\t172\t226\t253\t253\t253\t253\t244\t133\t11\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t136\t253\t253\t253\t212\t135\t132\t16\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_train) # numpy.ndarray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM7aUZ720qg7",
        "outputId": "85c1aa15-b149-4d8f-8ea3-c6b3b39f329a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 784)"
      ],
      "metadata": {
        "id": "oQGWZiwb0q69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape  # (60000, 784) 60000만개의 이미지를 각각 784차원으로 flatten시킨셈"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1sTWUOT173c",
        "outputId": "6714e4ad-9586-4950-8e77-82e1e3b89bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.astype('float64')\n",
        "X_train = X_train/255"
      ],
      "metadata": {
        "id": "0G8TXQgk2LbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X_test.reshape(X_test.shape[0], 784).astype('float64')/255"
      ],
      "metadata": {
        "id": "bHN5irZR2SZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"class: %d\" % (Y_class_train[0]))  # 클래스 5, 숫자 5를 의미함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfiT0X0b2iVF",
        "outputId": "79ea0d62-3edf-485f-8901-9b5ffcf03fc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install np_utils\n",
        "from keras.utils.np_utils import to_categorical\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GDHqvKd2y_u",
        "outputId": "45e3bb50-287b-4b92-a9e8-45202e2a33de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting np_utils\n",
            "  Downloading np_utils-0.6.0.tar.gz (61 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 43.3 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 20 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 30 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 40 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 654 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from np_utils) (1.19.5)\n",
            "Building wheels for collected packages: np-utils\n",
            "  Building wheel for np-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for np-utils: filename=np_utils-0.6.0-py3-none-any.whl size=56459 sha256=f75106368fed78eaea7b3507dff4b138c06d002c98f4db285c91364d01f97fcf\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/83/71/a781667865955ae7dc18e5a4038401deb56d96eb85d3a5f1c0\n",
            "Successfully built np-utils\n",
            "Installing collected packages: np-utils\n",
            "Successfully installed np-utils-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train = to_categorical(Y_class_train, 10)\n",
        "Y_test = to_categorical(Y_class_test, 10)"
      ],
      "metadata": {
        "id": "wGfSSdxu7gXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLkR-CtS2tqD",
        "outputId": "e49d65d2-48be-41ac-b538-19dd804f8923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 딥러닝 기본프레임 만들기\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_dim=784, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax')) # 범주가 3이상이기때문에 softmax로 진행\n"
      ],
      "metadata": {
        "id": "som-CnGu2ibn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "4tjd3R253qX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "MODEL_DIR = './model'\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "  os.mkdir(MODEL_DIR)\n"
      ],
      "metadata": {
        "id": "v0XL5hcd33iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelpath = './model/{epoch:02d}-{val_loss:.4f}.hdf5'\n",
        "checkpointer = ModelCheckpoint(filepath = modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)"
      ],
      "metadata": {
        "id": "biPzX2aU4moq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30, batch_size=200, verbose=0,\n",
        "                    callbacks=[early_stopping_callback, checkpointer])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q45kQrkn49AM",
        "outputId": "af2c7594-4462-4627-daf7-f08e3cf147bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.15213, saving model to ./model/01-0.1521.hdf5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.15213 to 0.10464, saving model to ./model/02-0.1046.hdf5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.10464 to 0.09039, saving model to ./model/03-0.0904.hdf5\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.09039 to 0.07768, saving model to ./model/04-0.0777.hdf5\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.07768 to 0.07094, saving model to ./model/05-0.0709.hdf5\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.07094 to 0.06733, saving model to ./model/06-0.0673.hdf5\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.06733 to 0.06663, saving model to ./model/07-0.0666.hdf5\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.06663 to 0.06328, saving model to ./model/08-0.0633.hdf5\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.06328\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.06328 to 0.06032, saving model to ./model/10-0.0603.hdf5\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.06032\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.06032\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.06032\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.06032\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.06032\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.06032\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.06032\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.06032\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.06032\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.06032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"test accurcy: %.4f\" % (model.evaluate(X_test, Y_test)[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-tBKsdX5KF2",
        "outputId": "dbd95a02-dd23-45c1-b2e1-471404834618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0872 - accuracy: 0.9800\n",
            "test accurcy: 0.9800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트셋의 오차\n",
        "y_vloss = history.history['val_loss']\n",
        "y_loss = history.history['loss']\n"
      ],
      "metadata": {
        "id": "3cCEHeld5nZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래프로 표현\n",
        "x_len = numpy.arange(len(y_loss))\n",
        "plt.plot(x_len, y_vloss, marker='.', c='red', label ='testset_loss')\n",
        "plt.plot(x_len, y_loss, marker='.', c='blue', label = 'trainset_loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show() # 테스트셋의 과적합이 일어나기전 학습을 끝낸모습임"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "_sqe1n-q8K38",
        "outputId": "28a8546d-409f-4934-93c2-4f5a0cd14dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 더 깊은 딥러닝\n",
        "# CNN : 케라스에서 컨볼루션 층을 추가하는 함수는 CONV2D()\n",
        "\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "import os"
      ],
      "metadata": {
        "id": "kXXV1RvR8CW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed=0\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(3)\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 28,28,1).astype('float32') / 255\n",
        "X_test = X_test.reshape(X_test.shape[0], 28,28,1).astype('float32') / 255\n",
        "\n",
        "Y_train = to_categorical(Y_train)\n",
        "Y_test = to_categorical(Y_test)\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3,3), input_shape=(28,28,1), activation='relu'))\n",
        "model.add(Conv2D(64, (3,3), input_shape=(28,28,1), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "# kernel의 개수:32 서로 다른 컨볼루션이 여러개 나옴\n",
        "# (3,3) kernel의 크기\n",
        "# input_shape\n",
        "# 풀림/샘플링 : 맥스풀링 / 평균풀링 등이 있음\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7Fi2kzZ9YsQ",
        "outputId": "9122ef44-5e01-4fe9-8c17-3cc49a7e629e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "E_dVZkSyBME0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_DIR = './modelCNN'\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "  os.mkdir(MODEL_DIR)\n"
      ],
      "metadata": {
        "id": "tZEyd0HqBTV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelpath = './modelCNN/{epoch:02d}-{val_loss:.4f}.hdf5'\n",
        "checkpointer =ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience =10)"
      ],
      "metadata": {
        "id": "9uZbSLvbCZrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, Y_train, validation_data = (X_test, Y_test), epochs=30, batch_size=200, verbose=0,\n",
        "                    callbacks=[early_stopping_callback, checkpointer])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxXyLyMNCvlW",
        "outputId": "100535f6-aa57-4d80-bc42-e40dcfa9639a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.05301, saving model to ./modelCNN/01-0.0530.hdf5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.05301 to 0.03550, saving model to ./modelCNN/02-0.0355.hdf5\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.03550\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.03550 to 0.03116, saving model to ./modelCNN/04-0.0312.hdf5\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.03116 to 0.02996, saving model to ./modelCNN/05-0.0300.hdf5\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.02996\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.02996 to 0.02651, saving model to ./modelCNN/07-0.0265.hdf5\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.02651\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.02651\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.02651\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.02651\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.02651\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.02651\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.02651\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.02651\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.02651\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.02651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, Y_test)[1] # 99.18%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3W8Z7kaBbda",
        "outputId": "b91fd996-6922-457f-a199-4ae36d1aa85b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0322 - accuracy: 0.9918\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9918000102043152"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_vloss=history.history['val_loss']\n",
        "y_loss=history.history['loss']\n",
        "# 그래프로 표현\n",
        "x_len = numpy.arange(len(y_loss))\n",
        "plt.plot(x_len, y_vloss, marker='.' , c='red', label ='testset loss')\n",
        "plt.plot(x_len, y_loss, marker='.' , c='blue', label ='trainset loss')\n",
        "\n",
        "# 그래프에 그리드를 주고 레이블 표시\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "LWwAZb-YDH0f",
        "outputId": "675d6649-3cc9-4947-858d-a4c1e49722d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6cx36ni88xhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 17장 딥러닝을 이용한 자연어 처리 "
      ],
      "metadata": {
        "id": "mKL9NBd0EoQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트의 토큰화\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "text = \"해보지 않으면 해낼 수 있다\"\n",
        "result = text_to_word_sequence(text)"
      ],
      "metadata": {
        "id": "vnx0gXswErdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0RRLzLpWOpq",
        "outputId": "560ef674-ba67-4fea-ce7b-7871753ac445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['해보지', '않으면', '해낼', '수', '있다']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bag of words : 같은 단어끼리 따로따로 가방에 담은 뒤 각 가방에 몇개의 단어가 들어가 있는지를 세는방법"
      ],
      "metadata": {
        "id": "2yhfBdyiWPGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "docs = [ '먼저 텍스트의 각 단어를 나누어 토큰화합니다.',\n",
        "        '텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.',\n",
        "        '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.']\n"
      ],
      "metadata": {
        "id": "4ry5ccLDWZW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token= Tokenizer()\n",
        "token.fit_on_texts(docs)\n",
        "print(token.word_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDQEXq15WpK-",
        "outputId": "b30dc11c-3384-4e7a-897e-4f8d50bfce3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화합니다', 1), ('단어로', 1), ('토큰화해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('토큰화한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " print(token.document_count) # 총 몇개의 문장이 들어가 있는지 카운트"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZsyNKbdWukH",
        "outputId": "cd04cbe5-05e1-4a83-b8f1-1351d2a0b788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(token.word_docs) # 각 단어가 몇개의 문장에 나오는지를 카운트함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RbNPWjRW3ip",
        "outputId": "f9753153-4dee-44c6-b51e-4c88d39627d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {'각': 1, '단어를': 1, '먼저': 1, '나누어': 1, '텍스트의': 2, '토큰화합니다': 1, '단어로': 1, '딥러닝에서': 2, '인식됩니다': 1, '토큰화해야': 1, '사용할': 1, '있습니다': 1, '수': 1, '토큰화한': 1, '결과는': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 단어에 메겨진 인덱스값을 출력\n",
        "print(token.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI6htGzBW-nJ",
        "outputId": "55f3fbc9-7d7d-4953-e979-253799fce486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'텍스트의': 1, '딥러닝에서': 2, '먼저': 3, '각': 4, '단어를': 5, '나누어': 6, '토큰화합니다': 7, '단어로': 8, '토큰화해야': 9, '인식됩니다': 10, '토큰화한': 11, '결과는': 12, '사용할': 13, '수': 14, '있습니다': 15}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어의 원핫 인코딩\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "text = '오랫동안 꿈꾸는 이는 그 꿈을 닮아간다.'\n",
        "token =Tokenizer()\n",
        "token.fit_on_texts([text])"
      ],
      "metadata": {
        "id": "CtMLnRRHXPXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(token.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjB8MJJOXhQS",
        "outputId": "9d1fbc2b-74dc-4e31-9a24-555786ab0f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'오랫동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(token.word_index) # 6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg4taX2vYfkO",
        "outputId": "747456da-4b92-4fa0-e288-3ba9785ab147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x= token.texts_to_sequences([text])"
      ],
      "metadata": {
        "id": "LsNL6uAzXi5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK4RFacYXn3n",
        "outputId": "b65565fa-cf9f-4f10-bed8-8e9d57a1f40b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4, 5, 6]]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_size = len(token.word_index)+1\n",
        "word_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sbn39WMXobz",
        "outputId": "e45bcbfc-3707-43b4-a4bf-562bcebea3e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x= to_categorical(x, num_classes=word_size)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvHLh8YKX2cf",
        "outputId": "22bf98a8-0e94-41c9-ec14-a3bb1ca56208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1.]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 임베딩\n",
        "from keras.layers import Embedding\n",
        "model= Sequential()\n",
        "model.add(Embedding(16,4)) # embedding함수는 최소 2개의 매개변수를 필요로함, 입력과 출력의 크기, 입력된 총단어수는 16개 출력되는 벡터크기는 4\n",
        "# input_length = n 추가옵션 : 총입력되는 단어수 16개중에서 매번 2개씩만 넣겠다는 뜻"
      ],
      "metadata": {
        "id": "diPLF95KX4Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs= ['너무 재밌네요','최고예요','참 잘 만든 영화예요','추천하고 싶은 영화입니다.','한번 더 보고싶네요','글쎄요','별로예요','생각보다 지루하네요','연기가 어색해요','재미없어요']\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ksb0oq6auS8",
        "outputId": "dcc9a2cd-3a40-4420-c428-fe0745f71d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['너무 재밌네요',\n",
              " '최고예요',\n",
              " '참 잘 만든 영화예요',\n",
              " '추천하고 싶은 영화입니다.',\n",
              " '한번 더 보고싶네요',\n",
              " '글쎄요',\n",
              " '별로예요',\n",
              " '생각보다 지루하네요',\n",
              " '연기가 어색해요',\n",
              " '재미없어요']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cls= np.array([1,1,1,1,1,0,0,0,0,0])\n",
        "cls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdzHQNH1Y1tg",
        "outputId": "06c5af63-4a06-42d6-abea-9a5fdc5ec44f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token = Tokenizer()\n",
        "token.fit_on_texts(docs)\n",
        "print(token.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3ZjRZA5bCf9",
        "outputId": "89d6a69a-bc2d-4a8a-d96c-5b89262b6386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로예요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x= token.texts_to_sequences(docs)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl92ahiecRbG",
        "outputId": "ed50b786-8717-4197-f997-d061cf7c41f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13], [14], [15], [16, 17], [18, 19], [20]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_x = pad_sequences(x,4)  # 길이 4로 통일시킴\n",
        "print(padded_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0IM2BkocXCf",
        "outputId": "73973028-4a1f-4d3b-8239-bd3cd0feeae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  1  2]\n",
            " [ 0  0  0  3]\n",
            " [ 4  5  6  7]\n",
            " [ 0  8  9 10]\n",
            " [ 0 11 12 13]\n",
            " [ 0  0  0 14]\n",
            " [ 0  0  0 15]\n",
            " [ 0  0 16 17]\n",
            " [ 0  0 18 19]\n",
            " [ 0  0  0 20]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_size = len(token.word_index)+1\n",
        "print(word_size) # 21"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0trqQXyce5G",
        "outputId": "2ca8520c-7171-422c-b3c7-72d8b8ee9be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Embedding(word_size,8, input_length=4) # 21,8,4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyrjJaw8c5CX",
        "outputId": "87a371d1-24f2-42cb-b750-2214eaea5624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.embeddings.Embedding at 0x7f4b1063f590>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(word_size,8, input_length =4))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model.fit(padded_x, cls, epochs=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCT6rQhVc-SH",
        "outputId": "c6bc4596-c7f0-4176-a84c-a2aa08bbf9c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1/1 [==============================] - 0s 367ms/step - loss: 0.6858 - accuracy: 0.6000\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6837 - accuracy: 0.8000\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6817 - accuracy: 1.0000\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6796 - accuracy: 1.0000\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6776 - accuracy: 1.0000\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6755 - accuracy: 0.9000\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6735 - accuracy: 0.9000\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6714 - accuracy: 0.9000\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6693 - accuracy: 0.9000\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6672 - accuracy: 0.9000\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6651 - accuracy: 0.9000\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6630 - accuracy: 0.9000\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6609 - accuracy: 0.9000\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6588 - accuracy: 0.9000\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6567 - accuracy: 0.9000\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6545 - accuracy: 0.9000\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6523 - accuracy: 0.9000\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6502 - accuracy: 0.9000\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6480 - accuracy: 0.9000\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6458 - accuracy: 0.9000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4bba1cc3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(padded_x, cls)[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buyOCI7gdVj2",
        "outputId": "e3ceb937-9033-49da-d948-59ed167536c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 88ms/step - loss: 0.6435 - accuracy: 0.9000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8999999761581421"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AxcVlfH5df7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18장 시퀀스 배열로 다루는 순환 신경망(RNN)"
      ],
      "metadata": {
        "id": "0BZFLRC8S-Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM을 이용한 로이터 뉴스 카테고리 분류"
      ],
      "metadata": {
        "id": "S4tkB9qJTCzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import reuters\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "l1jFqME0bZOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = reuters.load_data(num_words= 1000, test_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAolxxxYbeTi",
        "outputId": "4619a2f6-e9c1-4f4b-c26e-14ba45da8ef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n",
            "2121728/2110848 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "category = np.max(Y_train)+1\n",
        "print(category)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8duiQZAbqZ_",
        "outputId": "79dce892-2de9-4924-b406-1ed5cbb37679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3dYBRSKbvCc",
        "outputId": "fa2a6653-6255-4d00-864b-46e89b5df4c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8982"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4gYW4tmb1v1",
        "outputId": "c3f7a3b9-922b-43fc-e87f-8c73d0c1b327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2246"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc-399fPb3FX",
        "outputId": "597f1a36-af5a-455c-d2da-698339bdae8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 2, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 2, 2, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 2, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import sequence"
      ],
      "metadata": {
        "id": "FpaATdeAb5Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = sequence.pad_sequences(X_train, maxlen=100)\n",
        "x_test = sequence.pad_sequences(X_test, maxlen=100)"
      ],
      "metadata": {
        "id": "R9dmE-UzcDkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "y_train= to_categorical(Y_train)\n",
        "y_test = to_categorical(Y_test)"
      ],
      "metadata": {
        "id": "mXz7sSHccH8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(1000,100))\n",
        "model.add(LSTM(100, activation='tanh'))\n",
        "model.add(Dense(46, activation='softmax'))\n"
      ],
      "metadata": {
        "id": "G8i7jaR1cW5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "history = model.fit(x_train, y_train, batch_size=100, epochs=20, validation_data = (x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXCsGinpctvD",
        "outputId": "18169f68-ab3f-4381-ccaa-36fb2ffa37e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "90/90 [==============================] - 7s 17ms/step - loss: 2.5487 - accuracy: 0.3773 - val_loss: 2.1125 - val_accuracy: 0.4893\n",
            "Epoch 2/20\n",
            "90/90 [==============================] - 1s 11ms/step - loss: 2.0253 - accuracy: 0.4977 - val_loss: 1.9393 - val_accuracy: 0.5076\n",
            "Epoch 3/20\n",
            "90/90 [==============================] - 1s 11ms/step - loss: 1.8340 - accuracy: 0.5318 - val_loss: 1.7735 - val_accuracy: 0.5561\n",
            "Epoch 4/20\n",
            "90/90 [==============================] - 1s 12ms/step - loss: 1.7084 - accuracy: 0.5617 - val_loss: 1.7453 - val_accuracy: 0.5530\n",
            "Epoch 5/20\n",
            "90/90 [==============================] - 1s 11ms/step - loss: 1.6459 - accuracy: 0.5769 - val_loss: 1.6456 - val_accuracy: 0.5908\n",
            "Epoch 6/20\n",
            "90/90 [==============================] - 1s 11ms/step - loss: 1.5544 - accuracy: 0.6040 - val_loss: 1.6073 - val_accuracy: 0.5806\n",
            "Epoch 7/20\n",
            "90/90 [==============================] - 1s 11ms/step - loss: 1.4530 - accuracy: 0.6290 - val_loss: 1.4849 - val_accuracy: 0.6336\n",
            "Epoch 8/20\n",
            "90/90 [==============================] - 1s 11ms/step - loss: 1.3555 - accuracy: 0.6542 - val_loss: 1.4299 - val_accuracy: 0.6385\n",
            "Epoch 9/20\n",
            "90/90 [==============================] - 1s 11ms/step - loss: 1.2703 - accuracy: 0.6760 - val_loss: 1.3485 - val_accuracy: 0.6612\n",
            "Epoch 10/20\n",
            "90/90 [==============================] - 1s 11ms/step - loss: 1.2139 - accuracy: 0.6892 - val_loss: 1.3186 - val_accuracy: 0.6670\n",
            "Epoch 11/20\n",
            "90/90 [==============================] - 1s 11ms/step - loss: 1.1401 - accuracy: 0.7104 - val_loss: 1.2656 - val_accuracy: 0.6825\n",
            "Epoch 12/20\n",
            "90/90 [==============================] - 1s 11ms/step - loss: 1.0845 - accuracy: 0.7290 - val_loss: 1.2670 - val_accuracy: 0.6825\n",
            "Epoch 13/20\n",
            "90/90 [==============================] - 1s 11ms/step - loss: 1.0308 - accuracy: 0.7411 - val_loss: 1.2139 - val_accuracy: 0.6928\n",
            "Epoch 14/20\n",
            "90/90 [==============================] - 1s 11ms/step - loss: 0.9956 - accuracy: 0.7518 - val_loss: 1.2242 - val_accuracy: 0.6963\n",
            "Epoch 15/20\n",
            "90/90 [==============================] - 1s 11ms/step - loss: 0.9498 - accuracy: 0.7615 - val_loss: 1.1949 - val_accuracy: 0.7026\n",
            "Epoch 16/20\n",
            "90/90 [==============================] - 1s 11ms/step - loss: 0.8976 - accuracy: 0.7737 - val_loss: 1.1822 - val_accuracy: 0.7084\n",
            "Epoch 17/20\n",
            "90/90 [==============================] - 1s 11ms/step - loss: 0.8573 - accuracy: 0.7819 - val_loss: 1.1905 - val_accuracy: 0.7133\n",
            "Epoch 18/20\n",
            "90/90 [==============================] - 1s 11ms/step - loss: 0.8175 - accuracy: 0.7953 - val_loss: 1.1749 - val_accuracy: 0.7150\n",
            "Epoch 19/20\n",
            "90/90 [==============================] - 1s 11ms/step - loss: 0.7863 - accuracy: 0.8004 - val_loss: 1.1963 - val_accuracy: 0.7164\n",
            "Epoch 20/20\n",
            "90/90 [==============================] - 1s 11ms/step - loss: 0.7413 - accuracy: 0.8115 - val_loss: 1.1902 - val_accuracy: 0.7115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tzELEE9dKgk",
        "outputId": "047b0a7c-4a79-4099-b343-91143a6fed66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71/71 [==============================] - 0s 5ms/step - loss: 1.1902 - accuracy: 0.7115\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7114871144294739"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_vloss = history.history['val_loss']\n",
        "y_loss = history.history['loss']"
      ],
      "metadata": {
        "id": "jUbDAByydSjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_len = np.arange(len(y_loss))"
      ],
      "metadata": {
        "id": "OuFLOflJdfon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf \n",
        "\n",
        "plt.plot(x_len, y_vloss, marker ='.', c='red', label = 'testset_loss')\n",
        "plt.plot(x_len, y_loss, marker = '.', c='blue', label = 'trainset_loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "rNcnCaqJdjET",
        "outputId": "a891d696-ccfd-4fac-e58a-27a8dd064dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lstm과 cnn을 조합한 영화리뷰 분류"
      ],
      "metadata": {
        "id": "67JnR25Vd774"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.layers import Dense, Dropout, Activation, Embedding, LSTM, Conv1D, MaxPool1D\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=5000)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=100)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyCesRg1eRwf",
        "outputId": "7d421a25-5e43-41d9-9799-48ed4c832059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulJzGf9gf6Y_",
        "outputId": "6ec0c1c6-38f0-4bea-8775-6df11978af29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(5000,100))  # 총 5000개의 단어만 표시, 한 단어 벡터 임베딩은 100차원으로함\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv1D(64,5, padding='valid', activation='relu', strides=1))\n",
        "model.add(MaxPool1D(pool_size=4))\n",
        "\n",
        "model.add(LSTM(55))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIDzSdNAfgFg",
        "outputId": "4823bd3c-20b8-488c-c58e-46080242e85b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, None, 100)         500000    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, None, 100)         0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, None, 64)          32064     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, None, 64)         0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 55)                26400     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 56        \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 558,520\n",
            "Trainable params: 558,520\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Fw4druS7gpMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, batch_size= 100, epochs=10, validation_data = (x_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcMwxVuUg-uv",
        "outputId": "369a5fbf-68c6-4171-c6ca-547f84e7271c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "250/250 [==============================] - 4s 11ms/step - loss: 0.4492 - accuracy: 0.7757 - val_loss: 0.3378 - val_accuracy: 0.8507\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.2932 - accuracy: 0.8784 - val_loss: 0.3263 - val_accuracy: 0.8560\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.2484 - accuracy: 0.9004 - val_loss: 0.3340 - val_accuracy: 0.8544\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.2160 - accuracy: 0.9158 - val_loss: 0.3388 - val_accuracy: 0.8553\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.1833 - accuracy: 0.9308 - val_loss: 0.3674 - val_accuracy: 0.8531\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.1484 - accuracy: 0.9450 - val_loss: 0.3836 - val_accuracy: 0.8438\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.1230 - accuracy: 0.9552 - val_loss: 0.4658 - val_accuracy: 0.8480\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.1046 - accuracy: 0.9626 - val_loss: 0.4300 - val_accuracy: 0.8438\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.0908 - accuracy: 0.9679 - val_loss: 0.4777 - val_accuracy: 0.8450\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.0750 - accuracy: 0.9737 - val_loss: 0.5338 - val_accuracy: 0.8379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pzV6aUchFBu",
        "outputId": "bc167ddc-45f8-4fbc-a11a-e5f6778d1299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 4s 4ms/step - loss: 0.3904 - accuracy: 0.8421\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8420799970626831"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_vloss = history.history['val_loss']\n",
        "y_loss = history.history['loss']\n",
        "x_len = np.arange(len(y_loss))\n",
        "print(x_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dKNbmEmhHe_",
        "outputId": "c394fa28-49a0-444b-f14d-3d80458c770e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x_len, y_vloss, marker='.', c='red', label='testset_loss')\n",
        "plt.plot(x_len, y_loss, marker='.', c='blue', label='trainset_loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "0dx0hhWChbx0",
        "outputId": "1d769805-15d2-4350-aeb2-c2700e9973c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 19장 GAN, 오토인코"
      ],
      "metadata": {
        "id": "sAKdAVdaVmSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 생성적 적대신경망\n",
        "# 진짜같은 가짜를 만들기 위한 노력\n",
        "# DEEP CONVOOLUTION GAN (DCGAN) : CNN을 GAN에 적용\n",
        "# 최적화 과정이나 풀링과정이 없고, 패딩과정이 포함됨\n",
        "# 패딩이 등장하는 이유 : 입력크기와 출력크기를 똑같이 맞추기 위해서임 EX) PADDING=same 명령어가 필요함\n",
        "# batch normalization : 배치 정규화 : 입력데이터의 평균이0, 분산이 1이되도록 재배치 하는것, 다음층으로 입력될 값을 일정하게 재배치하는 역할, 이를 통해 층의 개수가 늘어나도 안정적인 학습을 진행가능\n",
        "# 생성자의 활성화 함수로 relu사용, 판별자로 넘겨주기 직전에 tanh함수 사용 출력되는값으 -1~1로 맞출수있음\n",
        "# \n"
      ],
      "metadata": {
        "id": "GORx7ov3hulq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, LeakyReLU, UpSampling1D, UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 생성자 모델 만들기\n",
        "\n",
        "generator = Sequential()\n",
        "generator.add(Dense(128*7*7, input_dim =10, activation = LeakyReLU(0.2)))  # 128은 임의로 정한 노드의 수 7*7은 이미지의 최초크기 , UPSAMPLING에서 2배씩 2번 4배가 되기때문에 이렇게 설정함\n",
        "generator.add(BatchNormalization())\n",
        "generator.add(Reshape((7,7,128)))\n",
        "generator.add(UpSampling2D())\n",
        "\n",
        "generator.add(Conv2D(64, kernel_size=5, padding='same'))\n",
        "\n",
        "generator.add(BatchNormalization())\n",
        "generator.add(Activation(LeakyReLU(0.2))) # GAN에서 RELU쓰면 학습이 불안정해지는 경우가 많아 변형된 RELU사용 \n",
        "generator.add(UpSampling2D())\n",
        "\n",
        "generator.add(Conv2D(1, kernel_size=5, padding='same', activation = 'tanh'))\n",
        "\n",
        "# generator.add(Dense(128*7*7, input_dim=100, activation= LeakyReLU(0.2)))\n",
        "generator.summary()"
      ],
      "metadata": {
        "id": "LOoy9vfKVq5E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33e13250-30f6-48b1-b826-9e8abd059fee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_8 (Dense)             (None, 6272)              68992     \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 6272)             25088     \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " reshape_3 (Reshape)         (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " up_sampling2d_6 (UpSampling  (None, 14, 14, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 14, 14, 64)        204864    \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 14, 14, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " up_sampling2d_7 (UpSampling  (None, 28, 28, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 28, 28, 1)         1601      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 300,801\n",
            "Trainable params: 288,129\n",
            "Non-trainable params: 12,672\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 판별자 모델 만들기\n",
        "\n",
        "discriminator = Sequential()\n",
        "\n",
        "discriminator.add(Conv2D(64, kernel_size = 5, strides=2 ,input_shape=(28,28,1), padding='same'))\n",
        "\n",
        "discriminator.add(Activation(LeakyReLU(0.2)))\n",
        "discriminator.add(Dropout(0.3))\n",
        "\n",
        "discriminator.add(Conv2D(128, kernel_size=5, strides=2, padding='same'))\n",
        "\n",
        "discriminator.add(Activation(LeakyReLU(0.2)))\n",
        "discriminator.add(Dropout(0.3))\n",
        "discriminator.add(Flatten())\n",
        "\n",
        "discriminator.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "discriminator.trainable =False   \n",
        "# 가짜인지 진짜인지 판별만 해줄뿐 자기자신이 학습을 해서는 안됨/\n",
        "# 판별자가 얻은 가중치는 판별자 자신이 학습하는데 쓰이는게 아니라 생성자로 넘겨주어 생성자가 업데이트된 이미지를 만들도록 해야함\n",
        "discriminator.summary()"
      ],
      "metadata": {
        "id": "TuqyPp-ve2pe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8209b0f6-bbfe-4f17-f530-94915249ef0c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_14 (Conv2D)          (None, 14, 14, 64)        1664      \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 7, 7, 128)         204928    \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 6272)              0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 6273      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 212,865\n",
            "Trainable params: 0\n",
            "Non-trainable params: 212,865\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 생성자와 판별자 모델을 연결시키는 gan만들기 \n",
        "ginput= Input(shape = (100,))\n",
        "\n",
        "dis_output = discriminator(generator(ginput))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "j-N20d-hf7yp",
        "outputId": "5803a61d-82bc-4c5d-d920-f56aa90180bc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 10) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name='dense_8_input'), name='dense_8_input', description=\"created by layer 'dense_8_input'\"), but it was called on an input with incompatible shape (None, 100).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c72ee19e2317>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mginput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdis_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mginput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshape_as_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m           raise ValueError(\n\u001b[0;32m--> 248\u001b[0;31m               \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m               \u001b[0;34mf'incompatible with the layer: expected axis {axis}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m               \u001b[0;34mf'of input shape to have value {value}, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"sequential_6\" (type Sequential).\n\nInput 0 of layer \"dense_8\" is incompatible with the layer: expected axis -1of input shape to have value 10, but received input with shape (None, 100)\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 100), dtype=float32)\n  • training=None\n  • mask=None"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gan = Model(ginput, dis_output)\n",
        "gan.compile(loss='binary_crossentropy', optimizer = 'adam')\n",
        "gan.summary()"
      ],
      "metadata": {
        "id": "8LGNhggzJA6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gan_train(epoch, batch_size, saving_interval):\n",
        "  (X_train, _), (_,_) = mnist.load_data()\n",
        "  X_train = X_train.reshape(X_trin.shape[0] ,28,28,1).astype('float32')\n",
        "  X_train = (X_train - 127.5)/127.5\n",
        "  true = np.ones((batch_size, 1))\n",
        "  fake = np.zeros((batch_size,1))\n",
        "  for i in range(epoch):\n",
        "    #실제데이터를 판별자에 입력\n",
        "    idx = np.random(0,X_train.shape[0], batch_size)\n",
        "    imgs = X_train[idx]\n",
        "    d_loss_real = discriminator.train_on_batch(imgs, true)\n",
        "\n",
        "    #가상이미지를 판별자에 입력\n",
        "    noise = np.random.normal(0,1, (batch_size,100))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
        "\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "    g_loss = gen.train_on_batch(noise, true)\n",
        "\n",
        "    print('epoch:%d' % i, ' d_loss:%.4f' % d_loss, 'g_loss:%.4f' % g_loss)\n",
        "\n",
        "    if i % saving_interval == 0 :\n",
        "    noise = np.random.normal(0,1,(25,100))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    gen_imgs = 0.5*gen_imgs +0.5\n",
        "    fig, axs = plt.subplots(5,5)\n",
        "    count = 0\n",
        "    for j in range(5):\n",
        "    for k in range(5):\n",
        "    axs[k,k].imshow(gen_imgs[count,:,:,0], cmap='gray')\n",
        "    axs[j,k].axis('off')\n",
        "    count+=1\n",
        "    fig.savefig(\"gan_images/gan_mnist_%d.png\" % i)\n",
        "\n",
        "gen_train(4001, 32, 200)\n"
      ],
      "metadata": {
        "id": "6gzsAL4Ylb48"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1jXCCbU_lcAU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}